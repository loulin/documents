#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CRFæ•°æ®æŒ–æ˜æ™ºèƒ½åˆ†æAgent
Clinical Research Data Mining & Analysis Agent

åŠŸèƒ½ç‰¹ç‚¹:
1. å®æ—¶æ•°æ®å¤„ç†ä¸è´¨é‡è¯„ä¼° (é…ç½®é©±åŠ¨)
2. å¤šç»´åº¦ç»Ÿè®¡åˆ†æä¸å…³è”å‘ç° (é…ç½®é©±åŠ¨)
3. ç ”ç©¶ä»·å€¼è¯„ä¼°ä¸è®ºæ–‡åˆ‡å…¥ç‚¹æ¨è
4. è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆ
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
import json
import warnings
from dataclasses import dataclass
from enum import Enum
import os

# --- å¯é€‰ä¾èµ–åŒ… ---
try:
    import yaml
    HAS_YAML = True
except ImportError:
    HAS_YAML = False

try:
    import seaborn as sns
    import matplotlib.pyplot as plt
    HAS_PLOTTING = True
except ImportError:
    HAS_PLOTTING = False

try:
    from scipy import stats
    HAS_SCIPY = True
except ImportError:
    HAS_SCIPY = False

try:
    from sklearn.cluster import KMeans
    from sklearn.preprocessing import StandardScaler
    from sklearn.ensemble import RandomForestClassifier
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False

try:
    import networkx as nx
    HAS_NETWORKX = True
except ImportError:
    HAS_NETWORKX = False

warnings.filterwarnings('ignore')

# --- æ•°æ®ç»“æ„å®šä¹‰ ---
class ResearchValue(Enum):
    """ç ”ç©¶ä»·å€¼ç­‰çº§"""
    VERY_HIGH = "æé«˜ä»·å€¼"
    HIGH = "é«˜ä»·å€¼"
    MODERATE = "ä¸­ç­‰ä»·å€¼"
    LOW = "ä½ä»·å€¼"
    INSUFFICIENT = "æ•°æ®ä¸è¶³"

class PublicationOpportunity(Enum):
    """å‘è¡¨æœºä¼šç±»å‹"""
    ORIGINAL_RESEARCH = "åŸåˆ›ç ”ç©¶"
    OBSERVATIONAL_STUDY = "è§‚å¯Ÿæ€§ç ”ç©¶"
    CROSS_SECTIONAL = "æ¨ªæ–­é¢ç ”ç©¶"
    LONGITUDINAL = "çºµå‘ç ”ç©¶"
    META_ANALYSIS = "èŸèƒåˆ†æ"
    CASE_SERIES = "ç—…ä¾‹ç³»åˆ—"

@dataclass
class ResearchInsight:
    """ç ”ç©¶å‘ç°"""
    title: str
    description: str
    value_level: ResearchValue
    publication_type: PublicationOpportunity
    statistical_power: float
    sample_size: int
    key_variables: List[str]
    statistical_methods: List[str]
    expected_impact_factor: str
    implementation_difficulty: str
    recommendations: List[str]

# --- ä¸»Agentç±» ---
class CRFResearchMiningAgent:
    """CRFæ•°æ®æŒ–æ˜æ™ºèƒ½åˆ†æä»£ç† (é…ç½®é©±åŠ¨ç‰ˆ)"""
    
    def __init__(self, config_dir: str, data_storage_path: str = "./crf_data/"):
        """åˆå§‹åŒ–CRFåˆ†æä»£ç†"""
        self.config_dir = config_dir
        self.data_storage_path = data_storage_path
        self.data_cache = {}
        self.analysis_history = []
        self.research_insights = []
        
        if not HAS_YAML:
            print("âŒ å…³é”®ä¾èµ–åŒ… PyYAML æœªå®‰è£…ï¼ŒAgentæ— æ³•å¯åŠ¨ã€‚è¯·è¿è¡Œ: pip install pyyaml")
            raise ImportError("PyYAML not found.")

        self.config = self._load_config()
        self.data_sources_config = self.config.get('data_sources', {})
        self.quality_rules = self.config.get('quality_rules', {})
        self.analysis_config = self.config.get('analysis', [])

        os.makedirs(data_storage_path, exist_ok=True)
        print("âœ… CRFæ•°æ®æŒ–æ˜Agentåˆå§‹åŒ–æˆåŠŸ")

    def _load_config(self) -> Dict:
        """åŠ è½½ç›®å½•ä¸­çš„æ‰€æœ‰YAMLé…ç½®æ–‡ä»¶"""
        full_config = {}
        try:
            for filename in os.listdir(self.config_dir):
                if filename.endswith(('.yaml', '.yml')):
                    config_path = os.path.join(self.config_dir, filename)
                    with open(config_path, 'r', encoding='utf-8') as f:
                        config_key = os.path.splitext(filename)[0]
                        full_config[config_key] = yaml.safe_load(f)
                    print(f"âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
            return full_config
        except Exception as e:
            print(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶ç›®å½• '{self.config_dir}' å¤±è´¥: {e}")
            return {}
        
    def load_crf_data(self) -> Dict[str, pd.DataFrame]:
        """æ ¹æ®é…ç½®æ–‡ä»¶åŠ è½½CRFæ•°æ®"""
        datasets = {}
        if not self.data_sources_config:
            print("âš ï¸ é…ç½®æ–‡ä»¶ä¸­æ— æ•°æ®æºä¿¡æ¯(data_sources.yaml)ï¼Œè·³è¿‡åŠ è½½ã€‚")
            return datasets

        for data_type, info in self.data_sources_config.items():
            file_path = info.get('path')
            if not file_path:
                print(f"âš ï¸ è­¦å‘Š: {data_type} æœªé…ç½®'path'ï¼Œè·³è¿‡ã€‚")
                continue
            try:
                if os.path.exists(file_path):
                    if file_path.endswith('.csv'):
                        datasets[data_type] = pd.read_csv(file_path)
                    elif file_path.endswith('.json'):
                        with open(file_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                        # ä½¿ç”¨ json_normalize æ¥â€œå‹å¹³â€åµŒå¥—çš„JSONç»“æ„
                        datasets[data_type] = pd.json_normalize(data)
                    print(f"âœ… æˆåŠŸåŠ è½½ {data_type}: {len(datasets[data_type])} æ¡è®°å½•")
                else:
                    print(f"âš ï¸ è­¦å‘Š: æ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨ '{file_path}'ï¼Œè·³è¿‡åŠ è½½ {data_type}")
            except Exception as e:
                print(f"âŒ åŠ è½½ {data_type} å¤±è´¥: {e}")
        self.data_cache = datasets
        return datasets

    # --- æ•°æ®è´¨é‡å¼•æ“ ---
    def assess_data_quality(self) -> Dict[str, Any]:
        """æ ¹æ®é…ç½®è§„åˆ™å…¨é¢è¯„ä¼°æ•°æ®è´¨é‡"""
        quality_report = {}
        for dataset_name, df in self.data_cache.items():
            quality_metrics = {
                'total_records': len(df),
                'total_features': len(df.columns),
                'missing_rate': df.isnull().mean().mean(),
                'duplicate_rate': df.duplicated().sum() / len(df) if len(df) > 0 else 0,
            }
            dataset_rules = self.quality_rules.get(dataset_name, {})
            generic_rules = self.quality_rules.get('__GENERIC__', {})
            quality_metrics['quality_issues'] = self._apply_rules(df, dataset_rules, generic_rules)
            quality_metrics.update(self._analyze_data_distribution(df, dataset_name))
            quality_report[dataset_name] = quality_metrics
        return quality_report

    def _apply_rules(self, df: pd.DataFrame, specific_rules: Dict, generic_rules: Dict) -> List[str]:
        """åº”ç”¨æ•°æ®è´¨é‡è§„åˆ™"""
        issues = []
        def get_col_rules(col_name, col_type):
            col_rules = list(specific_rules.get(col_name, []))
            if col_type == 'numeric':
                col_rules.extend(specific_rules.get('__ALL_NUMERIC__', []))
                col_rules.extend(generic_rules.get('__ALL_NUMERIC__', []))
            elif col_type == 'date':
                col_rules.extend(specific_rules.get('__ALL_DATE__', []))
                col_rules.extend(generic_rules.get('__ALL_DATE__', []))
            return col_rules

        for col in df.columns:
            col_type = 'other'
            if pd.api.types.is_numeric_dtype(df[col]): col_type = 'numeric'
            elif pd.api.types.is_datetime64_any_dtype(df[col]) or 'date' in col.lower() or 'time' in col.lower(): col_type = 'date'

            for rule in get_col_rules(col, col_type):
                try:
                    rule_type, params, message = rule.get('type'), rule.get('params'), rule.get('message', f"{col}æœªé€šè¿‡{rule.get('type')}æ£€æŸ¥")
                    if rule_type == 'range' and col_type == 'numeric' and not df[col].empty:
                        invalid = df[(df[col] < params[0]) | (df[col] > params[1])]
                        if not invalid.empty: issues.append(f"{message} ({len(invalid)}æ¡è®°å½•)")
                    elif rule_type == 'expression':
                        invalid = df.query(f"not ({params})")
                        if not invalid.empty: issues.append(f"{message} ({len(invalid)}æ¡è®°å½•)")
                    elif rule_type == 'outlier_iqr' and col_type == 'numeric' and not df[col].empty:
                        Q1, Q3, IQR = df[col].quantile(0.25), df[col].quantile(0.75), df[col].quantile(0.75) - df[col].quantile(0.25)
                        if IQR > 0:
                            outliers = df[(df[col] < Q1 - params * IQR) | (df[col] > Q3 + params * IQR)]
                            if not outliers.empty: issues.append(f"{col}: {message} ({len(outliers)}æ¡è®°å½•)")
                    elif rule_type == 'is_date' and col_type == 'date' and not df[col].empty:
                        invalid_count = pd.to_datetime(df[col], errors='coerce').isnull().sum() - df[col].isnull().sum()
                        if invalid_count > 0: issues.append(f"{col}: {message} ({invalid_count}æ¡è®°å½•)")
                except Exception as e: issues.append(f"è§„åˆ™æ‰§è¡Œé”™è¯¯ '{rule.get('type')}' on column '{col}': {e}")
        return list(set(issues))

    def _analyze_data_distribution(self, df: pd.DataFrame, dataset_name: str) -> Dict[str, Any]:
        """åˆ†ææ•°æ®åˆ†å¸ƒ (æè¿°æ€§ç»Ÿè®¡)"""
        dist_report = {}
        # ä»…å¯¹æ•°å€¼å‹æ•°æ®è¿›è¡Œæè¿°æ€§ç»Ÿè®¡
        numeric_df = df.select_dtypes(include=np.number)
        if not numeric_df.empty:
            desc = numeric_df.describe().to_dict()
            dist_report['numeric_distribution'] = desc
        
        # å¯¹ç±»åˆ«å‹æ•°æ®è¿›è¡Œé¢‘æ•°ç»Ÿè®¡
        categorical_df = df.select_dtypes(include=['object', 'category'])
        if not categorical_df.empty:
            cat_report = {}
            for col in categorical_df.columns:
                # ä»…æŠ¥å‘Šç±»åˆ«æ•°å°äº50çš„åˆ—ï¼Œé¿å…è¿‡äºå†—é•¿
                if categorical_df[col].nunique() < 50:
                    cat_report[col] = categorical_df[col].value_counts().to_dict()
            dist_report['categorical_distribution'] = cat_report
            
        return dist_report

    # --- ç ”ç©¶åˆ†æå¼•æ“ ---
    def discover_research_opportunities(self) -> List[ResearchInsight]:
        """æ ¹æ®åˆ†æé…ç½®æ–‡ä»¶åŠ¨æ€å‘ç°ç ”ç©¶æœºä¼š"""
        self.research_insights = []
        if not self.analysis_config: return []

        for i, analysis_def in enumerate(self.analysis_config, 1):
            try:
                print(f"\nğŸš€ [åˆ†æ {i}/{len(self.analysis_config)}] {analysis_def.get('title')}")
                df = self._prepare_analysis_data(analysis_def.get('datasets', []))
                if df is None or df.empty: 
                    print(f"âš ï¸ æ•°æ®å‡†å¤‡å¤±è´¥æˆ–ä¸ºç©ºï¼Œè·³è¿‡åˆ†æ: {analysis_def.get('title')}")
                    continue

                df = self._create_analysis_variables(df, analysis_def.get('variables', {}))
                analysis_func = self._get_analysis_function(analysis_def['analysis']['type'])
                analysis_results = analysis_func(df, analysis_def['analysis']['params'])
                analysis_results['sample_size'] = len(df)

                if self._check_trigger(analysis_results, analysis_def.get('trigger', {})):
                    print(f"âœ… å‘ç°æ½œåœ¨ç ”ç©¶æœºä¼š: {analysis_def.get('title')}")
                    self.research_insights.append(self._generate_insight(analysis_def, analysis_results))
                else:
                    print(f"â„¹ï¸ åˆ†æå®Œæˆï¼Œæœªè¾¾åˆ°è§¦å‘æ¡ä»¶ã€‚")
            except Exception as e:
                print(f"âŒ åˆ†æ '{analysis_def.get('id', 'N/A')}' æ‰§è¡Œå¤±è´¥: {e}")
        return self.research_insights

    def _prepare_analysis_data(self, d_defs: List[Dict]) -> Optional[pd.DataFrame]:
        if not d_defs or d_defs[0]['source'] not in self.data_cache: return None
        df = self.data_cache[d_defs[0]['source']].copy()
        
        # å¤„ç†åç»­æ•°æ®é›†çš„åˆå¹¶
        if len(d_defs) > 1:
            merge_key = d_defs[0].get('merge_key', 'patient_id') # é»˜è®¤ä½¿ç”¨ patient_id
            for d_def in d_defs[1:]:
                source_name = d_def['source']
                if source_name not in self.data_cache: 
                    print(f"âš ï¸ åœ¨æ•°æ®ç¼“å­˜ä¸­æœªæ‰¾åˆ° {source_name}ï¼Œæ— æ³•åˆå¹¶ã€‚")
                    return None
                
                right_df = self.data_cache[source_name]
                how = d_def.get('how', 'inner')
                
                # æ£€æŸ¥åˆå¹¶é”®æ˜¯å¦å­˜åœ¨
                if merge_key not in df.columns or merge_key not in right_df.columns:
                    print(f"âš ï¸ åˆå¹¶é”® '{merge_key}' åœ¨æ•°æ®é›†ä¸­ä¸å­˜åœ¨ï¼Œæ— æ³•åˆå¹¶ {source_name}ã€‚")
                    # å°è¯•å¯»æ‰¾å¤‡ç”¨é”®ï¼Œä¾‹å¦‚ 'subject_id'
                    fallback_keys = ['subject_id', 'id']
                    found_key = False
                    for key in fallback_keys:
                        if key in df.columns and key in right_df.columns:
                            merge_key = key
                            print(f"â„¹ï¸ ä½¿ç”¨å¤‡ç”¨åˆå¹¶é”®: '{key}'")
                            found_key = True
                            break
                    if not found_key:
                        return None # å¦‚æœæ‰¾ä¸åˆ°ä»»ä½•åˆé€‚çš„é”®ï¼Œåˆ™æ”¾å¼ƒåˆå¹¶
                
                df = pd.merge(df, right_df, on=merge_key, how=how)
        return df

    def _create_analysis_variables(self, df: pd.DataFrame, v_defs: Dict) -> pd.DataFrame:
        for name, v_def in v_defs.items():
            try:
                if v_def['type'] == 'cut': 
                    df[name] = pd.cut(df[v_def['source_column']], **v_def['params'])
                elif v_def['type'] == 'expression':
                    df[name] = df.eval(v_def['expression'])
            except Exception as e:
                print(f"âš ï¸ åˆ›å»ºå˜é‡ '{name}' å¤±è´¥: {e}")
        return df

    def _get_analysis_function(self, a_type: str) -> callable:
        if a_type == 'chi2_contingency': return self._run_chi2_contingency
        if a_type == 'group_by_rate_diff': return self._run_group_by_rate_diff
        raise ValueError(f"æœªçŸ¥çš„åˆ†æç±»å‹: {a_type}")

    def _run_chi2_contingency(self, df: pd.DataFrame, params: Dict) -> Dict:
        if not HAS_SCIPY: return {'p_value': 1.0, 'chi2': 0}
        try:
            ct = pd.crosstab(df[params['index']], df[params['columns']])
            if ct.empty or ct.shape[0] < 2 or ct.shape[1] < 2:
                return {'p_value': 1.0, 'chi2': 0, 'error': 'Contingency table is too small'}
            chi2, p, _, _ = stats.chi2_contingency(ct)
            return {'chi2': chi2, 'p_value': p}
        except KeyError as e:
            return {'p_value': 1.0, 'chi2': 0, 'error': f"Column not found: {e}"}

    def _run_group_by_rate_diff(self, df: pd.DataFrame, params: Dict) -> Dict:
        """Groups by a column, calculates rates of a boolean column, and finds the difference."""
        group_by_col = params['group_by_col']
        rate_col = params['rate_col']
        
        if group_by_col not in df.columns or rate_col not in df.columns:
            return {'rate_difference': 0, 'error': f"Column not found in group_by_rate_diff"}

        # Ensure rate_col is boolean or 0/1
        if df[rate_col].dtype != bool and df[rate_col].nunique() <= 2:
            df[rate_col] = df[rate_col].astype(bool)
        else:
            # å¦‚æœæ— æ³•å®‰å…¨è½¬æ¢ä¸ºboolï¼Œåˆ™è¿”å›é”™è¯¯
            return {'rate_difference': 0, 'error': f"Rate column '{rate_col}' is not binary."}

        rates = df.groupby(group_by_col)[rate_col].mean()
        
        if len(rates) != 2:
            print(f"âš ï¸ group_by_rate_difféœ€è¦group_byåˆ—æ­£å¥½æœ‰ä¸¤ä¸ªç»„ï¼Œä½†å‘ç°äº†{len(rates)}ä¸ªã€‚")
            return {'rate_difference': 0}

        rate1, rate2 = rates.values
        group1, group2 = rates.index
        
        results = {
            'rate_difference': abs(rate1 - rate2),
            f"{group1}_rate": rate1,
            f"{group2}_rate": rate2,
        }
        # A bit of a hack to support the description template for male/female
        if 'male' in str(group1).lower() or 'ç”·' in str(group1):
            results['male_rate'] = rate1
            results['female_rate'] = rate2
        elif 'female' in str(group1).lower() or 'å¥³' in str(group1):
            results['female_rate'] = rate1
            results['male_rate'] = rate2
            
        return results

    def _check_trigger(self, results: Dict, t_def: Dict) -> bool:
        if not t_def: return True
        val = results.get(t_def['metric'])
        if val is None: return False
        op_map = {'lt': lambda a, b: a < b, 'gt': lambda a, b: a > b, 'lte': lambda a,b: a <=b, 'gte': lambda a,b: a >= b}
        return op_map[t_def['operator']](val, t_def['value']) if t_def['operator'] in op_map else False

    def _generate_insight(self, a_def: Dict, results: Dict) -> ResearchInsight:
        i_def = a_def['insight']
        # ä½¿ç”¨ .get() å¹¶æä¾›é»˜è®¤å€¼æ¥å¢å¼ºé²æ£’æ€§
        description = a_def.get('description_template', "No description template provided.").format(**results)
        
        return ResearchInsight(
            title=a_def.get('title', 'Untitled Insight'),
            description=description,
            value_level=ResearchValue[i_def.get('value_level', 'LOW')],
            publication_type=PublicationOpportunity[i_def.get('publication_type', 'OBSERVATIONAL_STUDY')],
            statistical_power=results.get('power', 0.8), # Placeholder
            sample_size=results.get('sample_size', 0),
            key_variables=i_def.get('key_variables', []),
            statistical_methods=i_def.get('statistical_methods', []),
            expected_impact_factor=i_def.get('expected_impact_factor', 'N/A'),
            implementation_difficulty=i_def.get('implementation_difficulty', 'N/A'),
            recommendations=i_def.get('recommendations', [])
        )

    # --- æŠ¥å‘Šç”Ÿæˆ ---
    def generate_research_priority_matrix(self, insights: List[ResearchInsight]) -> Optional[Any]:
        """ç”Ÿæˆç ”ç©¶ä¼˜å…ˆçº§çŸ©é˜µå›¾"""
        if not HAS_PLOTTING or not insights:
            return None
        
        value_map = {val.value: i for i, val in enumerate(ResearchValue)}
        difficulty_map = {'ä½': 0, 'ä¸­': 1, 'é«˜': 2}

        data = {
            'title': [i.title for i in insights],
            'value': [value_map.get(i.value_level.value, 0) for i in insights],
            'difficulty': [difficulty_map.get(i.implementation_difficulty, 0) for i in insights],
            'sample_size': [i.sample_size for i in insights]
        }
        df = pd.DataFrame(data)

        plt.figure(figsize=(12, 8))
        sns.scatterplot(data=df, x='difficulty', y='value', size='sample_size', hue='title', sizes=(100, 2000), legend=False)
        
        plt.title('Research Priority Matrix', fontsize=16)
        plt.xlabel('Implementation Difficulty', fontsize=12)
        plt.ylabel('Research Value', fontsize=12)
        
        plt.xticks(ticks=list(difficulty_map.values()), labels=list(difficulty_map.keys()))
        plt.yticks(ticks=list(value_map.values()), labels=list(value_map.keys()))
        
        for i, row in df.iterrows():
            plt.text(row['difficulty'] + 0.05, row['value'], row['title'], fontsize=9)
            
        plt.grid(True)
        plt.tight_layout()
        
        matrix_path = os.path.join(self.data_storage_path, 'research_priority_matrix.png')
        plt.savefig(matrix_path)
        plt.close()
        print(f"âœ… ç ”ç©¶ä¼˜å…ˆçº§çŸ©é˜µå·²ä¿å­˜è‡³: {matrix_path}")
        return matrix_path
    
    def generate_publication_roadmap(self, insights: List[ResearchInsight]) -> Dict[str, List[str]]:
        """ç”Ÿæˆå‘è¡¨è·¯çº¿å›¾"""
        if not insights:
            return {}
        
        roadmap = {}
        sorted_insights = sorted(insights, key=lambda x: (
            list(ResearchValue).index(x.value_level), 
            {'ä½': 0, 'ä¸­': 1, 'é«˜': 2}.get(x.implementation_difficulty, 0)
        ), reverse=False) # ä»·å€¼è¶Šé«˜ã€éš¾åº¦è¶Šä½ï¼Œè¶Šé å‰

        for pub_type in PublicationOpportunity:
            roadmap[pub_type.value] = [
                f"{insight.title} (ä»·å€¼: {insight.value_level.value}, éš¾åº¦: {insight.implementation_difficulty})"
                for insight in sorted_insights if insight.publication_type == pub_type
            ]
        return {k: v for k, v in roadmap.items() if v}


    def generate_comprehensive_report(self, quality_report: Dict, insights: List[ResearchInsight], output_path: str) -> str:
        """ç”Ÿæˆå…¨é¢çš„Markdownæ ¼å¼åˆ†ææŠ¥å‘Š"""
        report_content = f"""
# CRFæ•°æ®æŒ–æ˜æ™ºèƒ½åˆ†ææŠ¥å‘Š

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## 1. æ•°æ®è´¨é‡è¯„ä¼°

æœ¬éƒ¨åˆ†æ€»ç»“äº†æ‰€åŠ è½½æ•°æ®é›†çš„æ•´ä½“è´¨é‡æƒ…å†µã€‚

"""
        for name, metrics in quality_report.items():
            report_content += f"""
### 1.1 æ•°æ®é›†: `{name}`

*   **åŸºæœ¬ä¿¡æ¯**:
    *   æ€»è®°å½•æ•°: {metrics.get('total_records', 'N/A')}
    *   æ€»ç‰¹å¾æ•°: {metrics.get('total_features', 'N/A')}
*   **æ•°æ®è´¨é‡æŒ‡æ ‡**:
    *   å¹³å‡ç¼ºå¤±ç‡: {metrics.get('missing_rate', 0):.2%}
    *   é‡å¤è®°å½•ç‡: {metrics.get('duplicate_rate', 0):.2%}
*   **å‘ç°çš„è´¨é‡é—®é¢˜**:
"""
            issues = metrics.get('quality_issues', [])
            if issues:
                for issue in issues:
                    report_content += f"    *   {issue}\n"
            else:
                report_content += "    *   æœªå‘ç°æ˜æ˜¾çš„è´¨é‡é—®é¢˜ã€‚\n"

        report_content += "\n---\n\n## 2. ç ”ç©¶æœºä¼šæ´å¯Ÿ\n"

        if not insights:
            report_content += "æœ¬æ¬¡åˆ†ææœªå‘ç°ç¬¦åˆè§¦å‘æ¡ä»¶çš„æ˜¾è‘—ç ”ç©¶æœºä¼šã€‚\n"
        else:
            matrix_path = self.generate_research_priority_matrix(insights)
            if matrix_path:
                report_content += "### 2.1 ç ”ç©¶ä¼˜å…ˆçº§çŸ©é˜µ\n\n"
                report_content += f"![ç ”ç©¶ä¼˜å…ˆçº§çŸ©é˜µ]({os.path.basename(matrix_path)})\n\n"

            roadmap = self.generate_publication_roadmap(insights)
            if roadmap:
                report_content += "### 2.2 å‘è¡¨è·¯çº¿å›¾å»ºè®®\n\n"
                for pub_type, items in roadmap.items():
                    report_content += f"#### {pub_type}\n"
                    for item in items:
                        report_content += f"- {item}\n"
                    report_content += "\n"

            report_content += "### 2.3 ç ”ç©¶æœºä¼šè¯¦æƒ…\n\n"
            for i, insight in enumerate(insights, 1):
                report_content += f"""
#### {i}. {insight.title}

*   **ç ”ç©¶ä»·å€¼**: {insight.value_level.value}
*   **å‘è¡¨æœºä¼š**: {insight.publication_type.value}
*   **æ ¸å¿ƒå‘ç°**: {insight.description}
*   **æ ·æœ¬é‡**: {insight.sample_size}
*   **å…³é”®å˜é‡**: `{', '.join(insight.key_variables)}`
*   **ç»Ÿè®¡æ–¹æ³•**: `{', '.join(insight.statistical_methods)}`
*   **é¢„æœŸå½±å“å› å­**: {insight.expected_impact_factor}
*   **å®æ–½éš¾åº¦**: {insight.implementation_difficulty}
*   **å»ºè®®**:
"""
                for rec in insight.recommendations:
                    report_content += f"    *   {rec}\n"
                report_content += "\n"

        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            print(f"âœ… ç»¼åˆæŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")
        except Exception as e:
            print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
            
        return report_content

# --- ä¸»æ‰§è¡Œå‡½æ•° ---
def main():
    """ä½¿ç”¨ç¤ºä¾‹"""
    print("--- CRFæ•°æ®æŒ–æ˜æ™ºèƒ½åˆ†æAgent (é…ç½®é©±åŠ¨ç‰ˆ) ---")
    config_dir = 'config'
    if not os.path.isdir(config_dir):
        print(f"âŒ é”™è¯¯: é…ç½®ç›®å½•ä¸å­˜åœ¨äº '{config_dir}'")
        print("è¯·ç¡®ä¿ 'config' ç›®å½•å­˜åœ¨ï¼Œå¹¶ä¸”åŒ…å« data_sources.yaml, quality_rules.yaml, analysis.yaml")
        return

    agent = CRFResearchMiningAgent(config_dir=config_dir)
    
    print("\n--- 1. åŠ è½½æ•°æ® ---")
    datasets = agent.load_crf_data()
    if not datasets: return

    print("\n--- 2. è¯„ä¼°æ•°æ®è´¨é‡ ---")
    quality_report = agent.assess_data_quality()
    for name, metrics in quality_report.items():
        print(f"\næ•°æ®é›†: {name}")
        print(f"  - é—®é¢˜åˆ—è¡¨: {metrics.get('quality_issues', [])}")

    print("\n--- 3. å‘ç°ç ”ç©¶æœºä¼š ---")
    insights = agent.discover_research_opportunities()
    
    print("\n--- 4. ç”Ÿæˆç»¼åˆæŠ¥å‘Š ---")
    report = agent.generate_comprehensive_report(quality_report, insights, 'CRF_Research_Analysis_Report.md')
    
    print("\nâœ… åˆ†æå®Œæˆï¼")
    print(f"ğŸ“„ å‘ç° {len(agent.research_insights)} ä¸ªç ”ç©¶æœºä¼š")
    print("ğŸ¯ è¯·æŸ¥çœ‹ 'CRF_Research_Analysis_Report.md' è·å–è¯¦ç»†æŠ¥å‘Š")

if __name__ == "__main__":
    main()
