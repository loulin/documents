# åŸºäºå¤§æ¨¡å‹çš„åŒ»ç–—æ•°æ®ç»“æ„åŒ–å¤„ç†æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—è¯¦ç»†è¯´æ˜å¦‚ä½•ç»“åˆæˆ‘ä»¬çš„64ä¸ªå†…åˆ†æ³Œä»£è°¢ç–¾ç—…CSV Schemaå’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¯¹åŒ»ç–—åŸå§‹æ•°æ®è¿›è¡Œæ™ºèƒ½ç»“æ„åŒ–å¤„ç†ã€‚æ”¯æŒå®æ—¶å¤„ç†å’Œæ‰¹é‡å¤„ç†ä¸¤ç§æ¨¡å¼ï¼Œå¸®åŠ©åŒ»ç”Ÿå¿«é€Ÿå°†éç»“æ„åŒ–çš„åŒ»ç–—è®°å½•è½¬æ¢ä¸ºæ ‡å‡†åŒ–çš„ç»“æ„åŒ–æ•°æ®ã€‚

## ğŸ¯ å¤„ç†ç›®æ ‡

- **æ™ºèƒ½è§£æ**ï¼šè‡ªåŠ¨è¯†åˆ«åŒ»ç–—æ–‡æœ¬ä¸­çš„å…³é”®ä¿¡æ¯
- **ç»“æ„åŒ–è¾“å‡º**ï¼šæŒ‰ç…§Schemaæ ‡å‡†è¾“å‡ºç»“æ„åŒ–æ•°æ®
- **è´¨é‡ä¿è¯**ï¼šç¡®ä¿æ•°æ®å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
- **æ•ˆç‡æå‡**ï¼šå¤§å¹…å‡å°‘äººå·¥å½•å…¥å·¥ä½œé‡
- **æ ‡å‡†åŒ–**ï¼šç»Ÿä¸€çš„æ•°æ®æ ¼å¼ä¾¿äºåç»­åˆ†æ

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

```
åŸå§‹åŒ»ç–—æ•°æ® â†’ SchemaåŠ è½½å™¨ â†’ å¤§æ¨¡å‹å¤„ç†å™¨ â†’ ç»“æ„åŒ–è¾“å‡º â†’ è´¨é‡éªŒè¯ â†’ æ•°æ®å­˜å‚¨
     â†“              â†“              â†“             â†“            â†“           â†“
   æ–‡æœ¬/å›¾åƒ      CSVè§£æ      æç¤ºå·¥ç¨‹+LLM     JSON/CSV     è§„åˆ™æ£€æŸ¥     æ•°æ®åº“
```

## ğŸ’» æŠ€æœ¯å®ç°æ–¹æ¡ˆ

### æ–¹æ¡ˆä¸€ï¼šå®æ—¶å¤„ç†ç³»ç»Ÿ

é€‚ç”¨äºé—¨è¯Šã€ä½é™¢ç­‰éœ€è¦å³æ—¶å¤„ç†çš„åœºæ™¯ã€‚

#### 1. ç³»ç»Ÿç»„ä»¶

```python
# requirements.txt
openai>=1.0.0
pandas>=1.5.0
pydantic>=2.0.0
fastapi>=0.100.0
uvicorn>=0.20.0
redis>=4.5.0
```

#### 2. SchemaåŠ è½½å™¨

```python
import pandas as pd
import json
from typing import Dict, List, Any
from pathlib import Path

class SchemaLoader:
    def __init__(self, schema_dir: str = "./SchemaCSV"):
        self.schema_dir = Path(schema_dir)
        self.schemas = {}
        self.load_all_schemas()

    def load_all_schemas(self):
        """åŠ è½½æ‰€æœ‰CSV Schemaæ–‡ä»¶"""
        for csv_file in self.schema_dir.glob("*-schema.csv"):
            schema_name = csv_file.stem.replace("-schema", "")
            try:
                df = pd.read_csv(csv_file)
                self.schemas[schema_name] = {
                    'fields': df.to_dict('records'),
                    'field_names': df['fieldName'].tolist(),
                    'field_descriptions': dict(zip(df['fieldName'], df['description'])),
                    'data_types': dict(zip(df['fieldName'], df['dataType'])),
                    'examples': dict(zip(df['fieldName'], df['examples']))
                }
                print(f"âœ… å·²åŠ è½½Schema: {schema_name} ({len(df)}ä¸ªå­—æ®µ)")
            except Exception as e:
                print(f"âŒ åŠ è½½Schemaå¤±è´¥: {csv_file} - {e}")

    def get_schema(self, schema_name: str) -> Dict:
        """è·å–æŒ‡å®šSchema"""
        return self.schemas.get(schema_name)

    def list_schemas(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨Schema"""
        return list(self.schemas.keys())

    def generate_prompt_template(self, schema_name: str) -> str:
        """ç”Ÿæˆç”¨äºå¤§æ¨¡å‹çš„æç¤ºè¯æ¨¡æ¿"""
        schema = self.get_schema(schema_name)
        if not schema:
            return ""

        template = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—æ•°æ®ç»“æ„åŒ–ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹Schemaå°†åŒ»ç–—æ–‡æœ¬è½¬æ¢ä¸ºç»“æ„åŒ–çš„JSONæ ¼å¼ã€‚

Schemaåç§°: {schema_name}

å­—æ®µå®šä¹‰:
"""
        for field in schema['fields'][:20]:  # æ˜¾ç¤ºå‰20ä¸ªå­—æ®µé¿å…è¿‡é•¿
            template += f"- {field['fieldName']} ({field['fieldNameCN']}): {field['description']} [ç±»å‹: {field['dataType']}]\n"

        if len(schema['fields']) > 20:
            template += f"... è¿˜æœ‰{len(schema['fields']) - 20}ä¸ªå­—æ®µ\n"

        template += """
**ä¸¥æ ¼è¦æ±‚**:
1. **ä»…æå–æ˜ç¡®å­˜åœ¨çš„ä¿¡æ¯**ï¼šåªä»æ–‡æœ¬ä¸­æå–æ˜ç¡®æåˆ°çš„æ•°æ®ï¼Œç»ä¸æ¨æµ‹ã€ä¼°ç®—æˆ–ç¼–é€ 
2. **ç¼ºå¤±æ•°æ®å¿…é¡»æ ‡è®°NULL**ï¼šå¦‚æœæŸä¸ªå­—æ®µåœ¨æ–‡æœ¬ä¸­æ²¡æœ‰æ˜ç¡®ä¿¡æ¯ï¼Œå¿…é¡»è®¾ç½®ä¸ºnull
3. **ç¦æ­¢æ•°æ®æ¨æµ‹**ï¼šä¸¥ç¦åŸºäºå…¶ä»–ä¿¡æ¯æ¨æµ‹ç¼ºå¤±å­—æ®µçš„å¯èƒ½å€¼
4. **æ•°å€¼ç²¾ç¡®æå–**ï¼šæ•°å€¼å‹æ•°æ®å¿…é¡»ä¸åŸæ–‡å®Œå…¨ä¸€è‡´ï¼Œä¸å¾—ä¿®æ”¹æˆ–"ä¼˜åŒ–"
5. **æ—¥æœŸæ ¼å¼ç»Ÿä¸€**ï¼šæ—¥æœŸæ ¼å¼ç»Ÿä¸€ä¸ºYYYY-MM-DDï¼Œæ— æ³•ç¡®å®šçš„æ—¥æœŸè®¾ä¸ºnull
6. **å¯ä¿¡åº¦æ ‡è®°**ï¼šä¸ºæ¯ä¸ªæå–çš„å­—æ®µæ ‡è®°ç½®ä¿¡åº¦ï¼ˆhigh/medium/lowï¼‰
7. **è¾“å‡ºæ ‡å‡†JSONæ ¼å¼**ï¼šåŒ…å«æ•°æ®å­—æ®µå’Œå…ƒæ•°æ®å­—æ®µ

åŒ»ç–—æ–‡æœ¬:
{medical_text}

è¯·è¾“å‡ºç»“æ„åŒ–JSON:
"""
        return template
```

#### 3. å¤§æ¨¡å‹å¤„ç†å™¨

```python
import openai
from typing import Dict, Any, Optional
import json
import asyncio

class MedicalDataProcessor:
    def __init__(self, api_key: str, model: str = "gpt-4", schema_loader: SchemaLoader = None):
        self.client = openai.OpenAI(api_key=api_key)
        self.model = model
        self.schema_loader = schema_loader

    async def process_medical_text(self, text: str, schema_name: str) -> Dict[str, Any]:
        """å¤„ç†å•æ¡åŒ»ç–—æ–‡æœ¬"""
        try:
            # ç”Ÿæˆæç¤ºè¯
            prompt = self.schema_loader.generate_prompt_template(schema_name)
            formatted_prompt = prompt.format(medical_text=text)

            # è°ƒç”¨å¤§æ¨¡å‹
            response = await self._call_llm(formatted_prompt)

            # è§£æJSONå“åº”
            structured_data = self._parse_llm_response(response, schema_name)

            # æ•°æ®éªŒè¯
            validated_data = self._validate_data(structured_data, schema_name)

            return {
                "success": True,
                "data": validated_data,
                "schema": schema_name,
                "processed_at": pd.Timestamp.now().isoformat()
            }

        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "schema": schema_name,
                "processed_at": pd.Timestamp.now().isoformat()
            }

    async def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹"""
        response = await asyncio.to_thread(
            self.client.chat.completions.create,
            model=self.model,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—æ•°æ®ç»“æ„åŒ–ä¸“å®¶ï¼Œä¸¥æ ¼æŒ‰ç…§è¦æ±‚è¾“å‡ºJSONæ ¼å¼ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            max_tokens=4000
        )
        return response.choices[0].message.content

    def _parse_llm_response(self, response: str, schema_name: str) -> Dict:
        """è§£æå¤§æ¨¡å‹è¿”å›çš„JSON"""
        try:
            # æå–JSONéƒ¨åˆ†
            start = response.find('{')
            end = response.rfind('}') + 1
            if start != -1 and end > start:
                json_str = response[start:end]
                return json.loads(json_str)
            else:
                raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONæ ¼å¼")
        except json.JSONDecodeError as e:
            raise ValueError(f"JSONè§£æé”™è¯¯: {e}")

    def _validate_data(self, data: Dict, schema_name: str) -> Dict:
        """éªŒè¯æ•°æ®æ ¼å¼å’Œç±»å‹"""
        schema = self.schema_loader.get_schema(schema_name)
        if not schema:
            return data

        validated = {}
        for field_name, field_type in schema['data_types'].items():
            value = data.get(field_name)
            if value is not None:
                try:
                    if field_type == 'Number':
                        validated[field_name] = float(value) if '.' in str(value) else int(value)
                    elif field_type == 'Boolean':
                        validated[field_name] = bool(value) if isinstance(value, bool) else str(value).lower() in ['true', '1', 'yes', 'æ˜¯']
                    else:  # String
                        validated[field_name] = str(value)
                except (ValueError, TypeError):
                    validated[field_name] = value  # ä¿æŒåŸå€¼
            else:
                validated[field_name] = None

        return validated
```

#### 4. å®æ—¶APIæœåŠ¡

```python
from fastapi import FastAPI, HTTPException, UploadFile, File
from pydantic import BaseModel
import uvicorn
import asyncio
from typing import Optional

app = FastAPI(title="åŒ»ç–—æ•°æ®ç»“æ„åŒ–API", version="1.0.0")

# å…¨å±€ç»„ä»¶
schema_loader = SchemaLoader()
processor = MedicalDataProcessor(
    api_key="your-openai-api-key",
    schema_loader=schema_loader
)

class ProcessRequest(BaseModel):
    text: str
    schema_name: str
    patient_id: Optional[str] = None

class ProcessResponse(BaseModel):
    success: bool
    data: Optional[dict] = None
    error: Optional[str] = None
    schema: str
    processed_at: str

@app.get("/schemas")
async def get_available_schemas():
    """è·å–æ‰€æœ‰å¯ç”¨çš„Schema"""
    return {
        "schemas": schema_loader.list_schemas(),
        "total": len(schema_loader.schemas)
    }

@app.post("/process", response_model=ProcessResponse)
async def process_medical_data(request: ProcessRequest):
    """å¤„ç†åŒ»ç–—æ–‡æœ¬æ•°æ®"""
    if request.schema_name not in schema_loader.schemas:
        raise HTTPException(status_code=400, detail=f"Schema '{request.schema_name}' ä¸å­˜åœ¨")

    result = await processor.process_medical_text(request.text, request.schema_name)
    return ProcessResponse(**result)

@app.post("/process-file")
async def process_file(file: UploadFile = File(...), schema_name: str = "diabetes-comprehensive"):
    """å¤„ç†ä¸Šä¼ çš„æ–‡æœ¬æ–‡ä»¶"""
    if file.content_type not in ["text/plain", "application/pdf"]:
        raise HTTPException(status_code=400, detail="ä»…æ”¯æŒæ–‡æœ¬æ–‡ä»¶å’ŒPDFæ–‡ä»¶")

    content = await file.read()
    text = content.decode('utf-8')

    result = await processor.process_medical_text(text, schema_name)
    return result

@app.get("/health")
async def health_check():
    return {"status": "healthy", "schemas_loaded": len(schema_loader.schemas)}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

#### 5. å®¢æˆ·ç«¯ä½¿ç”¨ç¤ºä¾‹

```python
import requests
import json

class MedicalDataClient:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url

    def get_schemas(self):
        """è·å–å¯ç”¨Schemaåˆ—è¡¨"""
        response = requests.get(f"{self.base_url}/schemas")
        return response.json()

    def process_text(self, text: str, schema_name: str, patient_id: str = None):
        """å¤„ç†åŒ»ç–—æ–‡æœ¬"""
        data = {
            "text": text,
            "schema_name": schema_name,
            "patient_id": patient_id
        }
        response = requests.post(f"{self.base_url}/process", json=data)
        return response.json()

    def process_file(self, file_path: str, schema_name: str):
        """å¤„ç†æ–‡ä»¶"""
        with open(file_path, 'rb') as f:
            files = {'file': f}
            params = {'schema_name': schema_name}
            response = requests.post(f"{self.base_url}/process-file", files=files, params=params)
        return response.json()

# ä½¿ç”¨ç¤ºä¾‹
client = MedicalDataClient()

# è·å–Schemaåˆ—è¡¨
schemas = client.get_schemas()
print("å¯ç”¨Schema:", schemas['schemas'])

# å¤„ç†åŒ»ç–—æ–‡æœ¬
medical_text = """
æ‚£è€…å¼ ä¸‰ï¼Œç”·ï¼Œ58å²ï¼Œç¡®è¯Š2å‹ç³–å°¿ç—…10å¹´ã€‚
ä»Šæ—¥é—¨è¯Šå¤æŸ¥ï¼šç©ºè…¹è¡€ç³–7.8mmol/Lï¼Œé¤å2å°æ—¶è¡€ç³–12.3mmol/Lï¼Œ
ç³–åŒ–è¡€çº¢è›‹ç™½8.2%ã€‚è¡€å‹145/88mmHgï¼Œä½“é‡75kgï¼Œèº«é«˜168cmã€‚
æ‚£è€…è¯‰è¿‘æœŸå¤šé¥®ã€å¤šå°¿ç—‡çŠ¶åŠ é‡ï¼Œä¹åŠ›æ˜æ˜¾ã€‚
è¶³éƒ¨æ£€æŸ¥æœªè§æ˜æ˜¾å¼‚å¸¸ã€‚
å»ºè®®è°ƒæ•´é™ç³–æ–¹æ¡ˆï¼ŒåŠ å¼ºè¡€ç³–ç›‘æµ‹ã€‚
"""

result = client.process_text(medical_text, "diabetes-comprehensive", "P202401001")
print("å¤„ç†ç»“æœ:", json.dumps(result, indent=2, ensure_ascii=False))
```

### æ–¹æ¡ˆäºŒï¼šæ‰¹é‡å¤„ç†ç³»ç»Ÿ

é€‚ç”¨äºå†å²æ•°æ®æ•´ç†ã€ç§‘ç ”æ•°æ®å¤„ç†ç­‰æ‰¹é‡åœºæ™¯ã€‚

#### 1. æ‰¹é‡å¤„ç†å™¨

```python
import pandas as pd
import asyncio
import aiofiles
from concurrent.futures import ThreadPoolExecutor
import logging
from tqdm.asyncio import tqdm
import json
from pathlib import Path

class BatchProcessor:
    def __init__(self, schema_loader: SchemaLoader, processor: MedicalDataProcessor,
                 batch_size: int = 10, max_workers: int = 5):
        self.schema_loader = schema_loader
        self.processor = processor
        self.batch_size = batch_size
        self.max_workers = max_workers
        self.setup_logging()

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('batch_processing.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    async def process_csv_file(self, input_file: str, output_dir: str,
                              text_column: str, schema_name: str,
                              patient_id_column: str = None):
        """å¤„ç†CSVæ–‡ä»¶ä¸­çš„åŒ»ç–—æ–‡æœ¬æ•°æ®"""
        self.logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {input_file}")

        # è¯»å–è¾“å…¥æ–‡ä»¶
        df = pd.read_csv(input_file)
        total_rows = len(df)
        self.logger.info(f"å…±{total_rows}æ¡è®°å½•éœ€è¦å¤„ç†")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        # å‡†å¤‡ç»“æœå­˜å‚¨
        results = []
        errors = []

        # åˆ†æ‰¹å¤„ç†
        semaphore = asyncio.Semaphore(self.max_workers)

        async def process_batch(batch_df):
            tasks = []
            async with semaphore:
                for idx, row in batch_df.iterrows():
                    text = row[text_column]
                    patient_id = row[patient_id_column] if patient_id_column else f"batch_{idx}"

                    task = self.process_single_record(text, schema_name, patient_id, idx)
                    tasks.append(task)

                return await asyncio.gather(*tasks, return_exceptions=True)

        # åˆ†æ‰¹æ‰§è¡Œ
        with tqdm(total=total_rows, desc="å¤„ç†è¿›åº¦") as pbar:
            for i in range(0, total_rows, self.batch_size):
                batch_df = df.iloc[i:i+self.batch_size]
                batch_results = await process_batch(batch_df)

                for result in batch_results:
                    if isinstance(result, Exception):
                        errors.append(str(result))
                    else:
                        results.append(result)

                pbar.update(len(batch_df))

        # ä¿å­˜ç»“æœ
        await self.save_results(results, errors, output_path, schema_name)

        self.logger.info(f"å¤„ç†å®Œæˆ: {len(results)}æˆåŠŸ, {len(errors)}å¤±è´¥")
        return {
            "total_processed": len(results) + len(errors),
            "successful": len(results),
            "failed": len(errors),
            "output_dir": str(output_path)
        }

    async def process_single_record(self, text: str, schema_name: str,
                                  patient_id: str, record_index: int):
        """å¤„ç†å•æ¡è®°å½•"""
        try:
            result = await self.processor.process_medical_text(text, schema_name)
            result['patient_id'] = patient_id
            result['record_index'] = record_index
            return result
        except Exception as e:
            self.logger.error(f"å¤„ç†è®°å½•{record_index}å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "patient_id": patient_id,
                "record_index": record_index
            }

    async def save_results(self, results: list, errors: list,
                          output_path: Path, schema_name: str):
        """ä¿å­˜å¤„ç†ç»“æœ"""
        # ä¿å­˜æˆåŠŸçš„ç»“æœ
        if results:
            successful_data = []
            for result in results:
                if result.get('success'):
                    data = result['data'].copy()
                    data['patient_id'] = result.get('patient_id')
                    data['processed_at'] = result.get('processed_at')
                    successful_data.append(data)

            if successful_data:
                df_results = pd.DataFrame(successful_data)
                output_file = output_path / f"{schema_name}_processed_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv"
                df_results.to_csv(output_file, index=False)
                self.logger.info(f"æˆåŠŸç»“æœå·²ä¿å­˜: {output_file}")

        # ä¿å­˜é”™è¯¯æ—¥å¿—
        if errors:
            error_file = output_path / f"errors_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.json"
            async with aiofiles.open(error_file, 'w') as f:
                await f.write(json.dumps(errors, indent=2, ensure_ascii=False))
            self.logger.info(f"é”™è¯¯æ—¥å¿—å·²ä¿å­˜: {error_file}")

    async def process_directory(self, input_dir: str, output_dir: str,
                               schema_name: str, file_pattern: str = "*.txt"):
        """æ‰¹é‡å¤„ç†ç›®å½•ä¸­çš„æ–‡æœ¬æ–‡ä»¶"""
        input_path = Path(input_dir)
        files = list(input_path.glob(file_pattern))

        self.logger.info(f"æ‰¾åˆ°{len(files)}ä¸ªæ–‡ä»¶éœ€è¦å¤„ç†")

        all_results = []
        for file_path in tqdm(files, desc="å¤„ç†æ–‡ä»¶"):
            try:
                async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                    content = await f.read()

                result = await self.processor.process_medical_text(content, schema_name)
                result['source_file'] = str(file_path)
                all_results.append(result)

            except Exception as e:
                self.logger.error(f"å¤„ç†æ–‡ä»¶{file_path}å¤±è´¥: {e}")
                all_results.append({
                    "success": False,
                    "error": str(e),
                    "source_file": str(file_path)
                })

        # ä¿å­˜æ±‡æ€»ç»“æœ
        output_path = Path(output_dir)
        await self.save_directory_results(all_results, output_path, schema_name)

        successful = sum(1 for r in all_results if r.get('success'))
        failed = len(all_results) - successful

        return {
            "total_files": len(files),
            "successful": successful,
            "failed": failed,
            "output_dir": str(output_path)
        }

    async def save_directory_results(self, results: list, output_path: Path, schema_name: str):
        """ä¿å­˜ç›®å½•å¤„ç†ç»“æœ"""
        output_path.mkdir(exist_ok=True)

        successful_data = []
        failed_data = []

        for result in results:
            if result.get('success'):
                data = result['data'].copy()
                data['source_file'] = result.get('source_file')
                data['processed_at'] = result.get('processed_at')
                successful_data.append(data)
            else:
                failed_data.append({
                    'source_file': result.get('source_file'),
                    'error': result.get('error'),
                    'processed_at': result.get('processed_at', pd.Timestamp.now().isoformat())
                })

        # ä¿å­˜æˆåŠŸç»“æœ
        if successful_data:
            df_success = pd.DataFrame(successful_data)
            success_file = output_path / f"{schema_name}_directory_results_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv"
            df_success.to_csv(success_file, index=False)
            self.logger.info(f"ç›®å½•å¤„ç†æˆåŠŸç»“æœ: {success_file}")

        # ä¿å­˜å¤±è´¥ç»“æœ
        if failed_data:
            df_failed = pd.DataFrame(failed_data)
            failed_file = output_path / f"failed_files_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv"
            df_failed.to_csv(failed_file, index=False)
            self.logger.info(f"ç›®å½•å¤„ç†å¤±è´¥ç»“æœ: {failed_file}")
```

#### 2. æ‰¹é‡å¤„ç†è„šæœ¬

```python
import asyncio
import argparse
from pathlib import Path

async def main():
    parser = argparse.ArgumentParser(description='åŒ»ç–—æ•°æ®æ‰¹é‡ç»“æ„åŒ–å¤„ç†')
    parser.add_argument('--input', required=True, help='è¾“å…¥æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„')
    parser.add_argument('--output', required=True, help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--schema', required=True, help='ä½¿ç”¨çš„Schemaåç§°')
    parser.add_argument('--mode', choices=['csv', 'directory'], default='csv', help='å¤„ç†æ¨¡å¼')
    parser.add_argument('--text-column', default='text', help='CSVæ–‡ä»¶ä¸­çš„æ–‡æœ¬åˆ—å')
    parser.add_argument('--patient-id-column', help='CSVæ–‡ä»¶ä¸­çš„æ‚£è€…IDåˆ—å')
    parser.add_argument('--batch-size', type=int, default=10, help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--max-workers', type=int, default=5, help='æœ€å¤§å¹¶å‘æ•°')
    parser.add_argument('--api-key', required=True, help='OpenAI APIå¯†é’¥')

    args = parser.parse_args()

    # åˆå§‹åŒ–ç»„ä»¶
    schema_loader = SchemaLoader()
    processor = MedicalDataProcessor(api_key=args.api_key, schema_loader=schema_loader)
    batch_processor = BatchProcessor(
        schema_loader=schema_loader,
        processor=processor,
        batch_size=args.batch_size,
        max_workers=args.max_workers
    )

    # éªŒè¯Schema
    if args.schema not in schema_loader.schemas:
        print(f"é”™è¯¯: Schema '{args.schema}' ä¸å­˜åœ¨")
        print(f"å¯ç”¨Schema: {schema_loader.list_schemas()}")
        return

    # æ‰§è¡Œå¤„ç†
    if args.mode == 'csv':
        result = await batch_processor.process_csv_file(
            input_file=args.input,
            output_dir=args.output,
            text_column=args.text_column,
            schema_name=args.schema,
            patient_id_column=args.patient_id_column
        )
    else:
        result = await batch_processor.process_directory(
            input_dir=args.input,
            output_dir=args.output,
            schema_name=args.schema
        )

    print("æ‰¹é‡å¤„ç†å®Œæˆ:")
    print(f"  æ€»è®¡å¤„ç†: {result.get('total_processed', result.get('total_files'))}")
    print(f"  æˆåŠŸ: {result['successful']}")
    print(f"  å¤±è´¥: {result['failed']}")
    print(f"  è¾“å‡ºç›®å½•: {result['output_dir']}")

if __name__ == "__main__":
    asyncio.run(main())
```

#### 3. æ‰¹é‡å¤„ç†ä½¿ç”¨ç¤ºä¾‹

```bash
# å¤„ç†CSVæ–‡ä»¶
python batch_process.py \
  --input ./data/medical_records.csv \
  --output ./processed_data/ \
  --schema diabetes-comprehensive \
  --mode csv \
  --text-column medical_text \
  --patient-id-column patient_id \
  --batch-size 20 \
  --max-workers 5 \
  --api-key your-openai-api-key

# å¤„ç†æ–‡æœ¬æ–‡ä»¶ç›®å½•
python batch_process.py \
  --input ./raw_texts/ \
  --output ./structured_data/ \
  --schema diabetes-nursing-assessment \
  --mode directory \
  --batch-size 10 \
  --api-key your-openai-api-key
```

## ğŸ›¡ï¸ æ•°æ®å®Œæ•´æ€§å’Œä¸¥è°¨æ€§æ§åˆ¶

### æ ¸å¿ƒåŸåˆ™ï¼šåŒ»ç–—æ•°æ®é›¶å®¹å¿ç­–ç•¥

åŒ»ç–—æ•°æ®çš„å‡†ç¡®æ€§ç›´æ¥å…³ç³»æ‚£è€…å®‰å…¨ï¼Œå› æ­¤å¿…é¡»å»ºç«‹ä¸¥æ ¼çš„æ•°æ®å®Œæ•´æ€§æ§åˆ¶æœºåˆ¶ï¼Œé˜²æ­¢å¤§æ¨¡å‹äº§ç”Ÿ"å¹»æƒ³"æ•°æ®ã€‚

#### 1. é˜²å¹»æƒ³æ•°æ®æœºåˆ¶

```python
class AntiHallucinationValidator:
    def __init__(self, schema_loader: SchemaLoader):
        self.schema_loader = schema_loader

    def create_strict_prompt(self, schema_name: str) -> str:
        """åˆ›å»ºé˜²å¹»æƒ³çš„ä¸¥æ ¼æç¤ºè¯"""
        schema = self.schema_loader.get_schema(schema_name)

        strict_prompt = f"""
ä½ æ˜¯åŒ»ç–—æ•°æ®æå–ä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è§„åˆ™æå–ä¿¡æ¯ï¼š

ğŸš« **ç»å¯¹ç¦æ­¢çš„è¡Œä¸º**ï¼š
- æ¨æµ‹ã€ä¼°ç®—æˆ–ç¼–é€ ä»»ä½•æ•°æ®
- åŸºäºå¸¸è¯†æˆ–ç»éªŒå¡«è¡¥ç¼ºå¤±ä¿¡æ¯
- å°†ç›¸ä¼¼æ¦‚å¿µçš„æ•°æ®ç”¨äºå…¶ä»–å­—æ®µ
- è¿›è¡Œä»»ä½•å½¢å¼çš„æ•°æ®"ä¼˜åŒ–"æˆ–"æ ‡å‡†åŒ–"

âœ… **å¿…é¡»éµå¾ªçš„è§„åˆ™**ï¼š
- åªæå–æ–‡æœ¬ä¸­æ˜ç¡®å­˜åœ¨çš„ä¿¡æ¯
- ç¼ºå¤±çš„å­—æ®µå¿…é¡»æ ‡è®°ä¸º null
- æ•°å€¼å¿…é¡»ä¸åŸæ–‡å®Œå…¨ä¸€è‡´
- ä¸ç¡®å®šçš„ä¿¡æ¯æ ‡è®°ä½ç½®ä¿¡åº¦

Schemaå­—æ®µå®šä¹‰ï¼š
{self._format_schema_for_strict_extraction(schema)}

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
```json
{{
  "extracted_data": {{
    // ä»…åŒ…å«ä»æ–‡æœ¬ä¸­æ˜ç¡®æå–çš„å­—æ®µ
  }},
  "metadata": {{
    "extraction_confidence": "high|medium|low",
    "null_fields": ["field1", "field2"],  // æ˜ç¡®æ ‡è®°ç¼ºå¤±å­—æ®µ
    "uncertain_fields": [{{
      "field": "fieldName",
      "reason": "åŸå› æè¿°",
      "confidence": "low"
    }}],
    "source_references": {{
      "field": "åŸæ–‡å¼•ç”¨ç‰‡æ®µ"
    }}
  }}
}}
```

åŒ»ç–—æ–‡æœ¬ï¼š
{{medical_text}}
"""
        return strict_prompt

    def _format_schema_for_strict_extraction(self, schema: Dict) -> str:
        """æ ¼å¼åŒ–Schemaä»¥å¼ºè°ƒä¸¥æ ¼æå–"""
        formatted = ""
        for field in schema['fields'][:15]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
            formatted += f"- {field['fieldName']} ({field['fieldNameCN']}): {field['description']}\n"
            formatted += f"  ç±»å‹: {field['dataType']} | ç¤ºä¾‹: {field['examples']}\n"
            formatted += f"  âš ï¸ ä»…åœ¨æ–‡æœ¬æ˜ç¡®æåŠæ—¶å¡«å†™ï¼Œå¦åˆ™è®¾ä¸º null\n\n"
        return formatted

    def validate_against_source(self, extracted_data: Dict, source_text: str,
                               schema_name: str) -> Dict:
        """éªŒè¯æå–æ•°æ®ä¸åŸæ–‡çš„ä¸€è‡´æ€§"""
        validation_result = {
            "is_valid": True,
            "concerns": [],
            "null_fields": [],
            "confidence_issues": []
        }

        # æ£€æŸ¥nullå­—æ®µ
        for field_name, value in extracted_data.get('extracted_data', {}).items():
            if value is None:
                validation_result["null_fields"].append(field_name)

        # æ£€æŸ¥æ•°å€¼ä¸€è‡´æ€§
        numeric_fields = self._extract_numbers_from_text(source_text)
        for field_name, value in extracted_data.get('extracted_data', {}).items():
            if isinstance(value, (int, float)):
                if not self._number_exists_in_source(value, numeric_fields):
                    validation_result["concerns"].append({
                        "field": field_name,
                        "issue": f"æ•°å€¼ {value} åœ¨åŸæ–‡ä¸­æœªæ‰¾åˆ°å¯¹åº”",
                        "severity": "high"
                    })
                    validation_result["is_valid"] = False

        return validation_result

    def _extract_numbers_from_text(self, text: str) -> List[float]:
        """ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰æ•°å­—"""
        import re
        numbers = re.findall(r'\d+\.?\d*', text)
        return [float(n) for n in numbers]

    def _number_exists_in_source(self, value: float, source_numbers: List[float]) -> bool:
        """æ£€æŸ¥æ•°å€¼æ˜¯å¦å­˜åœ¨äºåŸæ–‡ä¸­"""
        tolerance = 0.001  # æµ®ç‚¹æ•°è¯¯å·®å®¹å¿
        return any(abs(value - num) < tolerance for num in source_numbers)
```

#### 2. å¼ºåˆ¶NULLæ ‡è®°ç³»ç»Ÿ

```python
class NullFieldController:
    def __init__(self, schema_loader: SchemaLoader):
        self.schema_loader = schema_loader

    def enforce_null_completeness(self, data: Dict, schema_name: str) -> Dict:
        """å¼ºåˆ¶ç¡®ä¿æ‰€æœ‰Schemaå­—æ®µéƒ½æœ‰å€¼ï¼ˆåŒ…æ‹¬nullï¼‰"""
        schema = self.schema_loader.get_schema(schema_name)
        if not schema:
            return data

        complete_data = {}
        null_fields = []

        # ç¡®ä¿æ¯ä¸ªSchemaå­—æ®µéƒ½å­˜åœ¨
        for field_name in schema['field_names']:
            if field_name in data:
                complete_data[field_name] = data[field_name]
            else:
                complete_data[field_name] = None
                null_fields.append(field_name)

        # ç§»é™¤Schemaä¸­ä¸å­˜åœ¨çš„å­—æ®µï¼ˆé˜²æ­¢å¹»æƒ³å­—æ®µï¼‰
        extra_fields = set(data.keys()) - set(schema['field_names'])
        if extra_fields:
            print(f"âš ï¸ è­¦å‘Šï¼šå‘ç°Schemaå¤–å­—æ®µï¼Œå·²ç§»é™¤: {extra_fields}")

        return {
            "data": complete_data,
            "metadata": {
                "null_fields": null_fields,
                "null_percentage": len(null_fields) / len(schema['field_names']) * 100,
                "extra_fields_removed": list(extra_fields)
            }
        }

    def create_null_audit_report(self, processed_data: List[Dict],
                                schema_name: str) -> Dict:
        """åˆ›å»ºNULLå­—æ®µå®¡è®¡æŠ¥å‘Š"""
        schema = self.schema_loader.get_schema(schema_name)
        field_null_counts = {field: 0 for field in schema['field_names']}
        total_records = len(processed_data)

        for record in processed_data:
            for field_name, value in record.get('data', {}).items():
                if value is None:
                    field_null_counts[field_name] += 1

        # è®¡ç®—ç¼ºå¤±ç‡
        null_rates = {
            field: (count / total_records * 100)
            for field, count in field_null_counts.items()
        }

        # è¯†åˆ«é«˜ç¼ºå¤±ç‡å­—æ®µ
        high_null_fields = [
            field for field, rate in null_rates.items()
            if rate > 80  # è¶…è¿‡80%ç¼ºå¤±
        ]

        return {
            "total_records": total_records,
            "field_null_rates": null_rates,
            "high_null_fields": high_null_fields,
            "average_null_rate": sum(null_rates.values()) / len(null_rates),
            "data_completeness_score": 100 - (sum(null_rates.values()) / len(null_rates))
        }
```

#### 3. å¯ä¿¡åº¦è¯„ä¼°æœºåˆ¶

```python
class ConfidenceAssessor:
    def __init__(self):
        self.confidence_rules = {
            'high': {
                'numeric_exact_match': True,
                'clear_terminology': True,
                'complete_context': True
            },
            'medium': {
                'numeric_approximate': True,
                'partial_context': True,
                'standard_terminology': True
            },
            'low': {
                'ambiguous_wording': True,
                'incomplete_information': True,
                'requires_inference': True
            }
        }

    def assess_field_confidence(self, field_name: str, extracted_value: Any,
                              source_text: str, context: str) -> Dict:
        """è¯„ä¼°å•ä¸ªå­—æ®µçš„æå–ç½®ä¿¡åº¦"""
        confidence_factors = {
            'source_clarity': self._assess_source_clarity(extracted_value, source_text),
            'context_completeness': self._assess_context_completeness(context),
            'terminology_precision': self._assess_terminology_precision(field_name, source_text),
            'numeric_precision': self._assess_numeric_precision(extracted_value, source_text)
        }

        # ç»¼åˆè®¡ç®—ç½®ä¿¡åº¦
        overall_confidence = self._calculate_overall_confidence(confidence_factors)

        return {
            "field": field_name,
            "confidence_level": overall_confidence,
            "confidence_score": self._get_confidence_score(confidence_factors),
            "factors": confidence_factors,
            "recommendation": self._get_confidence_recommendation(overall_confidence)
        }

    def _assess_source_clarity(self, value: Any, source_text: str) -> float:
        """è¯„ä¼°åŸæ–‡è¡¨è¿°çš„æ¸…æ™°åº¦"""
        if value is None:
            return 1.0  # NULLå€¼æ˜¯æ˜ç¡®çš„

        # æ£€æŸ¥æ•°å€¼æ˜¯å¦åœ¨åŸæ–‡ä¸­æ˜ç¡®å­˜åœ¨
        if isinstance(value, (int, float)):
            if str(value) in source_text:
                return 1.0
            else:
                return 0.3  # æ•°å€¼ä¸åŒ¹é…ï¼Œä½ç½®ä¿¡åº¦

        # æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ç›´æ¥æ¥æºäºåŸæ–‡
        if isinstance(value, str) and value.lower() in source_text.lower():
            return 0.9

        return 0.5

    def _get_confidence_recommendation(self, confidence: str) -> str:
        """è·å–ç½®ä¿¡åº¦å»ºè®®"""
        recommendations = {
            'high': "æ•°æ®å¯ç›´æ¥ä½¿ç”¨",
            'medium': "å»ºè®®äººå·¥å¤æ ¸",
            'low': "éœ€è¦äººå·¥éªŒè¯æˆ–é‡æ–°æå–"
        }
        return recommendations.get(confidence, "æœªçŸ¥ç½®ä¿¡åº¦")
```

#### 4. äººå·¥å®¡æ ¸æ¥å£

```python
class HumanReviewInterface:
    def __init__(self, confidence_assessor: ConfidenceAssessor):
        self.confidence_assessor = confidence_assessor

    def generate_review_queue(self, processed_data: List[Dict],
                            min_confidence: str = 'medium') -> List[Dict]:
        """ç”Ÿæˆéœ€è¦äººå·¥å®¡æ ¸çš„æ•°æ®é˜Ÿåˆ—"""
        confidence_order = {'low': 0, 'medium': 1, 'high': 2}
        min_level = confidence_order[min_confidence]

        review_queue = []
        for record in processed_data:
            metadata = record.get('metadata', {})

            # æ£€æŸ¥æ˜¯å¦æœ‰ä½ç½®ä¿¡åº¦å­—æ®µ
            uncertain_fields = metadata.get('uncertain_fields', [])
            needs_review = any(
                confidence_order.get(field.get('confidence', 'low'), 0) <= min_level
                for field in uncertain_fields
            )

            if needs_review or len(metadata.get('null_fields', [])) > 0.5 * len(record.get('data', {})):
                review_item = {
                    "record_id": record.get('patient_id', 'unknown'),
                    "review_priority": self._calculate_review_priority(metadata),
                    "issues": self._summarize_issues(metadata),
                    "data": record['data'],
                    "source_text": record.get('source_text', ''),
                    "metadata": metadata
                }
                review_queue.append(review_item)

        # æŒ‰ä¼˜å…ˆçº§æ’åº
        review_queue.sort(key=lambda x: x['review_priority'], reverse=True)
        return review_queue

    def _calculate_review_priority(self, metadata: Dict) -> int:
        """è®¡ç®—å®¡æ ¸ä¼˜å…ˆçº§ï¼ˆ1-10ï¼Œ10æœ€é«˜ï¼‰"""
        priority = 5  # åŸºç¡€ä¼˜å…ˆçº§

        # é«˜ç¼ºå¤±ç‡å¢åŠ ä¼˜å…ˆçº§
        null_percentage = metadata.get('null_percentage', 0)
        if null_percentage > 70:
            priority += 3
        elif null_percentage > 50:
            priority += 2

        # ä½ç½®ä¿¡åº¦å­—æ®µå¢åŠ ä¼˜å…ˆçº§
        uncertain_fields = metadata.get('uncertain_fields', [])
        low_confidence_count = sum(1 for f in uncertain_fields if f.get('confidence') == 'low')
        priority += min(low_confidence_count, 3)

        return min(priority, 10)

    def create_review_form(self, review_item: Dict) -> str:
        """åˆ›å»ºäººå·¥å®¡æ ¸è¡¨å•ï¼ˆHTMLæ ¼å¼ï¼‰"""
        html_form = f"""
        <div class="review-form">
            <h3>æ•°æ®å®¡æ ¸ - è®°å½•ID: {review_item['record_id']}</h3>
            <div class="priority">å®¡æ ¸ä¼˜å…ˆçº§: {review_item['review_priority']}/10</div>

            <div class="source-text">
                <h4>åŸå§‹æ–‡æœ¬:</h4>
                <pre>{review_item['source_text']}</pre>
            </div>

            <div class="extracted-data">
                <h4>æå–çš„æ•°æ®:</h4>
                <table border="1">
                    <tr><th>å­—æ®µ</th><th>æå–å€¼</th><th>ç½®ä¿¡åº¦</th><th>å®¡æ ¸ç»“æœ</th></tr>
        """

        for field, value in review_item['data'].items():
            confidence = self._get_field_confidence(field, review_item['metadata'])
            html_form += f"""
                    <tr>
                        <td>{field}</td>
                        <td>{value if value is not None else 'NULL'}</td>
                        <td>{confidence}</td>
                        <td>
                            <select name="review_{field}">
                                <option value="approve">æ‰¹å‡†</option>
                                <option value="modify">ä¿®æ”¹</option>
                                <option value="reject">æ‹’ç»</option>
                            </select>
                            <input type="text" name="modify_{field}" placeholder="ä¿®æ”¹å€¼">
                        </td>
                    </tr>
            """

        html_form += """
                </table>
            </div>

            <div class="issues">
                <h4>è¯†åˆ«çš„é—®é¢˜:</h4>
                <ul>
        """

        for issue in review_item['issues']:
            html_form += f"<li>{issue}</li>"

        html_form += """
                </ul>
            </div>

            <div class="actions">
                <button onclick="submitReview()">æäº¤å®¡æ ¸</button>
                <button onclick="requestReprocessing()">é‡æ–°å¤„ç†</button>
            </div>
        </div>
        """

        return html_form
```

#### 5. æ•°æ®æº¯æºå’Œå®¡è®¡

```python
class DataLineageTracker:
    def __init__(self):
        self.lineage_db = {}  # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­åº”ä½¿ç”¨æ•°æ®åº“

    def track_extraction(self, record_id: str, source_text: str,
                        extracted_data: Dict, metadata: Dict) -> str:
        """è·Ÿè¸ªæ•°æ®æå–è¿‡ç¨‹"""
        lineage_id = f"lineage_{record_id}_{int(time.time())}"

        lineage_record = {
            "lineage_id": lineage_id,
            "record_id": record_id,
            "timestamp": pd.Timestamp.now().isoformat(),
            "source_text_hash": hashlib.md5(source_text.encode()).hexdigest(),
            "source_text_length": len(source_text),
            "extracted_fields": list(extracted_data.keys()),
            "null_fields": [k for k, v in extracted_data.items() if v is None],
            "processing_metadata": metadata,
            "extraction_method": "llm_with_schema",
            "schema_version": "v4.0"
        }

        self.lineage_db[lineage_id] = lineage_record
        return lineage_id

    def create_audit_trail(self, lineage_id: str) -> Dict:
        """åˆ›å»ºå®¡è®¡è·Ÿè¸ªè®°å½•"""
        if lineage_id not in self.lineage_db:
            return {"error": "Lineage record not found"}

        lineage = self.lineage_db[lineage_id]

        audit_trail = {
            "extraction_summary": {
                "total_fields": len(lineage["extracted_fields"]),
                "null_fields": len(lineage["null_fields"]),
                "data_completeness": 1 - (len(lineage["null_fields"]) / len(lineage["extracted_fields"])),
                "processing_timestamp": lineage["timestamp"]
            },
            "data_quality_flags": self._generate_quality_flags(lineage),
            "compliance_status": self._check_compliance(lineage),
            "recommendation": self._generate_audit_recommendation(lineage)
        }

        return audit_trail

    def _generate_quality_flags(self, lineage: Dict) -> List[str]:
        """ç”Ÿæˆæ•°æ®è´¨é‡æ ‡è®°"""
        flags = []

        null_ratio = len(lineage["null_fields"]) / len(lineage["extracted_fields"])
        if null_ratio > 0.7:
            flags.append("HIGH_NULL_RATIO")

        if len(lineage["source_text"]) < 100:
            flags.append("SHORT_SOURCE_TEXT")

        return flags

    def _check_compliance(self, lineage: Dict) -> str:
        """æ£€æŸ¥åˆè§„æ€§"""
        flags = self._generate_quality_flags(lineage)

        if "HIGH_NULL_RATIO" in flags:
            return "NEEDS_REVIEW"
        elif len(flags) > 0:
            return "CAUTION"
        else:
            return "COMPLIANT"
```

#### 6. é…ç½®ä¸¥æ ¼æ¨¡å¼

```python
class StrictModeConfig:
    """ä¸¥æ ¼æ¨¡å¼é…ç½®ï¼Œç¡®ä¿åŒ»ç–—æ•°æ®å‡†ç¡®æ€§"""

    STRICT_MODE_SETTINGS = {
        "allow_inference": False,  # ç¦æ­¢æ¨æµ‹
        "allow_approximation": False,  # ç¦æ­¢è¿‘ä¼¼
        "require_source_reference": True,  # è¦æ±‚æºæ–‡æœ¬å¼•ç”¨
        "mandatory_null_marking": True,  # å¼ºåˆ¶NULLæ ‡è®°
        "confidence_threshold": 0.8,  # ç½®ä¿¡åº¦é˜ˆå€¼
        "auto_reject_low_confidence": True,  # è‡ªåŠ¨æ‹’ç»ä½ç½®ä¿¡åº¦
        "human_review_required": ["low", "medium"],  # éœ€è¦äººå·¥å®¡æ ¸çš„ç½®ä¿¡åº¦
        "audit_trail_enabled": True,  # å¯ç”¨å®¡è®¡è·Ÿè¸ª
    }

    @classmethod
    def get_strict_prompt_prefix(cls) -> str:
        return """
ğŸ”’ **åŒ»ç–—æ•°æ®ä¸¥æ ¼æ¨¡å¼å·²å¯ç”¨**

æ ¸å¿ƒåŸåˆ™ï¼š
- å‡†ç¡®æ€§ä¼˜äºå®Œæ•´æ€§
- NULLä¼˜äºçŒœæµ‹
- å¯éªŒè¯æ€§ä¼˜äºå¯è¯»æ€§
- å®¡æ…ä¼˜äºæ•ˆç‡

åœ¨æ­¤æ¨¡å¼ä¸‹ï¼Œä»»ä½•ä¸ç¡®å®šçš„ä¿¡æ¯éƒ½å¿…é¡»æ ‡è®°ä¸ºNULLæˆ–ä½ç½®ä¿¡åº¦ã€‚
        """

    @classmethod
    def validate_strict_compliance(cls, result: Dict) -> Dict:
        """éªŒè¯ä¸¥æ ¼æ¨¡å¼åˆè§„æ€§"""
        compliance_issues = []

        # æ£€æŸ¥æ˜¯å¦æœ‰æ¨æµ‹æ•°æ®
        metadata = result.get('metadata', {})
        if metadata.get('contains_inference', False):
            compliance_issues.append("åŒ…å«æ¨æµ‹æ•°æ®ï¼Œè¿åä¸¥æ ¼æ¨¡å¼")

        # æ£€æŸ¥ç½®ä¿¡åº¦
        uncertain_fields = metadata.get('uncertain_fields', [])
        low_confidence_fields = [f for f in uncertain_fields if f.get('confidence') == 'low']
        if low_confidence_fields and cls.STRICT_MODE_SETTINGS['auto_reject_low_confidence']:
            compliance_issues.append(f"å­˜åœ¨ä½ç½®ä¿¡åº¦å­—æ®µ: {[f['field'] for f in low_confidence_fields]}")

        return {
            "compliant": len(compliance_issues) == 0,
            "issues": compliance_issues,
            "compliance_score": max(0, 1 - len(compliance_issues) * 0.2)
        }
```

### ä½¿ç”¨ç¤ºä¾‹ï¼šå¯ç”¨ä¸¥æ ¼æ¨¡å¼

```python
# åˆå§‹åŒ–ä¸¥æ ¼æ¨¡å¼ç»„ä»¶
anti_hallucination = AntiHallucinationValidator(schema_loader)
null_controller = NullFieldController(schema_loader)
confidence_assessor = ConfidenceAssessor()
lineage_tracker = DataLineageTracker()

# ä½¿ç”¨ä¸¥æ ¼æ¨¡å¼å¤„ç†
async def process_with_strict_mode(text: str, schema_name: str, patient_id: str):
    # 1. ä½¿ç”¨é˜²å¹»æƒ³æç¤ºè¯
    strict_prompt = anti_hallucination.create_strict_prompt(schema_name)

    # 2. è°ƒç”¨å¤§æ¨¡å‹ï¼ˆä½¿ç”¨æ›´ä¿å®ˆçš„å‚æ•°ï¼‰
    response = await processor._call_llm(strict_prompt.format(medical_text=text))

    # 3. è§£æå¹¶éªŒè¯
    parsed_result = processor._parse_llm_response(response, schema_name)

    # 4. å¼ºåˆ¶NULLå®Œæ•´æ€§
    complete_result = null_controller.enforce_null_completeness(
        parsed_result.get('extracted_data', {}), schema_name
    )

    # 5. éªŒè¯ä¸åŸæ–‡ä¸€è‡´æ€§
    validation = anti_hallucination.validate_against_source(
        parsed_result, text, schema_name
    )

    # 6. è¯„ä¼°ç½®ä¿¡åº¦
    confidence_assessment = confidence_assessor.assess_field_confidence(
        "overall", complete_result['data'], text, ""
    )

    # 7. æ£€æŸ¥ä¸¥æ ¼æ¨¡å¼åˆè§„æ€§
    compliance = StrictModeConfig.validate_strict_compliance({
        'data': complete_result['data'],
        'metadata': {**complete_result['metadata'], **parsed_result.get('metadata', {})}
    })

    # 8. è®°å½•æ•°æ®æº¯æº
    lineage_id = lineage_tracker.track_extraction(
        patient_id, text, complete_result['data'], complete_result['metadata']
    )

    return {
        "success": compliance['compliant'],
        "data": complete_result['data'] if compliance['compliant'] else None,
        "metadata": {
            **complete_result['metadata'],
            "confidence_assessment": confidence_assessment,
            "validation_result": validation,
            "compliance_check": compliance,
            "lineage_id": lineage_id,
            "strict_mode": True
        },
        "requires_human_review": not compliance['compliant'] or
                                confidence_assessment['confidence_level'] in ['low', 'medium']
    }
```

è¿™å¥—ä¸¥æ ¼çš„æ•°æ®å®Œæ•´æ€§æ§åˆ¶æœºåˆ¶ç¡®ä¿ï¼š
1. **é›¶å®¹å¿å¹»æƒ³æ•°æ®**ï¼šä»»ä½•ä¸åœ¨åŸæ–‡ä¸­çš„ä¿¡æ¯éƒ½ä¸ä¼šè¢«å¡«å…¥
2. **å¼ºåˆ¶NULLæ ‡è®°**ï¼šç¼ºå¤±çš„å­—æ®µæ˜ç¡®æ ‡è®°ä¸ºNULL
3. **å¯ä¿¡åº¦è¯„ä¼°**ï¼šæ¯ä¸ªå­—æ®µéƒ½æœ‰ç½®ä¿¡åº¦è¯„åˆ†
4. **äººå·¥å®¡æ ¸æœºåˆ¶**ï¼šä½ç½®ä¿¡åº¦æ•°æ®å¿…é¡»äººå·¥éªŒè¯
5. **å®Œæ•´å®¡è®¡è·Ÿè¸ª**ï¼šæ‰€æœ‰æå–è¿‡ç¨‹å¯è¿½æº¯
6. **åˆè§„æ€§æ£€æŸ¥**ï¼šä¸¥æ ¼æ¨¡å¼åˆè§„æ€§éªŒè¯

## ğŸ” è´¨é‡æ§åˆ¶å’ŒéªŒè¯

### 1. æ•°æ®è´¨é‡æ£€æŸ¥å™¨

```python
class QualityController:
    def __init__(self, schema_loader: SchemaLoader):
        self.schema_loader = schema_loader

    def validate_completeness(self, data: Dict, schema_name: str,
                            required_fields: List[str] = None) -> Dict:
        """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§"""
        schema = self.schema_loader.get_schema(schema_name)
        if not schema:
            return {"valid": False, "error": "Schema not found"}

        # é»˜è®¤å¿…å¡«å­—æ®µ
        if not required_fields:
            required_fields = ['examDate', 'patientID', 'patientName']

        missing_fields = []
        for field in required_fields:
            if field not in data or data[field] is None or data[field] == "":
                missing_fields.append(field)

        return {
            "valid": len(missing_fields) == 0,
            "missing_fields": missing_fields,
            "completeness_score": 1 - (len(missing_fields) / len(required_fields))
        }

    def validate_data_types(self, data: Dict, schema_name: str) -> Dict:
        """éªŒè¯æ•°æ®ç±»å‹"""
        schema = self.schema_loader.get_schema(schema_name)
        if not schema:
            return {"valid": False, "error": "Schema not found"}

        type_errors = []
        for field_name, expected_type in schema['data_types'].items():
            if field_name in data and data[field_name] is not None:
                value = data[field_name]
                if expected_type == 'Number':
                    if not isinstance(value, (int, float)):
                        try:
                            float(value)
                        except ValueError:
                            type_errors.append(f"{field_name}: æœŸæœ›æ•°å€¼å‹ï¼Œå®é™…ä¸º {type(value).__name__}")
                elif expected_type == 'Boolean':
                    if not isinstance(value, bool) and str(value).lower() not in ['true', 'false', '0', '1']:
                        type_errors.append(f"{field_name}: æœŸæœ›å¸ƒå°”å‹ï¼Œå®é™…ä¸º {type(value).__name__}")

        return {
            "valid": len(type_errors) == 0,
            "type_errors": type_errors,
            "type_accuracy": 1 - (len(type_errors) / len(schema['data_types']))
        }

    def validate_value_ranges(self, data: Dict, schema_name: str) -> Dict:
        """éªŒè¯æ•°å€¼èŒƒå›´ï¼ˆåŸºäºåŒ»å­¦å¸¸è¯†ï¼‰"""
        range_errors = []

        # å®šä¹‰å¸¸è§åŒ»å­¦æŒ‡æ ‡çš„åˆç†èŒƒå›´
        medical_ranges = {
            'systolicBP': (50, 250),
            'diastolicBP': (30, 150),
            'heartRate': (30, 200),
            'bodyTemperature': (30, 45),
            'hbA1cLevel': (3, 20),
            'fastingGlucose': (1, 50),
            'age': (0, 150),
            'weight': (0.5, 300),
            'height': (30, 250)
        }

        for field, (min_val, max_val) in medical_ranges.items():
            if field in data and data[field] is not None:
                try:
                    value = float(data[field])
                    if not (min_val <= value <= max_val):
                        range_errors.append(f"{field}: å€¼{value}è¶…å‡ºåˆç†èŒƒå›´[{min_val}, {max_val}]")
                except (ValueError, TypeError):
                    continue

        return {
            "valid": len(range_errors) == 0,
            "range_errors": range_errors
        }
```

### 2. ç»“æœéªŒè¯å’Œä¿®æ­£

```python
class ResultValidator:
    def __init__(self, quality_controller: QualityController):
        self.qc = quality_controller

    def comprehensive_validation(self, data: Dict, schema_name: str) -> Dict:
        """ç»¼åˆéªŒè¯"""
        results = {
            "completeness": self.qc.validate_completeness(data, schema_name),
            "data_types": self.qc.validate_data_types(data, schema_name),
            "value_ranges": self.qc.validate_value_ranges(data, schema_name)
        }

        # è®¡ç®—æ€»ä½“è´¨é‡åˆ†æ•°
        total_score = (
            results["completeness"]["completeness_score"] * 0.4 +
            results["data_types"]["type_accuracy"] * 0.3 +
            (1.0 if results["value_ranges"]["valid"] else 0.5) * 0.3
        )

        return {
            "overall_quality_score": total_score,
            "quality_grade": self._get_quality_grade(total_score),
            "validation_details": results,
            "recommendations": self._generate_recommendations(results)
        }

    def _get_quality_grade(self, score: float) -> str:
        if score >= 0.9:
            return "A"
        elif score >= 0.8:
            return "B"
        elif score >= 0.7:
            return "C"
        elif score >= 0.6:
            return "D"
        else:
            return "F"

    def _generate_recommendations(self, results: Dict) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []

        if not results["completeness"]["valid"]:
            recommendations.append(f"è¡¥å……ç¼ºå¤±å­—æ®µ: {', '.join(results['completeness']['missing_fields'])}")

        if not results["data_types"]["valid"]:
            recommendations.append("æ£€æŸ¥å¹¶ä¿®æ­£æ•°æ®ç±»å‹é”™è¯¯")

        if not results["value_ranges"]["valid"]:
            recommendations.append("æ£€æŸ¥å¼‚å¸¸æ•°å€¼ï¼Œå¯èƒ½éœ€è¦é‡æ–°æå–ä¿¡æ¯")

        if not recommendations:
            recommendations.append("æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå¯ç›´æ¥ä½¿ç”¨")

        return recommendations
```

## ğŸ“Š æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–

### 1. æ€§èƒ½ç›‘æ§

```python
import time
from functools import wraps
import psutil
import asyncio

class PerformanceMonitor:
    def __init__(self):
        self.metrics = {
            'processing_times': [],
            'memory_usage': [],
            'api_calls': 0,
            'success_rate': 0
        }

    def performance_tracker(self, func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            start_memory = psutil.virtual_memory().used

            try:
                result = await func(*args, **kwargs)
                self.metrics['api_calls'] += 1

                # è®°å½•æˆåŠŸç‡
                if result.get('success'):
                    self.metrics['success_rate'] += 1

                # è®°å½•å¤„ç†æ—¶é—´
                processing_time = time.time() - start_time
                self.metrics['processing_times'].append(processing_time)

                # è®°å½•å†…å­˜ä½¿ç”¨
                memory_used = psutil.virtual_memory().used - start_memory
                self.metrics['memory_usage'].append(memory_used)

                return result
            except Exception as e:
                self.metrics['api_calls'] += 1
                raise e

        return wrapper

    def get_performance_report(self) -> Dict:
        if not self.metrics['processing_times']:
            return {"status": "No data available"}

        return {
            "avg_processing_time": sum(self.metrics['processing_times']) / len(self.metrics['processing_times']),
            "max_processing_time": max(self.metrics['processing_times']),
            "min_processing_time": min(self.metrics['processing_times']),
            "success_rate": (self.metrics['success_rate'] / self.metrics['api_calls']) * 100,
            "total_api_calls": self.metrics['api_calls'],
            "avg_memory_usage": sum(self.metrics['memory_usage']) / len(self.metrics['memory_usage'])
        }
```

### 2. ç¼“å­˜ç­–ç•¥

```python
import redis
import hashlib
import json

class CacheManager:
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis_client = redis.from_url(redis_url)
        self.cache_ttl = 3600  # 1å°æ—¶è¿‡æœŸ

    def _generate_cache_key(self, text: str, schema_name: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{text}:{schema_name}"
        return f"medical_data:{hashlib.md5(content.encode()).hexdigest()}"

    async def get_cached_result(self, text: str, schema_name: str) -> Optional[Dict]:
        """è·å–ç¼“å­˜ç»“æœ"""
        cache_key = self._generate_cache_key(text, schema_name)
        try:
            cached_data = self.redis_client.get(cache_key)
            if cached_data:
                return json.loads(cached_data)
        except Exception as e:
            print(f"ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        return None

    async def cache_result(self, text: str, schema_name: str, result: Dict):
        """ç¼“å­˜å¤„ç†ç»“æœ"""
        cache_key = self._generate_cache_key(text, schema_name)
        try:
            self.redis_client.setex(
                cache_key,
                self.cache_ttl,
                json.dumps(result, ensure_ascii=False)
            )
        except Exception as e:
            print(f"ç¼“å­˜å†™å…¥å¤±è´¥: {e}")
```

## ğŸš€ éƒ¨ç½²å’Œä½¿ç”¨æŒ‡å—

### 1. ç¯å¢ƒé…ç½®

```yaml
# docker-compose.yml
version: '3.8'
services:
  medical-processor:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - REDIS_URL=redis://redis:6379
    depends_on:
      - redis
    volumes:
      - ./SchemaCSV:/app/SchemaCSV
      - ./data:/app/data

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  postgres:
    image: postgres:15
    environment:
      - POSTGRES_DB=medical_data
      - POSTGRES_USER=admin
      - POSTGRES_PASSWORD=password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:
```

### 2. å¿«é€Ÿå¼€å§‹

```bash
# 1. å…‹éš†é¡¹ç›®å¹¶å®‰è£…ä¾èµ–
git clone <repository-url>
cd medical-data-processor
pip install -r requirements.txt

# 2. è®¾ç½®ç¯å¢ƒå˜é‡
export OPENAI_API_KEY="your-api-key"

# 3. å¯åŠ¨Redisï¼ˆå¦‚æœä½¿ç”¨ç¼“å­˜ï¼‰
docker run -d -p 6379:6379 redis:alpine

# 4. å¯åŠ¨APIæœåŠ¡
python api_server.py

# 5. æµ‹è¯•API
curl -X POST "http://localhost:8000/process" \
  -H "Content-Type: application/json" \
  -d '{"text": "æ‚£è€…è¡€ç³–8.5mmol/L", "schema_name": "diabetes-comprehensive"}'
```

### 3. ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–å»ºè®®

1. **è´Ÿè½½å‡è¡¡**: ä½¿ç”¨nginxæˆ–AWS ALBåˆ†å‘è¯·æ±‚
2. **ç¼“å­˜ç­–ç•¥**: Redisé›†ç¾¤æé«˜ç¼“å­˜æ€§èƒ½
3. **æ•°æ®åº“**: PostgreSQLå­˜å‚¨ç»“æ„åŒ–æ•°æ®
4. **ç›‘æ§**: Prometheus + Grafanaç›‘æ§ç³»ç»Ÿæ€§èƒ½
5. **å®‰å…¨**: APIè®¤è¯å’Œæ•°æ®åŠ å¯†
6. **å¤‡ä»½**: å®šæœŸå¤‡ä»½å¤„ç†ç»“æœå’Œé…ç½®

## ğŸ“ æœ€ä½³å®è·µ

### 1. æç¤ºè¯ä¼˜åŒ–
- æ ¹æ®ä¸åŒSchemaç±»å‹è°ƒæ•´æç¤ºè¯
- æ·»åŠ åŒ»å­¦ä¸“ä¸šæœ¯è¯­å’Œä¸Šä¸‹æ–‡
- ä½¿ç”¨å°‘æ ·æœ¬å­¦ä¹ æå‡å‡†ç¡®ç‡

### 2. é”™è¯¯å¤„ç†
- å®ç°é‡è¯•æœºåˆ¶
- è®°å½•è¯¦ç»†é”™è¯¯æ—¥å¿—
- æä¾›äººå·¥å®¡æ ¸æ¥å£

### 3. è´¨é‡ä¿è¯
- å¤šå±‚éªŒè¯æœºåˆ¶
- ä¸“å®¶æ ‡æ³¨æ•°æ®é›†éªŒè¯
- æŒç»­æ¨¡å‹æ€§èƒ½è¯„ä¼°

### 4. æˆæœ¬æ§åˆ¶
- æ™ºèƒ½ç¼“å­˜å‡å°‘APIè°ƒç”¨
- æ‰¹é‡å¤„ç†æé«˜æ•ˆç‡
- ä½¿ç”¨ä¸åŒæ¨¡å‹å¤„ç†ä¸åŒå¤æ‚åº¦ä»»åŠ¡

é€šè¿‡ä»¥ä¸Šæ–¹æ¡ˆï¼Œå¯ä»¥é«˜æ•ˆåœ°å°†éç»“æ„åŒ–åŒ»ç–—æ•°æ®è½¬æ¢ä¸ºç¬¦åˆ64ä¸ªSchemaæ ‡å‡†çš„ç»“æ„åŒ–æ•°æ®ï¼Œå¤§å¹…æå‡åŒ»ç–—æ•°æ®å¤„ç†æ•ˆç‡å’Œè´¨é‡ã€‚

## ğŸ”— ç›¸å…³èµ„æº

- [OpenAI API æ–‡æ¡£](https://platform.openai.com/docs)
- [FastAPI å®˜æ–¹æ–‡æ¡£](https://fastapi.tiangolo.com/)
- [Pandas æ•°æ®å¤„ç†](https://pandas.pydata.org/docs/)
- [Redis ç¼“å­˜ä½¿ç”¨](https://redis.io/documentation)

---
**ç»´æŠ¤å›¢é˜Ÿ**: å†…åˆ†æ³Œä»£è°¢ç–¾ç—…æ•°æ®æ ‡å‡†åŒ–é¡¹ç›®ç»„
**æœ€åæ›´æ–°**: 2024å¹´12æœˆ01æ—¥