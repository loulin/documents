#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CGMæ•°æ®è´¨é‡è¯„ä¼°æ¨¡å—
åœ¨è¿›è¡ŒAGPåˆ†æå‰ï¼Œå…ˆè¯„ä¼°æ•°æ®è´¨é‡ï¼Œç¡®ä¿åˆ†æç»“æœçš„å¯é æ€§
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import warnings
import logging

from ..config.config_manager import ConfigManager

class CGMDataQualityAssessor:
    """CGMæ•°æ®è´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.quality_criteria = {
            'minimum_days': 10,           # æœ€å°‘å¤©æ•°
            'minimum_data_coverage': 70,  # æœ€ä½æ•°æ®è¦†ç›–ç‡(%)
            'maximum_gap_hours': 6,       # æœ€å¤§è¿ç»­ç¼ºå¤±æ—¶é—´(å°æ—¶)
            'minimum_daily_points': 60,   # æ¯æ—¥æœ€å°‘æ•°æ®ç‚¹æ•°
            'glucose_range_min': 1.0,     # è¡€ç³–æœ€å°å€¼(mmol/L)
            'glucose_range_max': 33.3,    # è¡€ç³–æœ€å¤§å€¼(mmol/L)
            'maximum_duplicate_rate': 20, # æœ€å¤§é‡å¤å€¼æ¯”ä¾‹(%)
            'maximum_outlier_rate': 5,    # æœ€å¤§å¼‚å¸¸å€¼æ¯”ä¾‹(%)
            'minimum_variability': 0.5    # æœ€å°å˜å¼‚ç³»æ•°
        }
    
    def assess_data_quality(self, cgm_data: pd.DataFrame, analysis_days: int = 14) -> Dict:
        """
        å…¨é¢è¯„ä¼°CGMæ•°æ®è´¨é‡
        
        Args:
            cgm_data: CGMæ•°æ®DataFrame
            analysis_days: åˆ†æå¤©æ•°
            
        Returns:
            æ•°æ®è´¨é‡è¯„ä¼°ç»“æœ
        """
        if cgm_data.empty:
            return self._generate_failed_assessment("æ•°æ®ä¸ºç©º")
        
        try:
            # æ•°æ®é¢„å¤„ç†
            clean_data = self._preprocess_data(cgm_data)
            
            # æ‰§è¡Œå„é¡¹è´¨é‡æ£€æŸ¥
            quality_checks = {}
            
            # 1. æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
            quality_checks.update(self._check_data_completeness(clean_data, analysis_days))
            
            # 2. æ—¶é—´åºåˆ—è¿ç»­æ€§æ£€æŸ¥
            quality_checks.update(self._check_time_continuity(clean_data))
            
            # 3. è¡€ç³–å€¼åˆç†æ€§æ£€æŸ¥
            quality_checks.update(self._check_glucose_validity(clean_data))
            
            # 4. æ•°æ®å˜å¼‚æ€§æ£€æŸ¥
            quality_checks.update(self._check_data_variability(clean_data))
            
            # 5. å¼‚å¸¸å€¼æ£€æŸ¥
            quality_checks.update(self._check_outliers(clean_data))
            
            # 6. é‡å¤å€¼æ£€æŸ¥
            quality_checks.update(self._check_duplicates(clean_data))
            
            # 7. ä¼ æ„Ÿå™¨æ€§èƒ½æ£€æŸ¥
            quality_checks.update(self._check_sensor_performance(clean_data))
            
            # ç»¼åˆè¯„ä¼°
            overall_assessment = self._calculate_overall_quality(quality_checks)
            
            return {
                'overall_quality': overall_assessment,
                'detailed_checks': quality_checks,
                'data_summary': self._generate_data_summary(clean_data),
                'recommendations': self._generate_recommendations(quality_checks),
                'usable_for_analysis': overall_assessment['is_acceptable'],
                'assessment_timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logging.error(f"æ•°æ®è´¨é‡è¯„ä¼°å¤±è´¥: {str(e)}")
            return self._generate_failed_assessment(f"è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {str(e)}")
    
    def _preprocess_data(self, cgm_data: pd.DataFrame) -> pd.DataFrame:
        """æ•°æ®é¢„å¤„ç†"""
        data = cgm_data.copy()
        
        # ç¡®ä¿æ—¶é—´æˆ³åˆ—å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®
        if 'timestamp' not in data.columns:
            raise ValueError("ç¼ºå°‘timestampåˆ—")
        
        # ç¡®ä¿è¡€ç³–åˆ—å­˜åœ¨
        if 'glucose' not in data.columns:
            raise ValueError("ç¼ºå°‘glucoseåˆ—")
        
        # è½¬æ¢æ•°æ®ç±»å‹
        data['timestamp'] = pd.to_datetime(data['timestamp'])
        data['glucose'] = pd.to_numeric(data['glucose'], errors='coerce')
        
        # æŒ‰æ—¶é—´æ’åº
        data = data.sort_values('timestamp').reset_index(drop=True)
        
        # ç§»é™¤é‡å¤æ—¶é—´æˆ³
        data = data.drop_duplicates(subset=['timestamp']).reset_index(drop=True)
        
        return data
    
    def _check_data_completeness(self, data: pd.DataFrame, analysis_days: int) -> Dict:
        """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§"""
        checks = {}
        
        # æ•°æ®æ—¶é—´è·¨åº¦
        time_span = (data['timestamp'].max() - data['timestamp'].min()).days
        checks['time_span_days'] = time_span
        checks['time_span_sufficient'] = time_span >= self.quality_criteria['minimum_days']
        
        # æ•°æ®ç‚¹æ€»æ•°
        total_points = len(data)
        expected_points = analysis_days * 24 * 4  # å‡è®¾15åˆ†é’Ÿé—´éš”
        checks['total_data_points'] = total_points
        checks['expected_data_points'] = expected_points
        checks['data_coverage_rate'] = (total_points / expected_points) * 100 if expected_points > 0 else 0
        checks['coverage_sufficient'] = checks['data_coverage_rate'] >= self.quality_criteria['minimum_data_coverage']
        
        # æ¯æ—¥æ•°æ®ç‚¹æ•°
        daily_counts = data.groupby(data['timestamp'].dt.date).size()
        checks['daily_point_counts'] = daily_counts.to_dict()
        checks['min_daily_points'] = daily_counts.min()
        checks['max_daily_points'] = daily_counts.max()
        checks['avg_daily_points'] = daily_counts.mean()
        checks['daily_coverage_sufficient'] = daily_counts.min() >= self.quality_criteria['minimum_daily_points']
        
        # æœ‰æ•°æ®çš„å¤©æ•°
        unique_days = data['timestamp'].dt.date.nunique()
        checks['days_with_data'] = unique_days
        checks['days_sufficient'] = unique_days >= self.quality_criteria['minimum_days']
        
        return {'completeness': checks}
    
    def _check_time_continuity(self, data: pd.DataFrame) -> Dict:
        """æ£€æŸ¥æ—¶é—´åºåˆ—è¿ç»­æ€§"""
        checks = {}
        
        # è®¡ç®—æ—¶é—´é—´éš”
        time_diffs = data['timestamp'].diff().dt.total_seconds() / 60  # è½¬æ¢ä¸ºåˆ†é’Ÿ
        
        # é¢„æœŸé—´éš”(åˆ†é’Ÿ)
        median_interval = time_diffs.median()
        checks['median_interval_minutes'] = median_interval
        
        # è¯†åˆ«æ•°æ®ç¼ºå¤±gap
        # å®šä¹‰gapä¸ºè¶…è¿‡æ­£å¸¸é—´éš”3å€çš„æ—¶é—´é—´éš”
        normal_interval_threshold = median_interval * 3 if not pd.isna(median_interval) else 60
        gaps = time_diffs[time_diffs > normal_interval_threshold]
        
        checks['total_gaps'] = len(gaps)
        checks['max_gap_hours'] = gaps.max() / 60 if len(gaps) > 0 else 0
        checks['total_missing_hours'] = gaps.sum() / 60 if len(gaps) > 0 else 0
        checks['gap_acceptable'] = checks['max_gap_hours'] <= self.quality_criteria['maximum_gap_hours']
        
        # é—´éš”åˆ†å¸ƒåˆ†æ
        checks['interval_statistics'] = {
            'mean': time_diffs.mean(),
            'std': time_diffs.std(),
            'min': time_diffs.min(),
            'max': time_diffs.max(),
            'p95': time_diffs.quantile(0.95)
        }
        
        return {'continuity': checks}
    
    def _check_glucose_validity(self, data: pd.DataFrame) -> Dict:
        """æ£€æŸ¥è¡€ç³–å€¼åˆç†æ€§"""
        checks = {}
        glucose_values = data['glucose'].dropna()
        
        # åŸºç¡€ç»Ÿè®¡
        checks['glucose_statistics'] = {
            'count': len(glucose_values),
            'mean': glucose_values.mean(),
            'std': glucose_values.std(),
            'min': glucose_values.min(),
            'max': glucose_values.max(),
            'median': glucose_values.median()
        }
        
        # èŒƒå›´æ£€æŸ¥
        checks['values_in_valid_range'] = len(glucose_values[
            (glucose_values >= self.quality_criteria['glucose_range_min']) & 
            (glucose_values <= self.quality_criteria['glucose_range_max'])
        ])
        checks['invalid_values_count'] = len(glucose_values) - checks['values_in_valid_range']
        checks['invalid_rate'] = (checks['invalid_values_count'] / len(glucose_values)) * 100
        checks['range_valid'] = checks['invalid_rate'] < 1  # å…è®¸1%çš„æ— æ•ˆå€¼
        
        # ç”Ÿç†åˆç†æ€§æ£€æŸ¥
        checks['extreme_low_count'] = len(glucose_values[glucose_values < 2.2])  # <2.2 mmol/L
        checks['extreme_high_count'] = len(glucose_values[glucose_values > 22.2])  # >22.2 mmol/L
        checks['physiologically_reasonable'] = (checks['extreme_low_count'] + checks['extreme_high_count']) < len(glucose_values) * 0.02
        
        # ç¼ºå¤±å€¼æ£€æŸ¥
        total_rows = len(data)
        missing_count = data['glucose'].isna().sum()
        checks['missing_values_count'] = missing_count
        checks['missing_rate'] = (missing_count / total_rows) * 100
        checks['missing_acceptable'] = checks['missing_rate'] < 10  # å…è®¸10%ç¼ºå¤±
        
        return {'validity': checks}
    
    def _check_data_variability(self, data: pd.DataFrame) -> Dict:
        """æ£€æŸ¥æ•°æ®å˜å¼‚æ€§"""
        checks = {}
        glucose_values = data['glucose'].dropna()
        
        # å˜å¼‚ç³»æ•°
        cv = (glucose_values.std() / glucose_values.mean()) * 100 if glucose_values.mean() > 0 else 0
        checks['coefficient_of_variation'] = cv
        checks['variability_sufficient'] = cv >= self.quality_criteria['minimum_variability']
        
        # è¡€ç³–èŒƒå›´
        glucose_range = glucose_values.max() - glucose_values.min()
        checks['glucose_range'] = glucose_range
        checks['range_adequate'] = glucose_range > 2.0  # è‡³å°‘2 mmol/Lçš„å˜åŒ–
        
        # å”¯ä¸€å€¼æ•°é‡
        unique_values = glucose_values.nunique()
        checks['unique_values_count'] = unique_values
        checks['unique_values_rate'] = (unique_values / len(glucose_values)) * 100
        checks['diversity_adequate'] = checks['unique_values_rate'] > 10  # è‡³å°‘10%çš„å€¼æ˜¯å”¯ä¸€çš„
        
        return {'variability': checks}
    
    def _check_outliers(self, data: pd.DataFrame) -> Dict:
        """æ£€æŸ¥å¼‚å¸¸å€¼"""
        checks = {}
        glucose_values = data['glucose'].dropna()
        
        # ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
        Q1 = glucose_values.quantile(0.25)
        Q3 = glucose_values.quantile(0.75)
        IQR = Q3 - Q1
        
        # å®šä¹‰å¼‚å¸¸å€¼èŒƒå›´
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers = glucose_values[(glucose_values < lower_bound) | (glucose_values > upper_bound)]
        checks['outlier_count'] = len(outliers)
        checks['outlier_rate'] = (len(outliers) / len(glucose_values)) * 100
        checks['outlier_acceptable'] = checks['outlier_rate'] <= self.quality_criteria['maximum_outlier_rate']
        
        # æç«¯å¼‚å¸¸å€¼(è¶…è¿‡3ä¸ªIQR)
        extreme_lower = Q1 - 3 * IQR
        extreme_upper = Q3 + 3 * IQR
        extreme_outliers = glucose_values[(glucose_values < extreme_lower) | (glucose_values > extreme_upper)]
        checks['extreme_outlier_count'] = len(extreme_outliers)
        checks['extreme_outlier_rate'] = (len(extreme_outliers) / len(glucose_values)) * 100
        
        return {'outliers': checks}
    
    def _check_duplicates(self, data: pd.DataFrame) -> Dict:
        """æ£€æŸ¥é‡å¤å€¼"""
        checks = {}
        glucose_values = data['glucose'].dropna()
        
        # è¿ç»­é‡å¤å€¼æ£€æŸ¥
        consecutive_duplicates = 0
        max_consecutive = 0
        current_consecutive = 0
        
        for i in range(1, len(glucose_values)):
            if glucose_values.iloc[i] == glucose_values.iloc[i-1]:
                current_consecutive += 1
                consecutive_duplicates += 1
            else:
                max_consecutive = max(max_consecutive, current_consecutive)
                current_consecutive = 0
        
        max_consecutive = max(max_consecutive, current_consecutive)
        
        checks['consecutive_duplicates'] = consecutive_duplicates
        checks['max_consecutive_count'] = max_consecutive
        checks['consecutive_duplicate_rate'] = (consecutive_duplicates / len(glucose_values)) * 100
        checks['duplicates_acceptable'] = checks['consecutive_duplicate_rate'] <= self.quality_criteria['maximum_duplicate_rate']
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¿‡å¤šçš„å›ºå®šå€¼
        value_counts = glucose_values.value_counts()
        most_common_value = value_counts.iloc[0] if not value_counts.empty else 0
        checks['most_common_value_count'] = most_common_value
        checks['most_common_value_rate'] = (most_common_value / len(glucose_values)) * 100
        checks['no_single_value_dominance'] = checks['most_common_value_rate'] < 30  # å•ä¸€å€¼ä¸è¶…è¿‡30%
        
        return {'duplicates': checks}
    
    def _check_sensor_performance(self, data: pd.DataFrame) -> Dict:
        """æ£€æŸ¥ä¼ æ„Ÿå™¨æ€§èƒ½æŒ‡æ ‡"""
        checks = {}
        glucose_values = data['glucose'].dropna()
        
        if len(glucose_values) < 10:
            return {'sensor_performance': {'insufficient_data': True}}
        
        # è®¡ç®—ä¸€é˜¶å·®åˆ†(è¡€ç³–å˜åŒ–ç‡)
        glucose_diff = glucose_values.diff().dropna()
        
        # è¡€ç³–å˜åŒ–ç‡ç»Ÿè®¡
        checks['glucose_change_stats'] = {
            'mean_change': glucose_diff.mean(),
            'std_change': glucose_diff.std(),
            'max_positive_change': glucose_diff.max(),
            'max_negative_change': glucose_diff.min()
        }
        
        # ä¼ æ„Ÿå™¨å™ªå£°è¯„ä¼°(åŸºäºé«˜é¢‘æ³¢åŠ¨)
        if len(glucose_diff) > 5:
            # è®¡ç®—è¿ç»­å˜åŒ–æ–¹å‘æ”¹å˜çš„é¢‘ç‡(å™ªå£°æŒ‡æ ‡)
            sign_changes = np.sum(np.diff(np.sign(glucose_diff)) != 0)
            checks['noise_index'] = sign_changes / len(glucose_diff)
            checks['low_noise'] = checks['noise_index'] < 0.6  # ç»éªŒé˜ˆå€¼
        
        # ä¼ æ„Ÿå™¨ç¨³å®šæ€§(åŸºäºé•¿æœŸè¶‹åŠ¿çš„ä¸€è‡´æ€§)
        # ä½¿ç”¨ç§»åŠ¨å¹³å‡æ¥è¯„ä¼°ç¨³å®šæ€§
        if len(glucose_values) >= 20:
            window_size = min(20, len(glucose_values) // 4)
            moving_avg = glucose_values.rolling(window=window_size, center=True).mean()
            residuals = glucose_values - moving_avg
            checks['sensor_precision'] = residuals.std()
            checks['good_precision'] = checks['sensor_precision'] < 2.0  # æ ‡å‡†åå·®<2 mmol/L
        
        return {'sensor_performance': checks}
    
    def _calculate_overall_quality(self, quality_checks: Dict) -> Dict:
        """è®¡ç®—ç»¼åˆæ•°æ®è´¨é‡è¯„åˆ†"""
        
        # å„é¡¹æ£€æŸ¥çš„æƒé‡
        weights = {
            'completeness': 0.25,
            'continuity': 0.20,
            'validity': 0.25,
            'variability': 0.10,
            'outliers': 0.10,
            'duplicates': 0.05,
            'sensor_performance': 0.05
        }
        
        scores = {}
        
        # è®¡ç®—å„é¡¹å¾—åˆ†
        # å®Œæ•´æ€§å¾—åˆ†
        completeness = quality_checks.get('completeness', {})
        completeness_score = 0
        if completeness.get('coverage_sufficient', False): completeness_score += 40
        if completeness.get('days_sufficient', False): completeness_score += 30
        if completeness.get('daily_coverage_sufficient', False): completeness_score += 30
        scores['completeness'] = completeness_score
        
        # è¿ç»­æ€§å¾—åˆ†
        continuity = quality_checks.get('continuity', {})
        continuity_score = 0
        if continuity.get('gap_acceptable', False): continuity_score += 60
        if continuity.get('total_gaps', 10) <= 5: continuity_score += 40  # å°‘äº5ä¸ªgap
        scores['continuity'] = continuity_score
        
        # æœ‰æ•ˆæ€§å¾—åˆ†
        validity = quality_checks.get('validity', {})
        validity_score = 0
        if validity.get('range_valid', False): validity_score += 30
        if validity.get('physiologically_reasonable', False): validity_score += 35
        if validity.get('missing_acceptable', False): validity_score += 35
        scores['validity'] = validity_score
        
        # å˜å¼‚æ€§å¾—åˆ†
        variability = quality_checks.get('variability', {})
        variability_score = 0
        if variability.get('variability_sufficient', False): variability_score += 40
        if variability.get('range_adequate', False): variability_score += 30
        if variability.get('diversity_adequate', False): variability_score += 30
        scores['variability'] = variability_score
        
        # å¼‚å¸¸å€¼å¾—åˆ†
        outliers = quality_checks.get('outliers', {})
        outlier_score = 0
        if outliers.get('outlier_acceptable', False): outlier_score += 60
        if outliers.get('extreme_outlier_rate', 100) < 1: outlier_score += 40
        scores['outliers'] = outlier_score
        
        # é‡å¤å€¼å¾—åˆ†
        duplicates = quality_checks.get('duplicates', {})
        duplicate_score = 0
        if duplicates.get('duplicates_acceptable', False): duplicate_score += 50
        if duplicates.get('no_single_value_dominance', False): duplicate_score += 50
        scores['duplicates'] = duplicate_score
        
        # ä¼ æ„Ÿå™¨æ€§èƒ½å¾—åˆ†
        sensor = quality_checks.get('sensor_performance', {})
        sensor_score = 50  # é»˜è®¤åˆ†æ•°
        if sensor.get('low_noise', True): sensor_score += 25
        if sensor.get('good_precision', True): sensor_score += 25
        scores['sensor_performance'] = sensor_score
        
        # è®¡ç®—åŠ æƒæ€»åˆ†
        total_score = sum(scores[key] * weights[key] for key in scores.keys())
        
        # è´¨é‡ç­‰çº§åˆ¤æ–­
        if total_score >= 80:
            quality_level = "ä¼˜ç§€"
            is_acceptable = True
            description = "æ•°æ®è´¨é‡ä¼˜ç§€ï¼Œå®Œå…¨é€‚åˆè¿›è¡ŒAGPåˆ†æ"
        elif total_score >= 65:
            quality_level = "è‰¯å¥½"  
            is_acceptable = True
            description = "æ•°æ®è´¨é‡è‰¯å¥½ï¼Œé€‚åˆè¿›è¡ŒAGPåˆ†æ"
        elif total_score >= 50:
            quality_level = "ä¸€èˆ¬"
            is_acceptable = True
            description = "æ•°æ®è´¨é‡ä¸€èˆ¬ï¼Œå¯è¿›è¡ŒAGPåˆ†æä½†ç»“æœå¯ä¿¡åº¦æœ‰é™"
        else:
            quality_level = "ä¸åˆæ ¼"
            is_acceptable = False
            description = "æ•°æ®è´¨é‡ä¸åˆæ ¼ï¼Œä¸å»ºè®®è¿›è¡ŒAGPåˆ†æ"
        
        return {
            'total_score': round(total_score, 1),
            'quality_level': quality_level,
            'is_acceptable': is_acceptable,
            'description': description,
            'component_scores': scores
        }
    
    def _generate_data_summary(self, data: pd.DataFrame) -> Dict:
        """ç”Ÿæˆæ•°æ®æ‘˜è¦"""
        glucose_values = data['glucose'].dropna()
        
        return {
            'total_records': len(data),
            'valid_glucose_records': len(glucose_values),
            'time_span': {
                'start': data['timestamp'].min().isoformat(),
                'end': data['timestamp'].max().isoformat(),
                'days': (data['timestamp'].max() - data['timestamp'].min()).days
            },
            'glucose_summary': {
                'mean': round(glucose_values.mean(), 2),
                'median': round(glucose_values.median(), 2),
                'std': round(glucose_values.std(), 2),
                'min': round(glucose_values.min(), 2),
                'max': round(glucose_values.max(), 2),
                'cv': round((glucose_values.std() / glucose_values.mean()) * 100, 1)
            }
        }
    
    def _generate_recommendations(self, quality_checks: Dict) -> List[str]:
        """ç”Ÿæˆæ•°æ®è´¨é‡æ”¹å–„å»ºè®®"""
        recommendations = []
        
        # åŸºäºå„é¡¹æ£€æŸ¥ç»“æœç”Ÿæˆå»ºè®®
        completeness = quality_checks.get('completeness', {})
        if not completeness.get('coverage_sufficient', True):
            recommendations.append("æ•°æ®è¦†ç›–ç‡ä¸è¶³ï¼Œå»ºè®®å»¶é•¿CGMä½©æˆ´æ—¶é—´æˆ–æ£€æŸ¥è®¾å¤‡å·¥ä½œçŠ¶æ€")
        
        if not completeness.get('days_sufficient', True):
            recommendations.append("æ•°æ®å¤©æ•°ä¸è¶³ï¼Œå»ºè®®æ”¶é›†è‡³å°‘10-14å¤©çš„CGMæ•°æ®")
        
        continuity = quality_checks.get('continuity', {})
        if not continuity.get('gap_acceptable', True):
            recommendations.append("å­˜åœ¨è¾ƒé•¿çš„æ•°æ®ç¼ºå¤±é—´éš”ï¼Œå»ºè®®æ£€æŸ¥CGMè®¾å¤‡è¿æ¥çŠ¶æ€")
        
        validity = quality_checks.get('validity', {})
        if not validity.get('range_valid', True):
            recommendations.append("å­˜åœ¨è¶…å‡ºç”Ÿç†èŒƒå›´çš„è¡€ç³–å€¼ï¼Œå»ºè®®æ£€æŸ¥ä¼ æ„Ÿå™¨æ ¡å‡†")
        
        if not validity.get('missing_acceptable', True):
            recommendations.append("ç¼ºå¤±å€¼è¿‡å¤šï¼Œå»ºè®®æ£€æŸ¥ä¼ æ„Ÿå™¨è´´é™„çŠ¶æ€å’Œè®¾å¤‡ç”µé‡")
        
        variability = quality_checks.get('variability', {})
        if not variability.get('variability_sufficient', True):
            recommendations.append("è¡€ç³–å˜å¼‚æ€§è¿‡ä½ï¼Œå¯èƒ½å­˜åœ¨ä¼ æ„Ÿå™¨æ•…éšœæˆ–æ•°æ®å¼‚å¸¸")
        
        outliers = quality_checks.get('outliers', {})
        if not outliers.get('outlier_acceptable', True):
            recommendations.append("å¼‚å¸¸å€¼è¾ƒå¤šï¼Œå»ºè®®æ£€æŸ¥ä¼ æ„Ÿå™¨ç²¾åº¦å’Œæ‚£è€…æ´»åŠ¨è®°å½•")
        
        duplicates = quality_checks.get('duplicates', {})
        if not duplicates.get('duplicates_acceptable', True):
            recommendations.append("è¿ç»­é‡å¤å€¼è¿‡å¤šï¼Œå¯èƒ½å­˜åœ¨ä¼ æ„Ÿå™¨å¡æ»é—®é¢˜")
        
        if not recommendations:
            recommendations.append("æ•°æ®è´¨é‡è‰¯å¥½ï¼Œæ— éœ€ç‰¹æ®Šå¤„ç†")
        
        return recommendations
    
    def _generate_failed_assessment(self, error_message: str) -> Dict:
        """ç”Ÿæˆå¤±è´¥è¯„ä¼°ç»“æœ"""
        return {
            'overall_quality': {
                'total_score': 0,
                'quality_level': 'è¯„ä¼°å¤±è´¥',
                'is_acceptable': False,
                'description': f'æ•°æ®è´¨é‡è¯„ä¼°å¤±è´¥: {error_message}',
                'component_scores': {}
            },
            'detailed_checks': {},
            'data_summary': {},
            'recommendations': [f'è¯·æ£€æŸ¥æ•°æ®æ ¼å¼å’Œå®Œæ•´æ€§: {error_message}'],
            'usable_for_analysis': False,
            'assessment_timestamp': datetime.now().isoformat(),
            'error': error_message
        }
    
    def generate_quality_report(self, assessment_result: Dict) -> str:
        """ç”Ÿæˆå¯è¯»çš„è´¨é‡è¯„ä¼°æŠ¥å‘Š"""
        if not assessment_result.get('usable_for_analysis', False):
            return f"""
=== CGMæ•°æ®è´¨é‡è¯„ä¼°æŠ¥å‘Š ===

âŒ æ•°æ®è´¨é‡: {assessment_result['overall_quality']['quality_level']}
ğŸ“Š è´¨é‡å¾—åˆ†: {assessment_result['overall_quality']['total_score']}/100

âš ï¸ è¯„ä¼°ç»“æœ: {assessment_result['overall_quality']['description']}

ğŸ”§ æ”¹å–„å»ºè®®:
""" + '\n'.join(f"   â€¢ {rec}" for rec in assessment_result.get('recommendations', []))

        summary = assessment_result.get('data_summary', {})
        overall = assessment_result['overall_quality']
        
        report = f"""
=== CGMæ•°æ®è´¨é‡è¯„ä¼°æŠ¥å‘Š ===

âœ… æ•°æ®è´¨é‡: {overall['quality_level']}
ğŸ“Š è´¨é‡å¾—åˆ†: {overall['total_score']}/100
ğŸ“ˆ æ•°æ®æ¦‚å†µ: å…±{summary.get('total_records', 0)}ä¸ªè®°å½•ï¼Œ{summary.get('time_span', {}).get('days', 0)}å¤©

ğŸ“‹ è¡€ç³–æ•°æ®æ‘˜è¦:
   â€¢ å¹³å‡è¡€ç³–: {summary.get('glucose_summary', {}).get('mean', 0):.1f} mmol/L
   â€¢ è¡€ç³–èŒƒå›´: {summary.get('glucose_summary', {}).get('min', 0):.1f} - {summary.get('glucose_summary', {}).get('max', 0):.1f} mmol/L
   â€¢ å˜å¼‚ç³»æ•°: {summary.get('glucose_summary', {}).get('cv', 0):.1f}%

ğŸ¯ è´¨é‡åˆ†æ:
"""
        
        # æ·»åŠ å„ç»„ä»¶å¾—åˆ†
        scores = overall.get('component_scores', {})
        score_descriptions = {
            'completeness': 'æ•°æ®å®Œæ•´æ€§',
            'continuity': 'æ—¶é—´è¿ç»­æ€§', 
            'validity': 'æ•°æ®æœ‰æ•ˆæ€§',
            'variability': 'æ•°æ®å˜å¼‚æ€§',
            'outliers': 'å¼‚å¸¸å€¼æ§åˆ¶',
            'duplicates': 'é‡å¤å€¼æ§åˆ¶',
            'sensor_performance': 'ä¼ æ„Ÿå™¨æ€§èƒ½'
        }
        
        for key, desc in score_descriptions.items():
            if key in scores:
                score = scores[key]
                status = "âœ…" if score >= 70 else "âš ï¸" if score >= 50 else "âŒ"
                report += f"   {status} {desc}: {score}/100\n"
        
        report += f"\nğŸ”§ å»ºè®®äº‹é¡¹:\n"
        for rec in assessment_result.get('recommendations', []):
            report += f"   â€¢ {rec}\n"
        
        report += f"\nğŸ“Š åˆ†æå»ºè®®: {overall['description']}"
        
        return report


def main():
    """æµ‹è¯•æ•°æ®è´¨é‡è¯„ä¼°å™¨"""
    from CGM_AGP_Analyzer_Agent import CGMDataReader
    
    # åˆ›å»ºè´¨é‡è¯„ä¼°å™¨
    quality_assessor = CGMDataQualityAssessor()
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®è¿›è¡Œæµ‹è¯•
    print("ğŸ”¬ åˆ›å»ºæµ‹è¯•æ•°æ®...")
    
    # ç”Ÿæˆ14å¤©çš„æ¨¡æ‹ŸCGMæ•°æ®
    dates = pd.date_range('2024-01-01', periods=14*24*4, freq='15min')
    glucose_values = 7 + 2 * np.sin(2 * np.pi * np.arange(len(dates)) / (24*4)) + 0.5 * np.random.randn(len(dates))
    glucose_values = np.clip(glucose_values, 3.0, 20.0)
    
    # æ¨¡æ‹Ÿä¸€äº›æ•°æ®è´¨é‡é—®é¢˜
    # 1. éšæœºç¼ºå¤±ä¸€äº›æ•°æ®ç‚¹
    missing_indices = np.random.choice(len(dates), size=int(len(dates) * 0.05), replace=False)
    glucose_values[missing_indices] = np.nan
    
    # 2. æ·»åŠ ä¸€äº›å¼‚å¸¸å€¼
    outlier_indices = np.random.choice(len(dates), size=10, replace=False)
    glucose_values[outlier_indices] = np.random.choice([1.0, 25.0], size=10)
    
    # 3. æ·»åŠ ä¸€äº›è¿ç»­é‡å¤å€¼
    duplicate_start = 100
    glucose_values[duplicate_start:duplicate_start+20] = 8.5
    
    test_data = pd.DataFrame({
        'timestamp': dates,
        'glucose': glucose_values,
        'device_info': 'test'
    })
    
    print(f"ğŸ“Š ç”Ÿæˆæµ‹è¯•æ•°æ®: {len(test_data)}æ¡è®°å½•")
    
    # è¿›è¡Œè´¨é‡è¯„ä¼°
    print("\nğŸ” å¼€å§‹æ•°æ®è´¨é‡è¯„ä¼°...")
    assessment = quality_assessor.assess_data_quality(test_data, analysis_days=14)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = quality_assessor.generate_quality_report(assessment)
    print(report)
    
    # ä¿å­˜è¯¦ç»†è¯„ä¼°ç»“æœ
    import json
    with open(f"CGM_Quality_Assessment_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json", 'w', encoding='utf-8') as f:
        json.dump(assessment, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"\nğŸ’¾ è¯¦ç»†è¯„ä¼°ç»“æœå·²ä¿å­˜")
    
    # è¿”å›æ˜¯å¦å¯ç”¨äºåˆ†æ
    return assessment['usable_for_analysis']

if __name__ == "__main__":
    main()