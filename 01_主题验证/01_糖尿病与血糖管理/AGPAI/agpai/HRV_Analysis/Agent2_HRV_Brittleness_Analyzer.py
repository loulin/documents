#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Agent2_HRV_Brittleness_Analyzer.py

åŸºäºAgent2 v5.0è¡€ç³–è„†æ€§åˆ†ææ¶æ„çš„HRVè„†æ€§åˆ†æç³»ç»Ÿ
é€‚ç”¨äºå¿ƒç‡å˜å¼‚æ€§çš„è„†æ€§è¯„ä¼°ã€æ™ºèƒ½åˆ†æ®µå’Œè‡ªä¸»ç¥ç»åŠŸèƒ½åˆ†æ

ä½œè€…: AGPAI Team  
ç‰ˆæœ¬: v1.0
æ—¥æœŸ: 2025-08-28
"""

import numpy as np
import pandas as pd
from scipy import signal
from scipy.stats import entropy
import json
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

class HRVBrittenessAnalyzer:
    """HRVè„†æ€§åˆ†æå™¨ - åŸºäºAgent2æ··æ²ŒåŠ¨åŠ›å­¦æ¶æ„"""
    
    def __init__(self):
        self.brittleness_types = {
            1: "Iå‹æ­£å¸¸è°ƒèŠ‚å‹",
            2: "IIå‹è½»åº¦å¤±è°ƒå‹",
            3: "IIIå‹ä¸­åº¦å¤±è°ƒå‹", 
            4: "IVå‹é‡åº¦å¤±è°ƒå‹",
            5: "Vå‹æåº¦åˆšæ€§å‹"
        }
        
        # HRVæ­£å¸¸å‚è€ƒèŒƒå›´ (åŸºäºå¹´é¾„å’Œæ€§åˆ«)
        self.reference_ranges = {
            "RMSSD": {"normal": 50, "warning": 30, "risk": 15},
            "pNN50": {"normal": 15, "warning": 5, "risk": 2},
            "SDNN": {"normal": 100, "warning": 50, "risk": 30},
            "LF_HF_ratio": {"balanced_min": 1.0, "balanced_max": 2.5}
        }
    
    def preprocess_rr_data(self, rr_intervals):
        """RRé—´æœŸæ•°æ®é¢„å¤„ç†"""
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        rr_data = np.array(rr_intervals, dtype=float)
        
        # å¼‚å¸¸å€¼è¿‡æ»¤ (300-2000ms)
        valid_mask = (rr_data >= 300) & (rr_data <= 2000)
        rr_data = rr_data[valid_mask]
        
        # ç§»é™¤æç«¯å¼‚å¸¸å€¼ (3ÏƒåŸåˆ™)
        if len(rr_data) > 10:
            mean_rr = np.mean(rr_data)
            std_rr = np.std(rr_data)
            valid_mask = np.abs(rr_data - mean_rr) <= 3 * std_rr
            rr_data = rr_data[valid_mask]
        
        return rr_data
    
    def calculate_time_domain_hrv(self, rr_intervals):
        """è®¡ç®—HRVæ—¶åŸŸæŒ‡æ ‡"""
        if len(rr_intervals) < 5:
            return {
                'RMSSD': 0, 'pNN50': 0, 'SDNN': 0, 'SDANN': 0,
                'mean_RR': 0, 'mean_HR': 0
            }
        
        # åŸºç¡€ç»Ÿè®¡
        mean_rr = np.mean(rr_intervals)
        mean_hr = 60000 / mean_rr if mean_rr > 0 else 0
        
        # RMSSD: ç›¸é‚»RRé—´æœŸå·®å€¼çš„å‡æ–¹æ ¹
        rmssd = np.sqrt(np.mean(np.diff(rr_intervals) ** 2))
        
        # pNN50: ç›¸é‚»RRé—´æœŸå·®å€¼>50msçš„ç™¾åˆ†æ¯”
        nn50_count = np.sum(np.abs(np.diff(rr_intervals)) > 50)
        pnn50 = (nn50_count / (len(rr_intervals) - 1)) * 100
        
        # SDNN: RRé—´æœŸæ ‡å‡†å·®
        sdnn = np.std(rr_intervals)
        
        # SDANN: 5åˆ†é’Ÿæ®µå¹³å‡RRé—´æœŸçš„æ ‡å‡†å·®
        if len(rr_intervals) > 100:
            segment_size = len(rr_intervals) // 20  # åˆ†ä¸º20æ®µ
            segment_means = []
            for i in range(0, len(rr_intervals) - segment_size, segment_size):
                segment = rr_intervals[i:i + segment_size]
                segment_means.append(np.mean(segment))
            sdann = np.std(segment_means) if len(segment_means) > 1 else 0
        else:
            sdann = 0
        
        return {
            'RMSSD': rmssd,
            'pNN50': pnn50,
            'SDNN': sdnn,
            'SDANN': sdann,
            'mean_RR': mean_rr,
            'mean_HR': mean_hr
        }
    
    def calculate_frequency_domain_hrv(self, rr_intervals, sampling_rate=4):
        """è®¡ç®—HRVé¢‘åŸŸæŒ‡æ ‡"""
        try:
            if len(rr_intervals) < 50:
                return {'LF': 0, 'HF': 0, 'LF_HF_ratio': 0, 'total_power': 0}
            
            # RRé—´æœŸæ’å€¼åˆ°ç­‰é—´éš”æ—¶é—´åºåˆ—
            time_points = np.cumsum(rr_intervals) / 1000  # è½¬æ¢ä¸ºç§’
            time_uniform = np.arange(0, time_points[-1], 1/sampling_rate)
            
            # æ’å€¼
            rr_interpolated = np.interp(time_uniform, time_points[:-1], rr_intervals[:-1])
            
            # å»è¶‹åŠ¿
            rr_detrended = signal.detrend(rr_interpolated)
            
            # åŠŸç‡è°±å¯†åº¦ä¼°è®¡
            f, psd = signal.welch(rr_detrended, fs=sampling_rate, nperseg=min(256, len(rr_detrended)))
            
            # é¢‘æ®µåŠŸç‡è®¡ç®—
            vlf_mask = (f >= 0.003) & (f < 0.04)   # VLF: 0.003-0.04 Hz
            lf_mask = (f >= 0.04) & (f < 0.15)     # LF: 0.04-0.15 Hz  
            hf_mask = (f >= 0.15) & (f < 0.4)      # HF: 0.15-0.4 Hz
            
            vlf_power = np.sum(psd[vlf_mask])
            lf_power = np.sum(psd[lf_mask])
            hf_power = np.sum(psd[hf_mask])
            total_power = np.sum(psd)
            
            # LF/HFæ¯”å€¼
            lf_hf_ratio = lf_power / hf_power if hf_power > 0 else 0
            
            return {
                'VLF': vlf_power,
                'LF': lf_power,
                'HF': hf_power,
                'LF_HF_ratio': lf_hf_ratio,
                'total_power': total_power
            }
        except:
            return {'LF': 0, 'HF': 0, 'LF_HF_ratio': 0, 'total_power': 0}
    
    def calculate_lyapunov_exponent(self, rr_intervals, embedding_dim=3, delay=1):
        """è®¡ç®—LyapunovæŒ‡æ•° - æ··æ²ŒåŠ¨åŠ›å­¦æ ¸å¿ƒæŒ‡æ ‡"""
        try:
            if len(rr_intervals) < 100:
                return 0
            
            # ç›¸ç©ºé—´é‡æ„
            N = len(rr_intervals)
            reconstructed = np.zeros((N - (embedding_dim - 1) * delay, embedding_dim))
            
            for i in range(embedding_dim):
                start_idx = i * delay
                end_idx = N - (embedding_dim - 1 - i) * delay
                reconstructed[:, i] = rr_intervals[start_idx:end_idx]
            
            # è®¡ç®—å¹³å‡å¯¹æ•°æ•£åº¦
            divergences = []
            for i in range(1, len(reconstructed) - 1):
                # å¯»æ‰¾æœ€è¿‘é‚»ç‚¹
                distances = np.linalg.norm(reconstructed - reconstructed[i], axis=1)
                distances[i] = np.inf  # æ’é™¤è‡ªèº«
                nearest_idx = np.argmin(distances)
                
                # è®¡ç®—æ¼”åŒ–åçš„è·ç¦»
                if i + 1 < len(reconstructed) and nearest_idx + 1 < len(reconstructed):
                    initial_distance = distances[nearest_idx]
                    evolved_distance = np.linalg.norm(
                        reconstructed[i + 1] - reconstructed[nearest_idx + 1]
                    )
                    
                    if initial_distance > 0 and evolved_distance > 0:
                        divergence = np.log(evolved_distance / initial_distance)
                        divergences.append(divergence)
            
            lyapunov_exponent = np.mean(divergences) if divergences else 0
            return lyapunov_exponent
        except:
            return 0
    
    def calculate_approximate_entropy(self, rr_intervals, m=2, r=None):
        """è®¡ç®—è¿‘ä¼¼ç†µ"""
        try:
            N = len(rr_intervals)
            if N < 100 or r is None:
                r = 0.2 * np.std(rr_intervals) if N >= 10 else 1.0
            
            def maxdist(xi, xj):
                return max([abs(ua - va) for ua, va in zip(xi, xj)])
            
            def phi(m):
                patterns = np.array([rr_intervals[i:i + m] for i in range(N - m + 1)])
                C = np.zeros(N - m + 1)
                
                for i in range(N - m + 1):
                    template = patterns[i]
                    matches = 0
                    for j in range(N - m + 1):
                        if maxdist(template, patterns[j]) <= r:
                            matches += 1
                    C[i] = matches / (N - m + 1)
                
                phi_value = sum([np.log(c) if c > 0 else 0 for c in C]) / (N - m + 1)
                return phi_value
            
            approximate_entropy = phi(m) - phi(m + 1)
            return approximate_entropy
        except:
            return 0
    
    def calculate_hurst_exponent(self, rr_intervals):
        """è®¡ç®—HurstæŒ‡æ•°"""
        try:
            N = len(rr_intervals)
            if N < 100:
                return 0.5
            
            # R/Såˆ†æ
            lags = np.logspace(1, np.log10(N//4), 15).astype(int)
            rs_values = []
            
            for lag in lags:
                if lag >= N:
                    continue
                    
                sections = N // lag
                if sections == 0:
                    continue
                
                section_rs = []
                for i in range(sections):
                    start_idx = i * lag
                    end_idx = (i + 1) * lag
                    section_data = rr_intervals[start_idx:end_idx]
                    
                    # è®¡ç®—ç´¯ç§¯åå·®
                    mean_val = np.mean(section_data)
                    deviations = section_data - mean_val
                    cumsum_dev = np.cumsum(deviations)
                    
                    # R: æå·®
                    R = np.max(cumsum_dev) - np.min(cumsum_dev)
                    
                    # S: æ ‡å‡†å·®
                    S = np.std(section_data)
                    
                    if S > 0:
                        section_rs.append(R / S)
                
                if len(section_rs) > 0:
                    rs_values.append(np.mean(section_rs))
            
            # çº¿æ€§æ‹Ÿåˆè®¡ç®—HurstæŒ‡æ•°
            if len(rs_values) >= 3:
                valid_lags = lags[:len(rs_values)]
                log_lags = np.log10(valid_lags)
                log_rs = np.log10(rs_values)
                
                # è¿‡æ»¤æ— æ•ˆå€¼
                valid_mask = ~(np.isinf(log_rs) | np.isnan(log_rs))
                if np.sum(valid_mask) >= 3:
                    hurst = np.polyfit(log_lags[valid_mask], log_rs[valid_mask], 1)[0]
                    return max(0, min(1, hurst))
            
            return 0.5
        except:
            return 0.5
    
    def calculate_sample_entropy(self, rr_intervals, m=2, r=None):
        """è®¡ç®—æ ·æœ¬ç†µ"""
        try:
            N = len(rr_intervals)
            if N < 50:
                return 0
            
            if r is None:
                r = 0.2 * np.std(rr_intervals)
            
            def chebyshev_distance(a, b):
                return max([abs(x - y) for x, y in zip(a, b)])
            
            A = 0.0  # m+1é•¿åº¦åŒ¹é…æ•°
            B = 0.0  # mé•¿åº¦åŒ¹é…æ•°
            
            for i in range(N - m):
                template_m = rr_intervals[i:i + m]
                if i + m + 1 <= N:
                    template_m_plus = rr_intervals[i:i + m + 1]
                else:
                    continue
                
                for j in range(i + 1, N - m):
                    if j + m <= N:
                        test_m = rr_intervals[j:j + m]
                        
                        # mé•¿åº¦åŒ¹é…æ£€æŸ¥
                        if chebyshev_distance(template_m, test_m) <= r:
                            B += 1.0
                            
                            # m+1é•¿åº¦åŒ¹é…æ£€æŸ¥
                            if j + m + 1 <= N:
                                test_m_plus = rr_intervals[j:j + m + 1]
                                if chebyshev_distance(template_m_plus, test_m_plus) <= r:
                                    A += 1.0
            
            sample_entropy = -np.log(A / B) if B > 0 else 0
            return sample_entropy
        except:
            return 0
    
    def calculate_poincare_analysis(self, rr_intervals):
        """Poincareæ•£ç‚¹å›¾åˆ†æ"""
        try:
            if len(rr_intervals) < 10:
                return {'SD1': 0, 'SD2': 0, 'SD1_SD2_ratio': 0}
            
            # RR(n) vs RR(n+1)
            rr1 = rr_intervals[:-1]  # RR(n)
            rr2 = rr_intervals[1:]   # RR(n+1)
            
            # è®¡ç®—SD1å’ŒSD2
            # SD1: çŸ­è½´æ ‡å‡†å·® (å¿«é€Ÿå˜åŒ–)
            sd1 = np.std(rr1 - rr2) / np.sqrt(2)
            
            # SD2: é•¿è½´æ ‡å‡†å·® (æ…¢é€Ÿå˜åŒ–)
            sd2 = np.std(rr1 + rr2) / np.sqrt(2)
            
            # SD1/SD2æ¯”å€¼
            sd1_sd2_ratio = sd1 / sd2 if sd2 > 0 else 0
            
            return {
                'SD1': sd1,
                'SD2': sd2, 
                'SD1_SD2_ratio': sd1_sd2_ratio
            }
        except:
            return {'SD1': 0, 'SD2': 0, 'SD1_SD2_ratio': 0}
    
    def classify_hrv_brittleness(self, rr_intervals):
        """HRVè„†æ€§åˆ†å‹"""
        try:
            # æ•°æ®é¢„å¤„ç†
            clean_rr = self.preprocess_rr_data(rr_intervals)
            
            if len(clean_rr) < 50:
                return {"error": "RRé—´æœŸæ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå¯é åˆ†æ"}
            
            # æ—¶åŸŸåˆ†æ
            time_domain = self.calculate_time_domain_hrv(clean_rr)
            
            # é¢‘åŸŸåˆ†æ
            freq_domain = self.calculate_frequency_domain_hrv(clean_rr)
            
            # æ··æ²ŒåŠ¨åŠ›å­¦åˆ†æ
            lyapunov = self.calculate_lyapunov_exponent(clean_rr)
            approx_entropy = self.calculate_approximate_entropy(clean_rr)
            hurst_exponent = self.calculate_hurst_exponent(clean_rr)
            sample_entropy = self.calculate_sample_entropy(clean_rr)
            
            # Poincareåˆ†æ
            poincare = self.calculate_poincare_analysis(clean_rr)
            
            # HRVè„†æ€§è¯„åˆ†
            brittleness_score = self.calculate_hrv_brittleness_score(
                time_domain, freq_domain, lyapunov, approx_entropy, hurst_exponent
            )
            
            # è„†æ€§åˆ†å‹
            brittleness_type = self.determine_brittleness_type(brittleness_score)
            risk_level = self.assess_risk_level(brittleness_score)
            autonomic_balance = self.assess_autonomic_balance(freq_domain)
            
            return {
                "è„†æ€§åˆ†å‹": self.brittleness_types[brittleness_type],
                "è„†æ€§è¯„åˆ†": f"{brittleness_score:.1f}/100",
                "é£é™©ç­‰çº§": risk_level,
                "è‡ªä¸»ç¥ç»å¹³è¡¡": autonomic_balance,
                "æ—¶åŸŸHRVæŒ‡æ ‡": {
                    "RMSSD": f"{time_domain['RMSSD']:.1f} ms",
                    "pNN50": f"{time_domain['pNN50']:.1f}%",
                    "SDNN": f"{time_domain['SDNN']:.1f} ms",
                    "å¹³å‡å¿ƒç‡": f"{time_domain['mean_HR']:.1f} bpm"
                },
                "é¢‘åŸŸHRVæŒ‡æ ‡": {
                    "LFåŠŸç‡": f"{freq_domain['LF']:.2f} msÂ²",
                    "HFåŠŸç‡": f"{freq_domain['HF']:.2f} msÂ²",
                    "LF/HFæ¯”å€¼": f"{freq_domain['LF_HF_ratio']:.2f}"
                },
                "æ··æ²ŒåŠ¨åŠ›å­¦æŒ‡æ ‡": {
                    "LyapunovæŒ‡æ•°": f"{lyapunov:.6f}",
                    "è¿‘ä¼¼ç†µ": f"{approx_entropy:.4f}",
                    "HurstæŒ‡æ•°": f"{hurst_exponent:.4f}",
                    "æ ·æœ¬ç†µ": f"{sample_entropy:.4f}"
                },
                "Poincareåˆ†æ": {
                    "SD1": f"{poincare['SD1']:.1f} ms",
                    "SD2": f"{poincare['SD2']:.1f} ms",
                    "SD1/SD2": f"{poincare['SD1_SD2_ratio']:.3f}"
                },
                "ä¸´åºŠè§£è¯»": self.generate_clinical_interpretation(
                    brittleness_type, autonomic_balance, time_domain
                )
            }
            
        except Exception as e:
            return {"error": f"HRVåˆ†æè¿‡ç¨‹å‡ºé”™: {str(e)}"}
    
    def calculate_hrv_brittleness_score(self, time_domain, freq_domain, lyapunov, approx_entropy, hurst):
        """HRVè„†æ€§ç»¼åˆè¯„åˆ† (0-100)"""
        
        score = 0
        
        # æ—¶åŸŸæŒ‡æ ‡è¯„åˆ† (0-35åˆ†)
        # RMSSDè¯„åˆ†
        if time_domain['RMSSD'] < self.reference_ranges['RMSSD']['risk']:
            score += 15
        elif time_domain['RMSSD'] < self.reference_ranges['RMSSD']['warning']:
            score += 8
        
        # pNN50è¯„åˆ†
        if time_domain['pNN50'] < self.reference_ranges['pNN50']['risk']:
            score += 10
        elif time_domain['pNN50'] < self.reference_ranges['pNN50']['warning']:
            score += 5
        
        # SDNNè¯„åˆ†
        if time_domain['SDNN'] < self.reference_ranges['SDNN']['risk']:
            score += 10
        elif time_domain['SDNN'] < self.reference_ranges['SDNN']['warning']:
            score += 5
        
        # é¢‘åŸŸæŒ‡æ ‡è¯„åˆ† (0-25åˆ†)
        lf_hf_ratio = freq_domain['LF_HF_ratio']
        if lf_hf_ratio > 4.0 or lf_hf_ratio < 0.5:  # ä¸¥é‡å¤±è¡¡
            score += 15
        elif lf_hf_ratio > 3.0 or lf_hf_ratio < 0.8:  # ä¸­åº¦å¤±è¡¡
            score += 10
        elif lf_hf_ratio > self.reference_ranges['LF_HF_ratio']['balanced_max'] or \
             lf_hf_ratio < self.reference_ranges['LF_HF_ratio']['balanced_min']:
            score += 5
        
        # æ··æ²ŒåŠ¨åŠ›å­¦æŒ‡æ ‡è¯„åˆ† (0-40åˆ†)
        # LyapunovæŒ‡æ•°è¯„åˆ†
        if abs(lyapunov) > 0.01:
            score += 10
        elif abs(lyapunov) > 0.005:
            score += 5
        
        # è¿‘ä¼¼ç†µè¯„åˆ†
        if approx_entropy < 0.3 or approx_entropy > 2.0:
            score += 10
        elif approx_entropy < 0.5 or approx_entropy > 1.5:
            score += 5
        
        # HurstæŒ‡æ•°è¯„åˆ†
        if abs(hurst - 0.5) > 0.4:  # ä¸¥é‡åç¦»éšæœºæ€§
            score += 15
        elif abs(hurst - 0.5) > 0.3:
            score += 10
        elif abs(hurst - 0.5) > 0.2:
            score += 5
        
        return min(100, max(0, score))
    
    def determine_brittleness_type(self, score):
        """ç¡®å®šè„†æ€§åˆ†å‹"""
        if score <= 15:
            return 1  # Iå‹æ­£å¸¸è°ƒèŠ‚å‹
        elif score <= 35:
            return 2  # IIå‹è½»åº¦å¤±è°ƒå‹
        elif score <= 55:
            return 3  # IIIå‹ä¸­åº¦å¤±è°ƒå‹
        elif score <= 75:
            return 4  # IVå‹é‡åº¦å¤±è°ƒå‹
        else:
            return 5  # Vå‹æåº¦åˆšæ€§å‹
    
    def assess_risk_level(self, score):
        """è¯„ä¼°é£é™©ç­‰çº§"""
        if score <= 15:
            return "ğŸŸ¢ ä½é£é™©"
        elif score <= 35:
            return "ğŸŸ¡ ä¸­ä½é£é™©"
        elif score <= 55:
            return "ğŸŸ  ä¸­ç­‰é£é™©"
        elif score <= 75:
            return "ğŸ”´ é«˜é£é™©"
        else:
            return "ğŸ”´ æé«˜é£é™©"
    
    def assess_autonomic_balance(self, freq_domain):
        """è¯„ä¼°è‡ªä¸»ç¥ç»å¹³è¡¡"""
        lf_hf_ratio = freq_domain['LF_HF_ratio']
        
        if lf_hf_ratio > 3.0:
            return "äº¤æ„Ÿç¥ç»è¿‡åº¦æ¿€æ´»"
        elif lf_hf_ratio > 2.5:
            return "äº¤æ„Ÿç¥ç»ä¼˜åŠ¿"
        elif lf_hf_ratio >= 1.0:
            return "ç›¸å¯¹å¹³è¡¡"
        elif lf_hf_ratio >= 0.5:
            return "å‰¯äº¤æ„Ÿç¥ç»ä¼˜åŠ¿"
        else:
            return "å‰¯äº¤æ„Ÿç¥ç»è¿‡åº¦æ¿€æ´»"
    
    def generate_clinical_interpretation(self, brittleness_type, autonomic_balance, time_domain):
        """ç”Ÿæˆä¸´åºŠè§£è¯»"""
        base_interpretations = {
            1: "è‡ªä¸»ç¥ç»åŠŸèƒ½æ­£å¸¸ï¼Œå¿ƒç‡å˜å¼‚æ€§è‰¯å¥½ï¼Œé€‚åº”æ€§è°ƒèŠ‚èƒ½åŠ›å¼º",
            2: "è½»åº¦è‡ªä¸»ç¥ç»åŠŸèƒ½å¤±è°ƒï¼Œå»ºè®®å…³æ³¨ç”Ÿæ´»æ–¹å¼å› ç´ å’Œå‹åŠ›ç®¡ç†",
            3: "ä¸­åº¦è‡ªä¸»ç¥ç»åŠŸèƒ½å¼‚å¸¸ï¼Œå»ºè®®è¿›ä¸€æ­¥è¯„ä¼°å¿ƒè¡€ç®¡é£é™©å› ç´ ",
            4: "é‡åº¦è‡ªä¸»ç¥ç»åŠŸèƒ½å¤±è°ƒï¼Œå¿ƒè¡€ç®¡äº‹ä»¶é£é™©å¢åŠ ï¼Œå»ºè®®ä¸“ç§‘ä¼šè¯Š",
            5: "æåº¦è‡ªä¸»ç¥ç»åŠŸèƒ½åˆšæ€§ï¼Œå¿ƒè„çŒæ­»é«˜é£é™©ï¼Œéœ€è¦ç´§æ€¥åŒ»ç–—å…³æ³¨"
        }
        
        base_text = base_interpretations.get(brittleness_type, "éœ€è¦ä¸“ä¸šè¯„ä¼°")
        
        # æ·»åŠ è‡ªä¸»ç¥ç»å¹³è¡¡ä¿¡æ¯
        balance_text = f"è‡ªä¸»ç¥ç»çŠ¶æ€æ˜¾ç¤º{autonomic_balance}"
        
        # æ·»åŠ å¿ƒç‡ä¿¡æ¯
        hr_text = ""
        if time_domain['mean_HR'] > 100:
            hr_text = "ï¼Œä¼´æœ‰å¿ƒåŠ¨è¿‡é€Ÿ"
        elif time_domain['mean_HR'] < 50:
            hr_text = "ï¼Œä¼´æœ‰å¿ƒåŠ¨è¿‡ç¼“"
        
        return f"{base_text}ã€‚{balance_text}{hr_text}ã€‚"

def analyze_hrv_brittleness(rr_intervals, patient_id="Unknown"):
    """HRVè„†æ€§åˆ†æä¸»å‡½æ•°"""
    
    print(f"ğŸ’“ HRVè„†æ€§åˆ†æç³»ç»Ÿå¯åŠ¨ - æ‚£è€…ID: {patient_id}")
    print("="*60)
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = HRVBrittenessAnalyzer()
    
    # æ‰§è¡Œåˆ†æ
    result = analyzer.classify_hrv_brittleness(rr_intervals)
    
    # ç”ŸæˆæŠ¥å‘Š
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report = {
        "æŠ¥å‘Šå¤´ä¿¡æ¯": {
            "æŠ¥å‘Šç±»å‹": "HRVè„†æ€§åˆ†ææŠ¥å‘Š v1.0",
            "æ‚£è€…ID": patient_id,
            "åˆ†ææ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "åˆ†ææ–¹æ³•": "æ··æ²ŒåŠ¨åŠ›å­¦ + æ—¶é¢‘åŸŸHRV + è‡ªä¸»ç¥ç»å¹³è¡¡åˆ†æ",
            "æ•°æ®è´¨é‡": f"åˆ†æäº† {len(rr_intervals)} ä¸ªRRé—´æœŸ"
        },
        "HRVè„†æ€§è¯„ä¼°": result
    }
    
    # ä¿å­˜æŠ¥å‘Š
    filename = f"HRV_Brittleness_Analysis_{patient_id}_{timestamp}.json"
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2, default=str)
    
    print("ğŸ“Š HRVè„†æ€§åˆ†æå®Œæˆ")
    if "error" not in result:
        print(f"è„†æ€§åˆ†å‹: {result.get('è„†æ€§åˆ†å‹', 'N/A')}")
        print(f"è„†æ€§è¯„åˆ†: {result.get('è„†æ€§è¯„åˆ†', 'N/A')}")
        print(f"é£é™©ç­‰çº§: {result.get('é£é™©ç­‰çº§', 'N/A')}")
        print(f"è‡ªä¸»ç¥ç»å¹³è¡¡: {result.get('è‡ªä¸»ç¥ç»å¹³è¡¡', 'N/A')}")
    else:
        print(f"åˆ†æå‡ºé”™: {result['error']}")
    
    print(f"åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {filename}")
    
    return report

# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    # ç”Ÿæˆç¤ºä¾‹RRé—´æœŸæ•°æ®
    np.random.seed(42)
    
    # æ¨¡æ‹Ÿ24å°æ—¶RRé—´æœŸæ•°æ®
    # æ­£å¸¸æˆå¹´äººé™æ¯å¿ƒç‡çº¦60-80 bpmï¼ŒRRé—´æœŸçº¦750-1000ms
    
    # åŸºç¡€RRé—´æœŸ (å¿ƒç‡çº¦72 bpm)
    base_rr = 833  # ms
    num_beats = 5000  # å¤§çº¦1å°æ—¶çš„æ•°æ®
    
    # æ·»åŠ ç”Ÿç†æ€§å˜å¼‚
    # 1. å‘¼å¸æ€§çª¦æ€§å¿ƒå¾‹ä¸é½ (RSA) - é«˜é¢‘æˆåˆ†
    respiratory_freq = 0.25  # Hz (15æ¬¡/åˆ†é’Ÿ)
    time_points = np.arange(num_beats) * base_rr / 1000  # ç§’
    rsa_variation = 30 * np.sin(2 * np.pi * respiratory_freq * time_points)
    
    # 2. è¡€å‹è°ƒèŠ‚ç›¸å…³çš„ä½é¢‘å˜å¼‚
    lf_freq = 0.1  # Hz
    lf_variation = 20 * np.sin(2 * np.pi * lf_freq * time_points)
    
    # 3. éšæœºå™ªå£°
    random_variation = np.random.normal(0, 10, num_beats)
    
    # 4. æ˜¼å¤œèŠ‚å¾‹æ•ˆåº” (ç®€åŒ–)
    circadian_variation = 50 * np.sin(2 * np.pi * time_points / (24 * 3600))
    
    # åˆæˆRRé—´æœŸ
    rr_intervals = (base_rr + rsa_variation + lf_variation + 
                   random_variation + circadian_variation)
    
    # ç¡®ä¿RRé—´æœŸåœ¨åˆç†èŒƒå›´å†…
    rr_intervals = np.clip(rr_intervals, 400, 1500)
    
    print(f"ç”Ÿæˆäº† {len(rr_intervals)} ä¸ªRRé—´æœŸæ•°æ®ç‚¹")
    print(f"RRé—´æœŸèŒƒå›´: {np.min(rr_intervals):.1f} - {np.max(rr_intervals):.1f} ms")
    print(f"å¹³å‡å¿ƒç‡: {60000 / np.mean(rr_intervals):.1f} bpm")
    
    # æ‰§è¡Œåˆ†æ
    result = analyze_hrv_brittleness(rr_intervals, "Demo_Patient_HRV")
    
    print("\nğŸ¯ HRVæ¼”ç¤ºåˆ†æå®Œæˆï¼")