#!/usr/bin/env python3
"""
åŸºäºHRVå‚æ•°çš„ECGè‡ªåŠ¨è¯Šæ–­ç³»ç»Ÿ
ä¸SNOMED-CTé¢„æ ‡æ³¨è¯Šæ–­è¿›è¡Œå¯¹æ¯”åˆ†æ
"""

import pandas as pd
import numpy as np
import ast
import re
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

class ECGAutoDiagnosisSystem:
    def __init__(self):
        """åˆå§‹åŒ–è¯Šæ–­ç³»ç»Ÿ"""
        # SNOMEDç¼–ç åˆ°ä¸­æ–‡è¯Šæ–­çš„æ˜ å°„
        self.snomed_to_chinese = {
            '426177001': 'çª¦æ€§å¿ƒåŠ¨è¿‡ç¼“',
            '426783006': 'çª¦æ€§å¿ƒå¾‹', 
            '164889003': 'å¿ƒå¾‹ä¸é½',
            '427084000': 'å¿ƒæˆ¿é¢¤åŠ¨',
            '164890007': 'å®¤æ€§å¿ƒå¾‹å¤±å¸¸',
            '164934002': 'å¿ƒç”µå›¾å¼‚å¸¸',
            '55827005': 'å¿ƒè‚Œç¼ºè¡€',
            '59118001': 'å³æŸæ”¯é˜»æ»',
            '428750005': 'å·¦å‰åˆ†æ”¯é˜»æ»',
            '17338001': 'å·¦å¿ƒå®¤è‚¥åš',
            '429622005': 'å·¦æŸæ”¯é˜»æ»',
            '233917008': 'ä¸»åŠ¨è„‰ç“£ç‹­çª„',
            '270492004': 'ä¸€åº¦æˆ¿å®¤é˜»æ»',
            '164909002': 'å·¦å¿ƒæˆ¿æ‰©å¤§',
            '164912004': 'å³å¿ƒæˆ¿æ‰©å¤§',
            '164917005': 'åŒæˆ¿æ‰©å¤§',
            '251173003': 'Tæ³¢å¼‚å¸¸',
            '251199005': 'STæ®µå¼‚å¸¸',
            '251146004': 'å¿ƒç”µè½´åç§»',
            '284470004': 'å¿ƒæˆ¿æ—©æ',
            '39732003': 'å®¤æ€§æ—©æ',
            '59931005': 'æˆ¿å®¤äº¤ç•Œæ€§å¿ƒå¾‹',
            '47665007': 'å¤šå½¢æ€§å®¤æ€§å¿ƒåŠ¨è¿‡é€Ÿ',
            '698252002': 'äºŒåº¦æˆ¿å®¤é˜»æ»',
            '164865005': 'å®Œå…¨æ€§æˆ¿å®¤é˜»æ»'
        }
    
    def parse_quality_field(self, quality_str):
        """è§£æè´¨é‡å­—æ®µ"""
        try:
            if pd.isna(quality_str):
                return {'snr_db': np.nan, 'baseline_drift_ratio': np.nan, 
                       'saturation_ratio': np.nan, 'dynamic_range': np.nan}
            
            # æ¸…ç†å­—ç¬¦ä¸²æ ¼å¼
            quality_str = str(quality_str).replace("'", '"')
            quality_str = re.sub(r'np\.float64\((.*?)\)', r'\1', quality_str)
            quality_dict = ast.literal_eval(quality_str)
            return quality_dict
        except:
            return {'snr_db': np.nan, 'baseline_drift_ratio': np.nan, 
                   'saturation_ratio': np.nan, 'dynamic_range': np.nan}
    
    def rule_based_diagnosis(self, row):
        """åŸºäºè§„åˆ™çš„è¯Šæ–­ç®—æ³•"""
        diagnoses = []
        
        # è§£æä¿¡å·è´¨é‡
        quality = self.parse_quality_field(row.get('quality', '{}'))
        snr = quality.get('snr_db', np.nan)
        
        # å¦‚æœä¿¡å·è´¨é‡å¤ªå·®ï¼Œè¿”å›å¿ƒç”µå›¾å¼‚å¸¸
        if not pd.isna(snr) and snr < 5:
            diagnoses.append('164934002')  # å¿ƒç”µå›¾å¼‚å¸¸
        
        # è·å–å…³é”®å‚æ•°
        mean_hr = row.get('mean_hr', np.nan)
        std_rr = row.get('std_rr', np.nan)  # SDNN
        rmssd = row.get('rmssd', np.nan)
        pnn50 = row.get('pnn50', np.nan)
        triangular_index = row.get('triangular_index', np.nan)
        r_peaks_consistency = row.get('r_peaks_consistency', np.nan)
        lf_hf_ratio = row.get('lf_hf_ratio', np.nan)
        cv = row.get('cv', np.nan)
        
        # 1. å¿ƒç‡åŸºç¡€è¯Šæ–­
        if not pd.isna(mean_hr):
            if mean_hr < 50:
                diagnoses.append('426177001')  # çª¦æ€§å¿ƒåŠ¨è¿‡ç¼“
            elif mean_hr >= 50 and mean_hr <= 100 and r_peaks_consistency > 0.9:
                diagnoses.append('426783006')  # çª¦æ€§å¿ƒå¾‹
        
        # 2. å¿ƒå¾‹å¤±å¸¸è¯Šæ–­
        if not pd.isna(r_peaks_consistency) and r_peaks_consistency < 0.7:
            # Rå³°æ£€æµ‹ä¸€è‡´æ€§ä½ï¼Œæç¤ºä¸¥é‡å¿ƒå¾‹å¤±å¸¸
            if not pd.isna(pnn50) and pnn50 > 80:
                diagnoses.append('427084000')  # å¿ƒæˆ¿é¢¤åŠ¨
            else:
                diagnoses.append('164889003')  # å¿ƒå¾‹ä¸é½
        
        # 3. å¿ƒå¾‹ä¸é½è¿›ä¸€æ­¥åˆ†ç±»
        elif not pd.isna(std_rr) and not pd.isna(triangular_index):
            if std_rr > 100 and triangular_index < 6:
                if not pd.isna(pnn50) and pnn50 > 50:
                    diagnoses.append('427084000')  # æˆ¿é¢¤ç‰¹å¾
                else:
                    diagnoses.append('164889003')  # ä¸€èˆ¬å¿ƒå¾‹ä¸é½
        
        # 4. å®¤æ€§å¿ƒå¾‹å¤±å¸¸æ£€æµ‹
        if (not pd.isna(std_rr) and std_rr > 200 and 
            not pd.isna(cv) and cv > 40 and
            not pd.isna(r_peaks_consistency) and r_peaks_consistency < 0.6):
            diagnoses.append('164890007')  # å®¤æ€§å¿ƒå¾‹å¤±å¸¸
        
        # 5. ä¼ å¯¼é˜»æ»æ£€æµ‹ (åŸºäºRå³°ä¸€è‡´æ€§å’Œå¿ƒç‡å˜å¼‚æ€§)
        if (not pd.isna(r_peaks_consistency) and r_peaks_consistency < 0.8 and
            not pd.isna(std_rr) and std_rr > 50 and std_rr < 150):
            diagnoses.append('59118001')  # å³æŸæ”¯é˜»æ» (æœ€å¸¸è§)
        
        # 6. å¿ƒè‚Œç¼ºè¡€æ£€æµ‹ (åŸºäºHRVé¢‘åŸŸåˆ†æ)
        if not pd.isna(lf_hf_ratio) and (lf_hf_ratio < 0.5 or lf_hf_ratio > 4.0):
            diagnoses.append('55827005')  # å¿ƒè‚Œç¼ºè¡€
        
        # 7. è‡ªä¸»ç¥ç»åŠŸèƒ½å¼‚å¸¸
        if not pd.isna(rmssd):
            if rmssd < 15:
                # å‰¯äº¤æ„Ÿç¥ç»åŠŸèƒ½ä¸¥é‡å—æŸï¼Œå¯èƒ½åˆå¹¶å…¶ä»–å¼‚å¸¸
                if '164889003' not in diagnoses and '427084000' not in diagnoses:
                    diagnoses.append('164934002')  # å¿ƒç”µå›¾å¼‚å¸¸
            elif rmssd > 200:
                # æé«˜çš„RMSSDå¯èƒ½æç¤ºç—…ç†æ€§å˜å¼‚
                diagnoses.append('164889003')  # å¿ƒå¾‹ä¸é½
        
        # 8. å¦‚æœæ²¡æœ‰æ˜ç¡®è¯Šæ–­ä½†æœ‰å¼‚å¸¸å‚æ•°ï¼Œæ ‡è®°ä¸ºå¿ƒç”µå›¾å¼‚å¸¸
        if not diagnoses:
            # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•å¼‚å¸¸å‚æ•°
            abnormal_params = 0
            if not pd.isna(mean_hr) and (mean_hr < 50 or mean_hr > 100):
                abnormal_params += 1
            if not pd.isna(std_rr) and std_rr > 100:
                abnormal_params += 1
            if not pd.isna(r_peaks_consistency) and r_peaks_consistency < 0.9:
                abnormal_params += 1
            
            if abnormal_params >= 2:
                diagnoses.append('164934002')  # å¿ƒç”µå›¾å¼‚å¸¸
            else:
                diagnoses.append('426783006')  # çª¦æ€§å¿ƒå¾‹(æ­£å¸¸)
        
        return diagnoses
    
    def compare_diagnoses(self, original_diagnosis, predicted_diagnosis):
        """æ¯”è¾ƒåŸå§‹è¯Šæ–­å’Œé¢„æµ‹è¯Šæ–­"""
        # è§£æåŸå§‹è¯Šæ–­
        orig_codes = set(original_diagnosis.split(',')) if original_diagnosis else set()
        pred_codes = set(predicted_diagnosis)
        
        # è®¡ç®—åŒ¹é…æƒ…å†µ
        exact_match = orig_codes == pred_codes
        partial_match = len(orig_codes & pred_codes) > 0
        missed_diagnoses = orig_codes - pred_codes
        extra_diagnoses = pred_codes - orig_codes
        
        return {
            'exact_match': exact_match,
            'partial_match': partial_match,
            'overlap_count': len(orig_codes & pred_codes),
            'missed_diagnoses': missed_diagnoses,
            'extra_diagnoses': extra_diagnoses,
            'jaccard_similarity': len(orig_codes & pred_codes) / len(orig_codes | pred_codes) if orig_codes | pred_codes else 0
        }
    
    def analyze_dataset(self, csv_file):
        """åˆ†ææ•´ä¸ªæ•°æ®é›†"""
        print("ğŸ” åŠ è½½ECGåˆ†ææ•°æ®é›†...")
        df = pd.read_csv(csv_file)
        
        print(f"æ•°æ®é›†å¤§å°: {len(df)} ä¸ªè®°å½•")
        
        # å¯¹æ¯ä¸ªè®°å½•è¿›è¡Œè¯Šæ–­
        results = []
        for idx, row in df.iterrows():
            record_name = row['record_name']
            original_diagnosis = row['diagnosis']
            
            # è‡ªåŠ¨è¯Šæ–­
            predicted_diagnosis = self.rule_based_diagnosis(row)
            
            # æ¯”è¾ƒè¯Šæ–­ç»“æœ
            comparison = self.compare_diagnoses(original_diagnosis, predicted_diagnosis)
            
            result = {
                'record_name': record_name,
                'original_diagnosis': original_diagnosis,
                'predicted_diagnosis': ','.join(predicted_diagnosis),
                'original_chinese': self.get_chinese_diagnosis(original_diagnosis),
                'predicted_chinese': self.get_chinese_diagnosis(','.join(predicted_diagnosis)),
                **comparison
            }
            results.append(result)
        
        return pd.DataFrame(results), df
    
    def get_chinese_diagnosis(self, diagnosis_codes):
        """è·å–ä¸­æ–‡è¯Šæ–­"""
        if not diagnosis_codes:
            return "æ— è¯Šæ–­"
        
        codes = diagnosis_codes.split(',')
        chinese_names = []
        for code in codes:
            code = code.strip()
            if code in self.snomed_to_chinese:
                chinese_names.append(self.snomed_to_chinese[code])
            else:
                chinese_names.append(f"æœªçŸ¥ç¼–ç ({code})")
        
        return ' + '.join(chinese_names)
    
    def generate_performance_report(self, results_df):
        """ç”Ÿæˆæ€§èƒ½è¯„ä¼°æŠ¥å‘Š"""
        total_records = len(results_df)
        exact_matches = results_df['exact_match'].sum()
        partial_matches = results_df['partial_match'].sum()
        
        print("ğŸ“Š è¯Šæ–­æ€§èƒ½è¯„ä¼°æŠ¥å‘Š")
        print("=" * 50)
        print(f"æ€»è®°å½•æ•°: {total_records}")
        print(f"å®Œå…¨åŒ¹é…: {exact_matches} ({exact_matches/total_records*100:.1f}%)")
        print(f"éƒ¨åˆ†åŒ¹é…: {partial_matches} ({partial_matches/total_records*100:.1f}%)")
        print(f"å¹³å‡Jaccardç›¸ä¼¼åº¦: {results_df['jaccard_similarity'].mean():.3f}")
        print(f"å¹³å‡é‡å è¯Šæ–­æ•°: {results_df['overlap_count'].mean():.1f}")
        
        print("\nğŸ¯ æŒ‰è¯Šæ–­ç±»åˆ«åˆ†æ:")
        
        # åˆ†ææœ€å¸¸è§çš„åŸå§‹è¯Šæ–­
        orig_diag_counts = results_df['original_diagnosis'].value_counts().head(10)
        print("\nåŸå§‹è¯Šæ–­Top 10:")
        for diag, count in orig_diag_counts.items():
            chinese = self.get_chinese_diagnosis(diag)
            print(f"  {chinese}: {count}æ¬¡")
        
        # åˆ†æé¢„æµ‹è¯Šæ–­
        pred_diag_counts = results_df['predicted_diagnosis'].value_counts().head(10)
        print("\né¢„æµ‹è¯Šæ–­Top 10:")
        for diag, count in pred_diag_counts.items():
            chinese = self.get_chinese_diagnosis(diag)
            print(f"  {chinese}: {count}æ¬¡")
        
        # åˆ†æmissedå’Œextraè¯Šæ–­
        print("\nâŒ æœ€å¸¸æ¼è¯Šçš„ç–¾ç—…:")
        all_missed = []
        for missed_set in results_df['missed_diagnoses']:
            all_missed.extend(list(missed_set))
        
        if all_missed:
            missed_counts = pd.Series(all_missed).value_counts().head(5)
            for code, count in missed_counts.items():
                chinese = self.get_chinese_diagnosis(code)
                print(f"  {chinese} ({code}): æ¼è¯Š{count}æ¬¡")
        else:
            print("  æ— æ˜æ˜¾æ¼è¯Šæ¨¡å¼")
        
        print("\nâ• æœ€å¸¸è¿‡è¯Šçš„ç–¾ç—…:")
        all_extra = []
        for extra_set in results_df['extra_diagnoses']:
            all_extra.extend(list(extra_set))
        
        if all_extra:
            extra_counts = pd.Series(all_extra).value_counts().head(5)
            for code, count in extra_counts.items():
                chinese = self.get_chinese_diagnosis(code)
                print(f"  {chinese} ({code}): è¿‡è¯Š{count}æ¬¡")
        else:
            print("  æ— æ˜æ˜¾è¿‡è¯Šæ¨¡å¼")
        
        return {
            'total_records': total_records,
            'exact_match_rate': exact_matches / total_records,
            'partial_match_rate': partial_matches / total_records,
            'avg_jaccard_similarity': results_df['jaccard_similarity'].mean(),
            'avg_overlap_count': results_df['overlap_count'].mean()
        }
    
    def generate_detailed_comparison(self, results_df, output_file=None):
        """ç”Ÿæˆè¯¦ç»†çš„è¯Šæ–­å¯¹æ¯”è¡¨"""
        # é€‰æ‹©å…³é”®åˆ—è¿›è¡Œè¾“å‡º
        comparison_df = results_df[[
            'record_name', 
            'original_chinese', 
            'predicted_chinese',
            'exact_match',
            'jaccard_similarity',
            'overlap_count'
        ]].copy()
        
        # æ·»åŠ åŒ¹é…çŠ¶æ€åˆ—
        comparison_df['match_status'] = comparison_df.apply(lambda row: 
            'âœ…å®Œå…¨åŒ¹é…' if row['exact_match'] 
            else f'ğŸ”¶éƒ¨åˆ†åŒ¹é…({row["overlap_count"]}ä¸ª)' if row['overlap_count'] > 0
            else 'âŒå®Œå…¨ä¸åŒ¹é…', axis=1)
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        comparison_df = comparison_df.sort_values('jaccard_similarity', ascending=False)
        
        if output_file:
            comparison_df.to_csv(output_file, index=False, encoding='utf-8-sig')
            print(f"\nğŸ“„ è¯¦ç»†å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        return comparison_df

def main():
    """ä¸»å‡½æ•°"""
    diagnosis_system = ECGAutoDiagnosisSystem()
    
    # åˆ†ææ•°æ®é›†
    results_df, original_df = diagnosis_system.analyze_dataset('enhanced_ecg_analysis_results.csv')
    
    # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    performance_metrics = diagnosis_system.generate_performance_report(results_df)
    
    # ç”Ÿæˆè¯¦ç»†å¯¹æ¯”
    detailed_comparison = diagnosis_system.generate_detailed_comparison(
        results_df, 
        'ecg_diagnosis_comparison.csv'
    )
    
    # æ˜¾ç¤ºä¸€äº›å…·ä½“æ¡ˆä¾‹
    print("\nğŸ” è¯Šæ–­å¯¹æ¯”æ¡ˆä¾‹ (Top 10):")
    print("-" * 100)
    for idx, row in detailed_comparison.head(10).iterrows():
        print(f"è®°å½•: {row['record_name']}")
        print(f"  åŸå§‹è¯Šæ–­: {row['original_chinese']}")
        print(f"  é¢„æµ‹è¯Šæ–­: {row['predicted_chinese']}")
        print(f"  åŒ¹é…çŠ¶æ€: {row['match_status']} (ç›¸ä¼¼åº¦: {row['jaccard_similarity']:.3f})")
        print()
    
    return diagnosis_system, results_df, detailed_comparison, performance_metrics

if __name__ == '__main__':
    diagnosis_system, results_df, detailed_comparison, performance_metrics = main()