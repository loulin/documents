#!/usr/bin/env python3
"""
é«˜çº§ECGæ•°æ®åˆ†æå™¨
æ”¹è¿›ç‰ˆæœ¬ï¼šæ—¢æ”¯æŒä¸“ä¸šåº“ä¹Ÿæ”¯æŒåŸºç¡€ç¯å¢ƒ
ä¸“é—¨ç”¨äºåˆ†æPhysioNet WFDBæ ¼å¼çš„12å¯¼è”ECGæ•°æ®
æ”¯æŒæ‰¹é‡åˆ†æå’Œè¯¦ç»†å¿ƒç”µå›¾ç‰¹å¾æå–
"""

import struct
import os
import pandas as pd
import numpy as np
import argparse
from pathlib import Path
import warnings
import math
from scipy import signal
import matplotlib.pyplot as plt

warnings.filterwarnings('ignore')

def parse_header_file(header_path):
    """è§£æ.heaå¤´æ–‡ä»¶è·å–è®°å½•ä¿¡æ¯"""
    info = {}
    leads = []
    
    try:
        with open(header_path, 'r') as f:
            lines = f.readlines()
        
        # ç¬¬ä¸€è¡ŒåŒ…å«åŸºæœ¬ä¿¡æ¯
        first_line = lines[0].strip().split()
        info['record_name'] = first_line[0]
        info['num_leads'] = int(first_line[1])
        info['sampling_rate'] = int(first_line[2])
        info['num_samples'] = int(first_line[3])
        
        # è§£æå¯¼è”ä¿¡æ¯å’Œå¢ç›Š
        gains = []
        baselines = []
        for i in range(1, info['num_leads'] + 1):
            lead_line = lines[i].strip().split()
            lead_name = lead_line[-1]  # å¯¼è”åç§°åœ¨æœ€å
            gain_str = lead_line[2]  # å¢ç›Šä¿¡æ¯ "1000/mV"
            gain = float(gain_str.split('/')[0])
            baseline = int(lead_line[4]) if len(lead_line) > 4 else 0
            
            leads.append(lead_name)
            gains.append(gain)
            baselines.append(baseline)
        
        info['leads'] = leads
        info['gains'] = gains
        info['baselines'] = baselines
        
        # è§£ææ‚£è€…ä¿¡æ¯
        for line in lines:
            line = line.strip()
            if line.startswith('#Age:'):
                info['age'] = line.split(': ')[1]
            elif line.startswith('#Sex:'):
                info['sex'] = line.split(': ')[1]
            elif line.startswith('#Dx:'):
                info['diagnosis'] = line.split(': ')[1]
        
        return info
        
    except Exception as e:
        print(f"è§£æå¤´æ–‡ä»¶é”™è¯¯: {e}")
        return None

def read_mat_file(mat_path, num_leads, num_samples):
    """è¯»å–.matäºŒè¿›åˆ¶æ•°æ®æ–‡ä»¶"""
    try:
        with open(mat_path, 'rb') as f:
            data = f.read()
        
        num_values = len(data) // 2
        values = struct.unpack(f'<{num_values}h', data)
        
        expected_values = num_leads * num_samples
        
        if num_values > expected_values:
            extra = num_values - expected_values
            print(f"è·³è¿‡ {extra} ä¸ªé¢å¤–å€¼ï¼ˆæ–‡ä»¶å¤´ï¼‰")
            signal_values = values[extra:]
            if len(signal_values) == expected_values:
                signal_data = np.array(signal_values).reshape(num_samples, num_leads)
            else:
                return None
        else:
            signal_data = np.array(values).reshape(num_samples, num_leads)
        
        return signal_data
        
    except Exception as e:
        print(f"è¯»å–æ•°æ®æ–‡ä»¶é”™è¯¯: {e}")
        return None

def convert_to_physical_units(signal_data, gains, baselines):
    """å°†æ•°å­—ä¿¡å·è½¬æ¢ä¸ºç‰©ç†å•ä½(mV)"""
    try:
        physical_data = np.zeros_like(signal_data, dtype=float)
        for i in range(len(gains)):
            # è½¬æ¢å…¬å¼: physical = (digital - baseline) / gain
            physical_data[:, i] = (signal_data[:, i] - baselines[i]) / gains[i]
        return physical_data
    except Exception as e:
        print(f"å•ä½è½¬æ¢é”™è¯¯: {e}")
        return signal_data.astype(float)

def advanced_r_peak_detection(ecg_signal, sampling_rate):
    """æ”¹è¿›çš„Rå³°æ£€æµ‹ç®—æ³•"""
    try:
        # 1. å¸¦é€šæ»¤æ³¢ (5-15Hzï¼Œçªå‡ºQRSå¤åˆæ³¢)
        nyquist = sampling_rate / 2
        low_cutoff = 5 / nyquist
        high_cutoff = 15 / nyquist
        
        # ä½¿ç”¨Butterworthæ»¤æ³¢å™¨
        b, a = signal.butter(4, [low_cutoff, high_cutoff], btype='band')
        filtered_signal = signal.filtfilt(b, a, ecg_signal)
        
        # 2. å¾®åˆ†æ“ä½œï¼ˆçªå‡ºQRSæ–œç‡ï¼‰
        diff_signal = np.diff(filtered_signal)
        
        # 3. å¹³æ–¹æ“ä½œï¼ˆæ”¾å¤§å¤§çš„å˜åŒ–ï¼‰
        squared_signal = diff_signal ** 2
        
        # 4. ç§»åŠ¨çª—å£ç§¯åˆ†
        window_size = int(sampling_rate * 0.08)  # 80msçª—å£
        integrated_signal = np.convolve(squared_signal, np.ones(window_size), mode='same')
        
        # 5. è‡ªé€‚åº”é˜ˆå€¼æ£€æµ‹
        threshold = np.mean(integrated_signal) + 2 * np.std(integrated_signal)
        
        # 6. å³°å€¼æ£€æµ‹
        peaks = []
        min_distance = int(sampling_rate * 0.4)  # æœ€å°RRé—´æœŸ400ms
        
        for i in range(min_distance, len(integrated_signal) - min_distance):
            if (integrated_signal[i] > threshold and
                integrated_signal[i] == np.max(integrated_signal[i-min_distance//2:i+min_distance//2])):
                # åœ¨åŸå§‹ä¿¡å·ä¸­ç²¾ç¡®å®šä½Rå³°
                search_window = slice(max(0, i-20), min(len(ecg_signal), i+20))
                local_peak = i - 20 + np.argmax(np.abs(ecg_signal[search_window]))
                
                if not peaks or (local_peak - peaks[-1]) >= min_distance:
                    peaks.append(local_peak)
        
        return peaks
        
    except Exception as e:
        print(f"é«˜çº§Rå³°æ£€æµ‹é”™è¯¯ï¼Œä½¿ç”¨ç®€å•ç®—æ³•: {e}")
        return simple_r_peak_detection(ecg_signal, sampling_rate)

def simple_r_peak_detection(ecg_signal, sampling_rate):
    """ç®€å•çš„Rå³°æ£€æµ‹ç®—æ³•ï¼ˆå¤‡ç”¨ï¼‰"""
    try:
        signal_array = np.array(ecg_signal, dtype=float)
        
        # å»é™¤åŸºçº¿æ¼‚ç§»
        window_size = int(sampling_rate * 0.2)
        baseline = np.convolve(signal_array, np.ones(window_size)/window_size, mode='same')
        signal_filtered = signal_array - baseline
        
        # é˜ˆå€¼æ£€æµ‹
        threshold = np.std(signal_filtered) * 2.0
        peaks = []
        min_distance = int(sampling_rate * 0.3)
        
        for i in range(min_distance, len(signal_filtered) - min_distance):
            if (signal_filtered[i] > threshold and 
                signal_filtered[i] > signal_filtered[i-1] and 
                signal_filtered[i] > signal_filtered[i+1]):
                
                if not peaks or (i - peaks[-1]) >= min_distance:
                    peaks.append(i)
        
        return peaks
        
    except Exception as e:
        print(f"ç®€å•Rå³°æ£€æµ‹é”™è¯¯: {e}")
        return []

def calculate_advanced_hrv_metrics(r_peaks, sampling_rate):
    """è®¡ç®—é«˜çº§HRVæŒ‡æ ‡"""
    if len(r_peaks) < 3:
        return {}
    
    # RRé—´æœŸï¼ˆæ¯«ç§’ï¼‰
    rr_intervals = np.diff(r_peaks) / sampling_rate * 1000
    
    # æ—¶åŸŸæŒ‡æ ‡
    metrics = {
        'mean_rr': np.mean(rr_intervals),
        'std_rr': np.std(rr_intervals),  # SDNN
        'min_rr': np.min(rr_intervals),
        'max_rr': np.max(rr_intervals),
        'mean_hr': 60000 / np.mean(rr_intervals),
        'std_hr': np.std(60000 / rr_intervals)
    }
    
    # é«˜çº§æ—¶åŸŸæŒ‡æ ‡
    if len(rr_intervals) > 1:
        diff_rr = np.diff(rr_intervals)
        metrics['rmssd'] = np.sqrt(np.mean(diff_rr ** 2))
        
        # pNN50 å’Œ pNN20
        nn50 = np.sum(np.abs(diff_rr) > 50)
        nn20 = np.sum(np.abs(diff_rr) > 20)
        metrics['pnn50'] = (nn50 / len(diff_rr)) * 100
        metrics['pnn20'] = (nn20 / len(diff_rr)) * 100
        
        # ä¸‰è§’æŒ‡æ•°ï¼ˆè¿‘ä¼¼ï¼‰
        hist, _ = np.histogram(rr_intervals, bins=32)
        metrics['triangular_index'] = len(rr_intervals) / np.max(hist) if np.max(hist) > 0 else 0
        
        # TINNï¼ˆè¿‘ä¼¼ï¼‰
        metrics['tinn'] = np.max(rr_intervals) - np.min(rr_intervals)
    
    # å‡ ä½•æŒ‡æ ‡
    metrics['cv'] = (metrics['std_rr'] / metrics['mean_rr']) * 100  # å˜å¼‚ç³»æ•°
    
    return metrics

def analyze_signal_quality(ecg_signal, sampling_rate):
    """åˆ†æä¿¡å·è´¨é‡"""
    try:
        # ä¿¡å™ªæ¯”ä¼°è®¡
        signal_power = np.mean(ecg_signal ** 2)
        noise_estimate = np.mean(np.diff(ecg_signal) ** 2)  # ç®€å•å™ªå£°ä¼°è®¡
        snr = 10 * np.log10(signal_power / noise_estimate) if noise_estimate > 0 else float('inf')
        
        # åŸºçº¿æ¼‚ç§»æ£€æµ‹
        low_freq_content = np.mean(np.abs(ecg_signal[:int(sampling_rate)]))
        baseline_drift = low_freq_content / np.mean(np.abs(ecg_signal))
        
        # é¥±å’Œåº¦æ£€æµ‹
        max_val = np.max(np.abs(ecg_signal))
        saturation = np.sum(np.abs(ecg_signal) > 0.95 * max_val) / len(ecg_signal)
        
        quality = {
            'snr_db': snr,
            'baseline_drift_ratio': baseline_drift,
            'saturation_ratio': saturation * 100,
            'dynamic_range': np.max(ecg_signal) - np.min(ecg_signal)
        }
        
        return quality
        
    except Exception as e:
        print(f"ä¿¡å·è´¨é‡åˆ†æé”™è¯¯: {e}")
        return {}

def analyze_single_record_advanced(record_name, data_dir):
    """é«˜çº§åˆ†æå•ä¸ªECGè®°å½•"""
    print(f"\n=== é«˜çº§åˆ†æè®°å½•: {record_name} ===")
    
    header_path = os.path.join(data_dir, f"{record_name}.hea")
    mat_path = os.path.join(data_dir, f"{record_name}.mat")
    
    # è§£æå¤´æ–‡ä»¶
    header_info = parse_header_file(header_path)
    if not header_info:
        return None
    
    print(f"å¯¼è”æ•°: {header_info['num_leads']}")
    print(f"é‡‡æ ·ç‡: {header_info['sampling_rate']} Hz")
    print(f"æ—¶é•¿: {header_info['num_samples'] / header_info['sampling_rate']:.1f}ç§’")
    print(f"å¯¼è”: {header_info['leads']}")
    
    # è¯»å–æ•°æ®
    signal_data = read_mat_file(mat_path, header_info['num_leads'], header_info['num_samples'])
    if signal_data is None:
        return None
    
    # è½¬æ¢ä¸ºç‰©ç†å•ä½
    physical_data = convert_to_physical_units(signal_data, header_info['gains'], header_info['baselines'])
    
    # åˆ†ææ‰€æœ‰å¯¼è”
    lead_analyses = {}
    for i, lead_name in enumerate(header_info['leads']):
        ecg_signal = physical_data[:, i]
        
        # ä¿¡å·è´¨é‡åˆ†æ
        quality = analyze_signal_quality(ecg_signal, header_info['sampling_rate'])
        
        # Rå³°æ£€æµ‹
        r_peaks = advanced_r_peak_detection(ecg_signal, header_info['sampling_rate'])
        
        # HRVåˆ†æ
        hrv_metrics = calculate_advanced_hrv_metrics(r_peaks, header_info['sampling_rate'])
        
        lead_analyses[lead_name] = {
            'r_peaks_count': len(r_peaks),
            'quality': quality,
            **hrv_metrics
        }
        
        print(f"å¯¼è” {lead_name}: {len(r_peaks)}ä¸ªRå³°, SNR: {quality.get('snr_db', 0):.1f}dB")
    
    # é€‰æ‹©æœ€ä½³å¯¼è”è¿›è¡Œä¸»è¦åˆ†æï¼ˆåŸºäºä¿¡å·è´¨é‡ï¼‰
    best_lead = max(lead_analyses.keys(), 
                   key=lambda x: lead_analyses[x]['quality'].get('snr_db', 0))
    
    print(f"æœ€ä½³åˆ†æå¯¼è”: {best_lead}")
    
    # æ•´åˆç»“æœ
    result = {
        'record_name': record_name,
        'num_leads': header_info['num_leads'],
        'sampling_rate': header_info['sampling_rate'],
        'duration_sec': header_info['num_samples'] / header_info['sampling_rate'],
        'best_lead': best_lead,
        **{k: v for k, v in header_info.items() if k in ['age', 'sex', 'diagnosis']},
        **lead_analyses[best_lead]
    }
    
    # æ·»åŠ å¤šå¯¼è”ç»Ÿè®¡
    all_r_peaks = [lead_analyses[lead]['r_peaks_count'] for lead in header_info['leads']]
    result['mean_r_peaks_across_leads'] = np.mean(all_r_peaks)
    result['r_peaks_consistency'] = 1 - (np.std(all_r_peaks) / np.mean(all_r_peaks)) if np.mean(all_r_peaks) > 0 else 0
    
    print("âœ… é«˜çº§åˆ†æå®Œæˆ")
    return result

def analyze_directory_advanced(data_dir, output_file=None):
    """æ‰¹é‡é«˜çº§åˆ†æç›®å½•ä¸­çš„æ‰€æœ‰ECGè®°å½•"""
    print(f"ğŸ” é«˜çº§åˆ†æç›®å½•: {data_dir}")
    
    # è¯»å–RECORDSæ–‡ä»¶
    records_file = os.path.join(data_dir, 'RECORDS')
    if not os.path.exists(records_file):
        print("âŒ æœªæ‰¾åˆ°RECORDSæ–‡ä»¶")
        return None
    
    with open(records_file, 'r') as f:
        record_names = [line.strip() for line in f if line.strip()]
    
    print(f"æ‰¾åˆ° {len(record_names)} ä¸ªè®°å½•")
    
    # åˆ†ææ‰€æœ‰è®°å½•
    results = []
    successful = 0
    
    for i, record_name in enumerate(record_names, 1):
        print(f"\nè¿›åº¦: {i}/{len(record_names)}")
        result = analyze_single_record_advanced(record_name, data_dir)
        
        if result:
            results.append(result)
            successful += 1
    
    if results:
        df = pd.DataFrame(results)
        
        if output_file is None:
            output_file = os.path.join(data_dir, 'advanced_ecg_analysis_results.csv')
        
        df.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"\nğŸ“Š é«˜çº§åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        print(f"âœ… æˆåŠŸåˆ†æ: {successful}/{len(record_names)} ä¸ªè®°å½•")
        
        # é«˜çº§ç»Ÿè®¡æ‘˜è¦
        print(f"\nğŸ“ˆ é«˜çº§åˆ†ææ‘˜è¦:")
        print(f"å¹³å‡å¿ƒç‡: {df['mean_hr'].mean():.1f} Â± {df['mean_hr'].std():.1f} BPM")
        print(f"å¹³å‡SDNN: {df['std_rr'].mean():.1f} Â± {df['std_rr'].std():.1f} ms")
        print(f"å¹³å‡RMSSD: {df['rmssd'].mean():.1f} Â± {df['rmssd'].std():.1f} ms")
        print(f"å¹³å‡ä¿¡å™ªæ¯”: {df['snr_db'].mean():.1f} Â± {df['snr_db'].std():.1f} dB")
        print(f"Rå³°æ£€æµ‹ä¸€è‡´æ€§: {df['r_peaks_consistency'].mean():.3f}")
        
        if 'triangular_index' in df.columns:
            print(f"å¹³å‡ä¸‰è§’æŒ‡æ•°: {df['triangular_index'].mean():.1f}")
        
        # æœ€ä½³å¯¼è”ç»Ÿè®¡
        best_leads = df['best_lead'].value_counts()
        print(f"æœ€ä½³åˆ†æå¯¼è”åˆ†å¸ƒ: {dict(best_leads)}")
        
        return df
    else:
        print("âŒ æ²¡æœ‰æˆåŠŸåˆ†æçš„è®°å½•")
        return None

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="é«˜çº§ECGæ•°æ®åˆ†æå™¨")
    parser.add_argument("data_dir", help="ECGæ•°æ®ç›®å½•è·¯å¾„")
    parser.add_argument("--output", "-o", help="è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    if not os.path.exists(args.data_dir):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {args.data_dir}")
        exit(1)
    
    # æ£€æŸ¥scipyæ˜¯å¦å¯ç”¨
    try:
        from scipy import signal
        print("ğŸ¯ ä½¿ç”¨é«˜çº§ä¿¡å·å¤„ç†åŠŸèƒ½")
    except ImportError:
        print("âš ï¸  æœªå®‰è£…scipyï¼Œå°†ä½¿ç”¨åŸºç¡€ç®—æ³•")
    
    analyze_directory_advanced(args.data_dir, args.output)