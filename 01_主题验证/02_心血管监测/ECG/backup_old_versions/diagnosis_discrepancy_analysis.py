#!/usr/bin/env python3
"""
åŒ»å¸ˆè¯Šæ–­vsæ•°æ®é©±åŠ¨è¯Šæ–­å·®å¼‚æ ¹æœ¬åŸå› åˆ†æ
æ·±å…¥æ¢è®¨ä¸¤ç§è¯Šæ–­æ–¹æ³•çš„æœ¬è´¨å·®å¼‚
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import ast
import re

class DiagnosisDiscrepancyAnalyzer:
    def __init__(self):
        """åˆå§‹åŒ–å·®å¼‚åˆ†æå™¨"""
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
        plt.rcParams['axes.unicode_minus'] = False
        
        # ç–¾ç—…åˆ†ç±»ä½“ç³»
        self.disease_categories = {
            # å½¢æ€å­¦ä¾èµ–çš„ç–¾ç—…ï¼ˆéœ€è¦çœ‹æ³¢å½¢ï¼‰
            'morphology_dependent': {
                '59118001': 'å³æŸæ”¯é˜»æ»',    # éœ€è¦QRSå½¢æ€
                '429622005': 'å·¦æŸæ”¯é˜»æ»',   # éœ€è¦QRSå½¢æ€  
                '428750005': 'å·¦å‰åˆ†æ”¯é˜»æ»', # éœ€è¦ç”µè½´åˆ†æ
                '17338001': 'å·¦å¿ƒå®¤è‚¥åš',    # éœ€è¦ç”µå‹æ ‡å‡†
                '251173003': 'Tæ³¢å¼‚å¸¸',     # éœ€è¦Tæ³¢å½¢æ€
                '251199005': 'STæ®µå¼‚å¸¸',    # éœ€è¦STæ®µåˆ†æ
                '251146004': 'å¿ƒç”µè½´åç§»',   # éœ€è¦å‘é‡åˆ†æ
                '270492004': 'ä¸€åº¦æˆ¿å®¤é˜»æ»', # éœ€è¦PRé—´æœŸ
                '698252002': 'äºŒåº¦æˆ¿å®¤é˜»æ»', # éœ€è¦P-QRSå…³ç³»
                '164865005': 'å®Œå…¨æ€§æˆ¿å®¤é˜»æ»' # éœ€è¦æˆ¿å®¤åˆ†ç¦»åˆ†æ
            },
            
            # èŠ‚å¾‹ä¾èµ–çš„ç–¾ç—…ï¼ˆå¯ç”¨HRVåˆ†æï¼‰
            'rhythm_dependent': {
                '426177001': 'çª¦æ€§å¿ƒåŠ¨è¿‡ç¼“',  # å¿ƒç‡+è§„å¾‹æ€§
                '426783006': 'çª¦æ€§å¿ƒå¾‹',     # å¿ƒç‡+è§„å¾‹æ€§
                '427084000': 'å¿ƒæˆ¿é¢¤åŠ¨',     # å®Œå…¨ä¸è§„å¾‹
                '164889003': 'å¿ƒå¾‹ä¸é½',     # ä¸è§„å¾‹ç¨‹åº¦
                '164890007': 'å®¤æ€§å¿ƒå¾‹å¤±å¸¸', # ä¸¥é‡ä¸è§„å¾‹
                '284470004': 'å¿ƒæˆ¿æ—©æ',     # å¶å‘ä¸è§„å¾‹
                '39732003': 'å®¤æ€§æ—©æ',      # å¶å‘ä¸è§„å¾‹
                '59931005': 'æˆ¿å®¤äº¤ç•Œæ€§å¿ƒå¾‹' # ç‰¹æ®ŠèŠ‚å¾‹
            },
            
            # åŠŸèƒ½æ€§ç–¾ç—…ï¼ˆHRVæ•æ„Ÿï¼‰
            'functional_dependent': {
                '55827005': 'å¿ƒè‚Œç¼ºè¡€',     # è‡ªä¸»ç¥ç»åŠŸèƒ½å¼‚å¸¸
                '233917008': 'ä¸»åŠ¨è„‰ç“£ç‹­çª„', # è¡€æµåŠ¨åŠ›å­¦æ”¹å˜
                '164909002': 'å·¦å¿ƒæˆ¿æ‰©å¤§',   # å‹åŠ›è´Ÿè·
                '164912004': 'å³å¿ƒæˆ¿æ‰©å¤§',   # å‹åŠ›è´Ÿè·
                '164917005': 'åŒæˆ¿æ‰©å¤§'      # åŒä¾§è´Ÿè·
            },
            
            # éç‰¹å¼‚æ€§å¼‚å¸¸
            'nonspecific': {
                '164934002': 'å¿ƒç”µå›¾å¼‚å¸¸'    # æ³›æŒ‡å„ç§å¼‚å¸¸
            }
        }
    
    def analyze_diagnostic_paradigms(self, csv_file):
        """åˆ†æä¸¤ç§è¯Šæ–­èŒƒå¼çš„å·®å¼‚"""
        print("ğŸ” åˆ†æåŒ»å¸ˆè¯Šæ–­vsæ•°æ®é©±åŠ¨è¯Šæ–­çš„æ ¹æœ¬å·®å¼‚...")
        
        # è¯»å–åŸå§‹æ•°æ®å’Œè¯Šæ–­ç»“æœ
        original_df = pd.read_csv('enhanced_ecg_analysis_results.csv')
        comparison_df = pd.read_csv('ecg_diagnosis_comparison.csv')
        
        analysis_results = {
            'paradigm_differences': self._analyze_paradigm_differences(original_df, comparison_df),
            'information_gap': self._analyze_information_gap(original_df),
            'diagnostic_focus': self._analyze_diagnostic_focus(comparison_df),
            'accuracy_by_category': self._analyze_accuracy_by_category(comparison_df),
            'methodological_limitations': self._analyze_methodological_limitations(original_df)
        }
        
        return analysis_results
    
    def _analyze_paradigm_differences(self, original_df, comparison_df):
        """åˆ†æè¯Šæ–­èŒƒå¼å·®å¼‚"""
        print("\nğŸ“Š è¯Šæ–­èŒƒå¼å¯¹æ¯”åˆ†æ:")
        print("=" * 60)
        
        paradigm_analysis = {
            'åŒ»å¸ˆè¯Šæ–­ç‰¹ç‚¹': {
                'ä¿¡æ¯æ¥æº': 'å®Œæ•´12å¯¼è”ECGæ³¢å½¢å›¾',
                'åˆ†ææ–¹æ³•': 'å½¢æ€å­¦+èŠ‚å¾‹å­¦+ä¸´åºŠç»éªŒ',
                'è¯Šæ–­ä¾æ®': 'QRSæ³¢å½¢ã€ST-Tå˜åŒ–ã€PRé—´æœŸã€ç”µè½´',
                'ä¼˜åŠ¿': 'å…¨é¢ã€å‡†ç¡®ã€ç¬¦åˆåŒ»å­¦æ ‡å‡†',
                'å±€é™': 'ä¸»è§‚æ€§ã€ç»éªŒä¾èµ–ã€æ— æ³•é‡åŒ–HRV'
            },
            
            'æ•°æ®é©±åŠ¨è¯Šæ–­ç‰¹ç‚¹': {
                'ä¿¡æ¯æ¥æº': 'Rå³°æ—¶é—´åºåˆ—+HRVç»Ÿè®¡æŒ‡æ ‡',  
                'åˆ†ææ–¹æ³•': 'æ•°å€¼é˜ˆå€¼åˆ¤æ–­+ç»Ÿè®¡å­¦åˆ†æ',
                'è¯Šæ–­ä¾æ®': 'å¿ƒç‡ã€HRVæŒ‡æ ‡ã€ä¿¡å·è´¨é‡ã€ä¸€è‡´æ€§',
                'ä¼˜åŠ¿': 'å®¢è§‚ã€é‡åŒ–ã€è‡ªåŠ¨åŒ–ã€HRVæ•æ„Ÿ',
                'å±€é™': 'ç¼ºå°‘å½¢æ€å­¦ä¿¡æ¯ã€ç»éªŒä¸è¶³'
            }
        }
        
        print("ğŸ¥ åŒ»å¸ˆè¯Šæ–­èŒƒå¼:")
        for key, value in paradigm_analysis['åŒ»å¸ˆè¯Šæ–­ç‰¹ç‚¹'].items():
            print(f"  {key}: {value}")
        
        print("\nğŸ¤– æ•°æ®é©±åŠ¨è¯Šæ–­èŒƒå¼:")  
        for key, value in paradigm_analysis['æ•°æ®é©±åŠ¨è¯Šæ–­ç‰¹ç‚¹'].items():
            print(f"  {key}: {value}")
            
        return paradigm_analysis
    
    def _analyze_information_gap(self, df):
        """åˆ†æä¿¡æ¯ç¼ºå¤±é€ æˆçš„è¯Šæ–­å·®å¼‚"""
        print(f"\nğŸ” ä¿¡æ¯ç¼ºå¤±åˆ†æ:")
        print("-" * 40)
        
        # ç»Ÿè®¡æ¯ä¸ªç–¾ç—…ç±»åˆ«çš„å‡ºç°é¢‘æ¬¡
        all_diagnoses = []
        for _, row in df.iterrows():
            diagnosis = row['diagnosis']
            if pd.notna(diagnosis):
                codes = diagnosis.split(',')
                all_diagnoses.extend([code.strip() for code in codes])
        
        diagnosis_counts = Counter(all_diagnoses)
        
        # æŒ‰ç–¾ç—…ç±»åˆ«åˆ†ç»„åˆ†æ
        category_stats = {}
        for category, diseases in self.disease_categories.items():
            category_count = sum(diagnosis_counts.get(code, 0) for code in diseases.keys())
            category_stats[category] = {
                'count': category_count,
                'percentage': category_count / len(all_diagnoses) * 100,
                'diseases': diseases
            }
        
        print("æŒ‰ç–¾ç—…ç‰¹å¾åˆ†ç±»ç»Ÿè®¡:")
        for category, stats in category_stats.items():
            category_name = {
                'morphology_dependent': 'å½¢æ€å­¦ä¾èµ–ç–¾ç—…',
                'rhythm_dependent': 'èŠ‚å¾‹ä¾èµ–ç–¾ç—…', 
                'functional_dependent': 'åŠŸèƒ½æ€§ç–¾ç—…',
                'nonspecific': 'éç‰¹å¼‚æ€§å¼‚å¸¸'
            }.get(category, category)
            
            print(f"  {category_name}: {stats['count']}æ¬¡ ({stats['percentage']:.1f}%)")
            
            # æ˜¾ç¤ºå…·ä½“ç–¾ç—…
            for code, name in stats['diseases'].items():
                count = diagnosis_counts.get(code, 0)
                if count > 0:
                    print(f"    - {name}: {count}æ¬¡")
        
        # åˆ†æä¿¡æ¯ç¼ºå¤±çš„å½±å“
        morphology_ratio = category_stats['morphology_dependent']['percentage']
        rhythm_ratio = category_stats['rhythm_dependent']['percentage']
        
        info_gap_analysis = {
            'morphology_loss': f"å½¢æ€å­¦ç–¾ç—…å {morphology_ratio:.1f}%ï¼Œä½†HRVåˆ†ææ— æ³•è¯†åˆ«",
            'rhythm_advantage': f"èŠ‚å¾‹ç–¾ç—…å {rhythm_ratio:.1f}%ï¼ŒHRVåˆ†ææœ‰ä¼˜åŠ¿",
            'detection_capability': {
                'high': 'èŠ‚å¾‹ä¾èµ–ç–¾ç—…ï¼ˆå¿ƒæˆ¿é¢¤åŠ¨ã€å¿ƒå¾‹ä¸é½ï¼‰',
                'medium': 'åŠŸèƒ½æ€§ç–¾ç—…ï¼ˆå¿ƒè‚Œç¼ºè¡€ã€å¿ƒåŠ¨è¿‡ç¼“ï¼‰',
                'low': 'å½¢æ€å­¦ç–¾ç—…ï¼ˆä¼ å¯¼é˜»æ»ã€å¿ƒè‚ŒæŸä¼¤ï¼‰'
            }
        }
        
        print(f"\nå…³é”®å‘ç°:")
        print(f"  - {info_gap_analysis['morphology_loss']}")
        print(f"  - {info_gap_analysis['rhythm_advantage']}")
        
        return category_stats, info_gap_analysis
    
    def _analyze_diagnostic_focus(self, comparison_df):
        """åˆ†æä¸¤ç§è¯Šæ–­æ–¹æ³•çš„å…³æ³¨ç‚¹å·®å¼‚"""
        print(f"\nğŸ¯ è¯Šæ–­å…³æ³¨ç‚¹å·®å¼‚åˆ†æ:")
        print("-" * 40)
        
        # ç»Ÿè®¡åŸå§‹è¯Šæ–­å’Œé¢„æµ‹è¯Šæ–­çš„åˆ†å¸ƒ
        orig_diagnoses = []
        pred_diagnoses = []
        
        for _, row in comparison_df.iterrows():
            if pd.notna(row['original_diagnosis']):
                orig_codes = row['original_diagnosis'].split(',')
                orig_diagnoses.extend([code.strip() for code in orig_codes])
            
            if pd.notna(row['predicted_diagnosis']):
                pred_codes = row['predicted_diagnosis'].split(',')
                pred_diagnoses.extend([code.strip() for code in pred_codes])
        
        orig_counter = Counter(orig_diagnoses)
        pred_counter = Counter(pred_diagnoses)
        
        # æ‰¾å‡ºè¯Šæ–­å…³æ³¨ç‚¹çš„å·®å¼‚
        focus_differences = {
            'åŒ»å¸ˆå…³æ³¨ä½†ç®—æ³•å¿½ç•¥': [],
            'ç®—æ³•å…³æ³¨ä½†åŒ»å¸ˆè¾ƒå°‘': [],
            'åŒæ–¹éƒ½å…³æ³¨': []
        }
        
        all_codes = set(orig_counter.keys()) | set(pred_counter.keys())
        
        for code in all_codes:
            orig_count = orig_counter.get(code, 0)
            pred_count = pred_counter.get(code, 0)
            
            if orig_count >= 5 and pred_count < orig_count * 0.3:
                focus_differences['åŒ»å¸ˆå…³æ³¨ä½†ç®—æ³•å¿½ç•¥'].append((code, orig_count, pred_count))
            elif pred_count >= 5 and orig_count < pred_count * 0.3:
                focus_differences['ç®—æ³•å…³æ³¨ä½†åŒ»å¸ˆè¾ƒå°‘'].append((code, orig_count, pred_count))
            elif orig_count >= 3 and pred_count >= 3:
                focus_differences['åŒæ–¹éƒ½å…³æ³¨'].append((code, orig_count, pred_count))
        
        # è½¬æ¢ä¸ºä¸­æ–‡åç§°å¹¶æ˜¾ç¤º
        code_to_name = {}
        for category in self.disease_categories.values():
            code_to_name.update(category)
        
        for focus_type, codes_list in focus_differences.items():
            print(f"\n{focus_type}:")
            for code, orig_count, pred_count in codes_list:
                name = code_to_name.get(code, f"æœªçŸ¥({code})")
                print(f"  - {name}: åŒ»å¸ˆ{orig_count}æ¬¡, ç®—æ³•{pred_count}æ¬¡")
        
        return focus_differences, orig_counter, pred_counter
    
    def _analyze_accuracy_by_category(self, comparison_df):
        """æŒ‰ç–¾ç—…ç±»åˆ«åˆ†æè¯Šæ–­å‡†ç¡®æ€§"""
        print(f"\nğŸ“ˆ åˆ†ç–¾ç—…ç±»åˆ«å‡†ç¡®æ€§åˆ†æ:")
        print("-" * 40)
        
        category_accuracy = {}
        
        for category_name, diseases in self.disease_categories.items():
            category_results = {
                'total_cases': 0,
                'correct_detections': 0,
                'missed_detections': 0,
                'false_positives': 0,
                'accuracy_rate': 0
            }
            
            for _, row in comparison_df.iterrows():
                orig_codes = set(row['original_diagnosis'].split(',')) if pd.notna(row['original_diagnosis']) else set()
                pred_codes = set(row['predicted_diagnosis'].split(',')) if pd.notna(row['predicted_diagnosis']) else set()
                
                # è¯¥ç±»åˆ«åœ¨åŸå§‹è¯Šæ–­ä¸­çš„ç–¾ç—…
                orig_category_codes = orig_codes & diseases.keys()
                pred_category_codes = pred_codes & diseases.keys()
                
                if orig_category_codes:  # åŸå§‹è¯Šæ–­åŒ…å«è¯¥ç±»åˆ«ç–¾ç—…
                    category_results['total_cases'] += 1
                    
                    # è®¡ç®—æ­£ç¡®æ£€å‡ºã€æ¼æ£€
                    correctly_detected = len(orig_category_codes & pred_category_codes)
                    missed = len(orig_category_codes - pred_category_codes)
                    
                    if correctly_detected > 0:
                        category_results['correct_detections'] += 1
                    if missed > 0:
                        category_results['missed_detections'] += 1
                
                # è¯¯è¯Šæ£€æµ‹
                false_positive_codes = pred_category_codes - orig_codes
                if false_positive_codes:
                    category_results['false_positives'] += len(false_positive_codes)
            
            # è®¡ç®—å‡†ç¡®ç‡
            if category_results['total_cases'] > 0:
                category_results['accuracy_rate'] = category_results['correct_detections'] / category_results['total_cases']
            
            category_accuracy[category_name] = category_results
        
        # æ˜¾ç¤ºç»“æœ
        category_names = {
            'morphology_dependent': 'å½¢æ€å­¦ä¾èµ–ç–¾ç—…',
            'rhythm_dependent': 'èŠ‚å¾‹ä¾èµ–ç–¾ç—…',
            'functional_dependent': 'åŠŸèƒ½æ€§ç–¾ç—…', 
            'nonspecific': 'éç‰¹å¼‚æ€§å¼‚å¸¸'
        }
        
        for category, results in category_accuracy.items():
            if results['total_cases'] > 0:
                name = category_names.get(category, category)
                print(f"\n{name}:")
                print(f"  æ€»ç—…ä¾‹: {results['total_cases']}")
                print(f"  æ­£ç¡®æ£€å‡º: {results['correct_detections']} ({results['accuracy_rate']:.1%})")
                print(f"  æ¼æ£€: {results['missed_detections']}")
                print(f"  è¯¯æ£€: {results['false_positives']}")
        
        return category_accuracy
    
    def _analyze_methodological_limitations(self, df):
        """åˆ†ææ–¹æ³•å­¦å±€é™æ€§"""
        print(f"\nâš ï¸  æ–¹æ³•å­¦å±€é™æ€§åˆ†æ:")
        print("-" * 40)
        
        limitations = {
            'HRVæ–¹æ³•å±€é™æ€§': {
                'å½¢æ€å­¦ç›²åŒº': 'æ— æ³•åˆ†æQRSã€Pæ³¢ã€Tæ³¢å½¢æ€',
                'é—´æœŸæµ‹é‡ç¼ºå¤±': 'æ— æ³•æµ‹é‡PRã€QTã€QRSé—´æœŸ',
                'ç”µè½´åˆ†æç¼ºå¤±': 'æ— æ³•è¿›è¡Œå¿ƒç”µè½´è®¡ç®—',
                'å¯¼è”é—´å…³ç³»': 'ç¼ºå°‘å¯¼è”é—´ST-Tå¯¹åº”åˆ†æ',
                'åŠ¨æ€å˜åŒ–': 'ä»…10ç§’é™æ€åˆ†æï¼Œç¼ºå°‘åŠ¨æ€ç›‘æµ‹'
            },
            
            'åŒ»å¸ˆè¯Šæ–­ä¼˜åŠ¿': {
                'å½¢æ€å­¦ä¸“ä¸šæ€§': 'è®­ç»ƒæœ‰ç´ çš„æ³¢å½¢è¯†åˆ«èƒ½åŠ›',
                'ä¸´åºŠç›¸å…³æ€§': 'ç»“åˆæ‚£è€…ç—‡çŠ¶ã€ç—…å²ã€ä½“å¾',
                'ç»éªŒç§¯ç´¯': 'å¤§é‡ç—…ä¾‹ç§¯ç´¯çš„æ¨¡å¼è¯†åˆ«',
                'åŠ¨æ€åˆ¤æ–­': 'å¯ç»“åˆå¤šæ—¶é—´ç‚¹ECGå˜åŒ–',
                'è´¨é‡æ§åˆ¶': 'èƒ½è¯†åˆ«ä¼ªå·®ã€å¹²æ‰°ç­‰æŠ€æœ¯é—®é¢˜'
            },
            
            'HRVåˆ†æä¼˜åŠ¿': {
                'é‡åŒ–ç²¾ç¡®': 'æä¾›ç²¾ç¡®çš„æ•°å€¼åŒ–æŒ‡æ ‡',
                'è‡ªä¸»ç¥ç»': 'æ•æ„Ÿæ£€æµ‹è‡ªä¸»ç¥ç»åŠŸèƒ½çŠ¶æ€',
                'å®¢è§‚æ€§': 'é¿å…ä¸»è§‚åˆ¤æ–­å·®å¼‚',
                'æ‰¹é‡å¤„ç†': 'å¯è‡ªåŠ¨åˆ†æå¤§é‡æ•°æ®',
                'éšè—ä¿¡æ¯': 'å‘ç°è‚‰çœ¼éš¾ä»¥å¯Ÿè§‰çš„å¾®å°å˜åŒ–'
            }
        }
        
        for category, items in limitations.items():
            print(f"\n{category}:")
            for key, value in items.items():
                print(f"  - {key}: {value}")
        
        # æ•°æ®è´¨é‡å½±å“åˆ†æ
        print(f"\nğŸ“Š æ•°æ®è´¨é‡å¯¹è¯Šæ–­çš„å½±å“:")
        quality_stats = self._analyze_data_quality_impact(df)
        
        return limitations, quality_stats
    
    def _analyze_data_quality_impact(self, df):
        """åˆ†ææ•°æ®è´¨é‡å¯¹è¯Šæ–­å‡†ç¡®æ€§çš„å½±å“"""
        quality_impact = {}
        
        # è§£æä¿¡å·è´¨é‡æ•°æ®
        snr_values = []
        for _, row in df.iterrows():
            quality_str = row.get('quality', '{}')
            try:
                quality_str = str(quality_str).replace("'", '"')
                quality_str = re.sub(r'np\.float64\((.*?)\)', r'\1', quality_str)
                quality_dict = ast.literal_eval(quality_str)
                snr = quality_dict.get('snr_db', np.nan)
                if not pd.isna(snr):
                    snr_values.append(snr)
            except:
                continue
        
        if snr_values:
            quality_impact = {
                'snr_mean': np.mean(snr_values),
                'snr_std': np.std(snr_values),
                'snr_min': np.min(snr_values),
                'snr_max': np.max(snr_values),
                'poor_quality_ratio': np.sum(np.array(snr_values) < 10) / len(snr_values)
            }
            
            print(f"  ä¿¡å·è´¨é‡ç»Ÿè®¡ (SNR):")
            print(f"    å¹³å‡: {quality_impact['snr_mean']:.1f} dB")
            print(f"    èŒƒå›´: {quality_impact['snr_min']:.1f} - {quality_impact['snr_max']:.1f} dB")
            print(f"    è´¨é‡å·®å æ¯”: {quality_impact['poor_quality_ratio']:.1%} (SNR < 10dB)")
        
        return quality_impact
    
    def generate_comprehensive_report(self, csv_file):
        """ç”Ÿæˆç»¼åˆå·®å¼‚åˆ†ææŠ¥å‘Š"""
        analysis_results = self.analyze_diagnostic_paradigms(csv_file)
        
        # ç”Ÿæˆç»“è®º
        print(f"\nğŸ¯ å·®å¼‚æ ¹æœ¬åŸå› æ€»ç»“:")
        print("=" * 60)
        
        conclusions = [
            "1. ä¿¡æ¯ç»´åº¦å·®å¼‚ï¼šåŒ»å¸ˆçœ‹'å½¢æ€'ï¼Œç®—æ³•ç®—'èŠ‚å¾‹'",
            "2. ä¸“ä¸šçŸ¥è¯†å·®å¼‚ï¼šåŒ»å¸ˆæœ‰ä¸´åºŠç»éªŒï¼Œç®—æ³•ä¾èµ–ç»Ÿè®¡è§„å¾‹", 
            "3. è¯Šæ–­æ ‡å‡†å·®å¼‚ï¼šåŒ»å¸ˆéµå¾ªåŒ»å­¦æŒ‡å—ï¼Œç®—æ³•åŸºäºæ•°å€¼é˜ˆå€¼",
            "4. åº”ç”¨åœºæ™¯å·®å¼‚ï¼šåŒ»å¸ˆé€‚åˆç¡®è¯Šï¼Œç®—æ³•é€‚åˆç­›æŸ¥",
            "5. äº’è¡¥ä»·å€¼ï¼šç»“åˆä½¿ç”¨å¯æä¾›æ›´å…¨é¢çš„å¿ƒç”µè¯„ä¼°"
        ]
        
        for conclusion in conclusions:
            print(f"  {conclusion}")
        
        return analysis_results

def main():
    analyzer = DiagnosisDiscrepancyAnalyzer()
    results = analyzer.generate_comprehensive_report('enhanced_ecg_analysis_results.csv')
    return analyzer, results

if __name__ == '__main__':
    analyzer, results = main()