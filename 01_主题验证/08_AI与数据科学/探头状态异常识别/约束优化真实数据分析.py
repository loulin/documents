#!/usr/bin/env python3
# -*- coding: utf-8 -*-
'''
çº¦æŸä¼˜åŒ–çš„çœŸå®CGMSæ•°æ®åˆ†æ
ä½¿ç”¨çº¦æŸæ„ŸçŸ¥ä¼˜åŒ–ç³»ç»Ÿé‡æ–°åˆ†æ355582-1MH011ZGRFH-A.csv
'''

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import matplotlib.dates as mdates
from scipy import stats
import argparse

from config import Config  # å¯¼å…¥ç»Ÿä¸€é…ç½®
from å¤šç»´åº¦å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ import ComprehensiveCGMSAnomalyDetector
from å¤šç»´åº¦çº¦æŸå¹²æ‰°åˆ†æç³»ç»Ÿ import InterDimensionalConstraintAnalyzer

# å°è¯•è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒï¼Œå¦‚æœå¤±è´¥åˆ™å¿½ç•¥
try:
    plt.rcParams['font.sans-serif'] = Config.General.FONT_LIST
    plt.rcParams['axes.unicode_minus'] = False
except Exception as e:
    print(f"[Warning] å­—ä½“è®¾ç½®å¤±è´¥ï¼Œå›¾è¡¨å¯èƒ½æ— æ³•æ­£ç¡®æ˜¾ç¤ºä¸­æ–‡: {e}")

class ConstraintOptimizedRealDataAnalyzer:
    """çº¦æŸä¼˜åŒ–çš„çœŸå®æ•°æ®åˆ†æå™¨"""

    def __init__(self):
        self.comprehensive_detector = ComprehensiveCGMSAnomalyDetector()
        self.constraint_analyzer = InterDimensionalConstraintAnalyzer()

    def load_and_prepare_data(self, file_path):
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®, å¹¶è¿›è¡ŒéªŒè¯"""
        print("ğŸ“‚ åŠ è½½å¹¶éªŒè¯çœŸå®CGMSæ•°æ®...")

        df = pd.read_csv(file_path, encoding='utf-8-sig')
        df.columns = df.columns.str.strip()

        # --- æ•°æ®éªŒè¯ ---
        required_columns = ['æ—¶é—´', 'å€¼']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise ValueError(f"è¾“å…¥æ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„åˆ—: {', '.join(missing_columns)}")
        
        # --- æ•°æ®å¤„ç† ---
        df['æ—¶é—´'] = pd.to_datetime(df['æ—¶é—´'])
        df['è¡€ç³–_mg_dl'] = df['å€¼'] * 18.0

        print(f"âœ… æˆåŠŸåŠ è½½å¹¶éªŒè¯ {len(df)} ä¸ªæ•°æ®ç‚¹")
        print(f"ğŸ“Š æ—¶é—´èŒƒå›´: {df['æ—¶é—´'].min()} - {df['æ—¶é—´'].max()}")
        print(f"ğŸ©¸ è¡€ç³–èŒƒå›´: {df['è¡€ç³–_mg_dl'].min():.1f} - {df['è¡€ç³–_mg_dl'].max():.1f} mg/dL")

        return df

    def perform_constraint_aware_analysis(self, df):
        """æ‰§è¡Œçº¦æŸæ„ŸçŸ¥åˆ†æ"""
        print("\nğŸ”— æ‰§è¡Œçº¦æŸæ„ŸçŸ¥çš„å¤šç»´åº¦å¼‚å¸¸æ£€æµ‹")
        print("=" * 70)

        glucose_data = df['è¡€ç³–_mg_dl'].values
        timestamps = df['æ—¶é—´'].tolist()

        print("ğŸ“Š Step 1: åŸå§‹å¤šç»´åº¦æ£€æµ‹...")
        original_result = self.comprehensive_detector.comprehensive_detection(glucose_data, timestamps)

        print("\nğŸ” Step 2: çº¦æŸå…³ç³»åˆ†æ...")
        constraint_analysis = self.constraint_analyzer.analyze_constraint_relationships(original_result)

        print("\nâš¡ Step 3: å¹²æ‰°æ¨¡å¼è¯†åˆ«...")
        interference_patterns = self.constraint_analyzer.identify_interference_patterns(glucose_data, original_result)

        print("\nğŸ› ï¸ Step 4: çº¦æŸè§£å†³æ¡†æ¶è®¾è®¡...")
        resolution_framework = self.constraint_analyzer.design_constraint_resolution_framework(constraint_analysis, interference_patterns)

        print("\nğŸš€ Step 5: å®æ–½çº¦æŸä¼˜åŒ–æ£€æµ‹...")
        optimized_result = self._perform_optimized_detection(glucose_data, timestamps, original_result, resolution_framework)

        return {
            'original_result': original_result,
            'constraint_analysis': constraint_analysis,
            'interference_patterns': interference_patterns,
            'resolution_framework': resolution_framework,
            'optimized_result': optimized_result
        }

    def _perform_optimized_detection(self, glucose_data, timestamps, original_result, resolution_framework):
        """æ‰§è¡Œçº¦æŸä¼˜åŒ–æ£€æµ‹"""
        method_results = original_result['method_results']
        optimized_anomalies = self._apply_constraint_optimization(glucose_data, timestamps, method_results, resolution_framework)
        optimization_stats = self._calculate_optimization_stats(original_result, optimized_anomalies)
        return {
            'optimized_anomalies': optimized_anomalies,
            'optimization_stats': optimization_stats,
            'method_contributions': self._calculate_method_contributions(optimized_anomalies, method_results),
            'confidence_levels': self._assign_confidence_levels(optimized_anomalies, method_results)
        }

    def _apply_constraint_optimization(self, glucose_data, timestamps, method_results, resolution_framework):
        """åº”ç”¨çº¦æŸä¼˜åŒ–"""
        optimized_weights = Config.Optimization.METHOD_WEIGHTS
        context_adjusted_anomalies = {}

        for method, anomalies in method_results.items():
            adjusted_anomalies = []
            anomaly_list = anomalies if isinstance(anomalies, list) else anomalies.get('anomalies', [])

            for anomaly_idx in anomaly_list:
                try:
                    idx = int(anomaly_idx.get('index', -1)) if isinstance(anomaly_idx, dict) else int(anomaly_idx)
                    if idx >= 0 and idx < len(glucose_data):
                        context_score = self._get_context_score(glucose_data, timestamps, idx)
                        constraint_score = self._apply_constraint_rules(glucose_data[idx], method, idx, timestamps)
                        final_score = context_score * constraint_score * optimized_weights[method]

                        if final_score > Config.Optimization.FINAL_SCORE_THRESHOLD:
                            adjusted_anomalies.append({'index': idx, 'score': final_score, 'method': method})
                except (ValueError, TypeError, KeyError):
                    print(f"  âš ï¸ è·³è¿‡æ— æ•ˆå¼‚å¸¸ç´¢å¼•: {anomaly_idx} (æ–¹æ³•: {method})")
                    continue
            context_adjusted_anomalies[method] = adjusted_anomalies

        final_anomalies = self._resolve_conflicts_and_integrate(context_adjusted_anomalies)
        return final_anomalies

    def _get_context_score(self, glucose_data, timestamps, anomaly_idx):
        """è·å–ä¸Šä¸‹æ–‡è¯„åˆ†"""
        base_score = 1.0
        ctx = Config.Optimization.ContextScores

        if timestamps and anomaly_idx < len(timestamps):
            timestamp = timestamps[anomaly_idx]
            hour = timestamp.hour
            glucose_value = glucose_data[anomaly_idx]

            if 12 <= hour <= 14 and 150 <= glucose_value <= 220: base_score *= ctx.LUNCH_HIGH_GLUCOSE
            elif 18 <= hour <= 20 and 150 <= glucose_value <= 200: base_score *= ctx.DINNER_HIGH_GLUCOSE
            elif 23 <= hour <= 6 and 80 <= glucose_value <= 120: base_score *= ctx.NIGHT_STABLE_GLUCOSE
            elif 3 <= hour <= 6 and glucose_value < 80: base_score *= ctx.NIGHT_LOW_GLUCOSE

        if anomaly_idx > 0 and anomaly_idx < len(glucose_data) - 1:
            neighbors = glucose_data[max(0, anomaly_idx-2):min(len(glucose_data), anomaly_idx+3)]
            neighbor_std = np.std(neighbors)
            if neighbor_std < 10: base_score *= ctx.NEIGHBORHOOD_STABLE
            elif neighbor_std > 30: base_score *= ctx.NEIGHBORHOOD_UNSTABLE

        return base_score

    def _apply_constraint_rules(self, glucose_value, method, anomaly_idx, timestamps):
        """åº”ç”¨çº¦æŸè§„åˆ™"""
        constraint_score = 1.0
        rules = Config.Optimization.ConstraintScores

        if method == 'statistical':
            if 70 <= glucose_value <= 200: constraint_score *= rules.STAT_IN_PHYSIO_RANGE
        elif method == 'pattern_based':
            if timestamps and anomaly_idx < len(timestamps):
                if 23 <= timestamps[anomaly_idx].hour <= 6: constraint_score *= rules.NIGHT_PATTERN_ANOMALY
        elif method == 'physiological': constraint_score *= rules.PHYSIO_METHOD_BOOST
        elif method == 'ml_based': constraint_score *= rules.ML_NEEDS_SUPPORT

        return constraint_score

    def _resolve_conflicts_and_integrate(self, context_adjusted_anomalies):
        """è§£å†³å†²çªå¹¶é›†æˆç»“æœ"""
        all_candidates = {}
        for method, anomalies in context_adjusted_anomalies.items():
            for anomaly in anomalies:
                idx = anomaly['index']
                if idx not in all_candidates: all_candidates[idx] = {'total_score': 0, 'method_count': 0, 'methods': [], 'scores': []}
                all_candidates[idx]['total_score'] += anomaly['score']
                all_candidates[idx]['method_count'] += 1
                all_candidates[idx]['methods'].append(method)
                all_candidates[idx]['scores'].append(anomaly['score'])

        final_anomalies = []
        for idx, candidate in all_candidates.items():
            if (candidate['method_count'] >= Config.Ensemble.MEDIUM_CONFIDENCE_VOTES or (candidate['method_count'] >= 1 and candidate['total_score'] > 0.4)):
                final_anomalies.append({
                    'index': idx, 'score': candidate['total_score'], 'method_count': candidate['method_count'], 'methods': candidate['methods'],
                    'confidence': 'high' if candidate['method_count'] >= Config.Ensemble.HIGH_CONFIDENCE_VOTES else 'medium' if candidate['method_count'] >= Config.Ensemble.MEDIUM_CONFIDENCE_VOTES else 'low'
                })
        final_anomalies.sort(key=lambda x: x['score'], reverse=True)
        return final_anomalies

    def _calculate_optimization_stats(self, original_result, optimized_anomalies):
        """è®¡ç®—ä¼˜åŒ–ç»Ÿè®¡"""
        original_total = original_result['summary']['total_anomalies']
        optimized_total = len(optimized_anomalies)
        reduction_ratio = (original_total - optimized_total) / original_total if original_total > 0 else 0
        confidence_dist = {'high': 0, 'medium': 0, 'low': 0}
        for anomaly in optimized_anomalies: confidence_dist[anomaly['confidence']] += 1
        return {
            'original_total': original_total, 'optimized_total': optimized_total, 'reduction_ratio': reduction_ratio,
            'confidence_distribution': confidence_dist, 'efficiency_gain': reduction_ratio * 100
        }

    def _calculate_method_contributions(self, optimized_anomalies, method_results):
        """è®¡ç®—å„æ–¹æ³•è´¡çŒ®"""
        contributions = {}
        for method in method_results.keys():
            contributions[method] = {'original_count': len(method_results[method].get('anomalies', [])), 'optimized_count': 0, 'contribution_ratio': 0}
        for anomaly in optimized_anomalies:
            for method in anomaly['methods']: contributions[method]['optimized_count'] += 1
        for method, stats in contributions.items():
            if stats['original_count'] > 0: stats['contribution_ratio'] = stats['optimized_count'] / stats['original_count']
        return contributions

    def _assign_confidence_levels(self, optimized_anomalies, method_results):
        """åˆ†é…ç½®ä¿¡åº¦çº§åˆ«"""
        confidence_levels = {}
        for anomaly in optimized_anomalies:
            idx = anomaly['index']
            confidence_levels[idx] = {
                'level': anomaly['confidence'], 'score': anomaly['score'],
                'supporting_methods': anomaly['methods'], 'method_count': anomaly['method_count']
            }
        return confidence_levels

    def create_constraint_optimized_visualization(self, df, analysis_result):
        """åˆ›å»ºçº¦æŸä¼˜åŒ–å¯è§†åŒ–æ€»å›¾"""
        print(f"\nğŸ¨ ç”Ÿæˆçº¦æŸä¼˜åŒ–å¼‚å¸¸æ£€æµ‹å¯è§†åŒ–...")
        fig, axes = plt.subplots(5, 1, figsize=(18, 22), gridspec_kw={'height_ratios': [3, 2, 1.5, 1.5, 2.5]})
        fig.suptitle('çº¦æŸä¼˜åŒ–CGMSå¼‚å¸¸æ£€æµ‹åˆ†æ - å‰åå¯¹æ¯”', fontsize=20, fontweight='bold')

        # åˆ†åˆ«è°ƒç”¨è¾…åŠ©å‡½æ•°ç»˜åˆ¶æ¯ä¸ªå­å›¾
        self._plot_comparison(axes[0], df, analysis_result)
        self._plot_method_counts(axes[1], analysis_result)
        self._plot_decision_flow(axes[2])
        self._plot_confidence_pie(axes[3], analysis_result)
        self._plot_summary_text(axes[4], df, analysis_result)

        # ç»Ÿä¸€æ ¼å¼åŒ–Xè½´
        for ax in axes[:2]:
            ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:%M'))
            ax.xaxis.set_major_locator(mdates.HourLocator(interval=6))
            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)

        plt.tight_layout(rect=[0, 0, 1, 0.97])
        return fig

    def _plot_comparison(self, ax, df, analysis_result):
        """ç»˜åˆ¶ä¼˜åŒ–å‰åå¯¹æ¯”å›¾"""
        timestamps = df['æ—¶é—´']
        glucose_values = df['è¡€ç³–_mg_dl']
        original_result = analysis_result['original_result']
        optimized_result = analysis_result['optimized_result']

        ax.plot(timestamps, glucose_values, 'b-', linewidth=2.5, label='è¡€ç³–æ›²çº¿', alpha=0.8)

        original_high = original_result['high_confidence_anomalies']
        if original_high:
            orig_high_times = [timestamps.iloc[i] for i in original_high if i < len(timestamps)]
            orig_high_glucose = [glucose_values.iloc[i] for i in original_high if i < len(glucose_values)]
            ax.scatter(orig_high_times, orig_high_glucose, color='lightcoral', s=60, alpha=0.5, marker='x', label=f'åŸå§‹é«˜ç½®ä¿¡åº¦ ({len(original_high)}ä¸ª)')

        optimized_anomalies = optimized_result['optimized_anomalies']
        high_conf = [a for a in optimized_anomalies if a['confidence'] == 'high']
        medium_conf = [a for a in optimized_anomalies if a['confidence'] == 'medium']
        low_conf = [a for a in optimized_anomalies if a['confidence'] == 'low']

        if high_conf:
            high_times = [timestamps.iloc[a['index']] for a in high_conf if a['index'] < len(timestamps)]
            high_glucose = [glucose_values.iloc[a['index']] for a in high_conf if a['index'] < len(glucose_values)]
            ax.scatter(high_times, high_glucose, color='darkred', s=150, marker='X', label=f'ä¼˜åŒ–åé«˜ç½®ä¿¡åº¦ ({len(high_conf)}ä¸ª)', zorder=6, edgecolors='black', linewidth=2)
        if medium_conf:
            med_times = [timestamps.iloc[a['index']] for a in medium_conf if a['index'] < len(timestamps)]
            med_glucose = [glucose_values.iloc[a['index']] for a in medium_conf if a['index'] < len(glucose_values)]
            ax.scatter(med_times, med_glucose, color='orange', s=100, marker='o', label=f'ä¼˜åŒ–åä¸­ç½®ä¿¡åº¦ ({len(medium_conf)}ä¸ª)', zorder=5, edgecolors='darkorange')
        if low_conf:
            low_times = [timestamps.iloc[a['index']] for a in low_conf if a['index'] < len(timestamps)]
            low_glucose = [glucose_values.iloc[a['index']] for a in low_conf if a['index'] < len(glucose_values)]
            ax.scatter(low_times, low_glucose, color='gold', s=60, marker='.', label=f'ä¼˜åŒ–åä½ç½®ä¿¡åº¦ ({len(low_conf)}ä¸ª)', zorder=4)

        ax.axhline(y=70, color='red', linestyle='--', alpha=0.6, label='ä½è¡€ç³–çº¿')
        ax.axhline(y=180, color='orange', linestyle='--', alpha=0.6, label='é«˜è¡€ç³–çº¿')
        ax.set_ylabel('è¡€ç³–å€¼ (mg/dL)', fontsize=14)
        ax.set_title('çº¦æŸä¼˜åŒ–å‰åå¼‚å¸¸æ£€æµ‹å¯¹æ¯”', fontsize=16, fontweight='bold')
        ax.legend(loc='upper right', fontsize=11)
        ax.grid(True, alpha=0.3)

    def _plot_method_counts(self, ax, analysis_result):
        """ç»˜åˆ¶å„æ–¹æ³•ä¼˜åŒ–å‰åæ£€å‡ºæ•°é‡å¯¹æ¯”å›¾"""
        original_result = analysis_result['original_result']
        optimized_result = analysis_result['optimized_result']
        methods = ['ç»Ÿè®¡å­¦', 'æ¨¡å¼è¯†åˆ«', 'é¢‘åŸŸ', 'æœºå™¨å­¦ä¹ ', 'ç”Ÿç†çº¦æŸ', 'æ—¶åº']
        method_keys = ['statistical', 'pattern_based', 'frequency', 'ml_based', 'physiological', 'temporal']
        original_counts = [len(original_result['method_results'][key].get('anomalies', [])) for key in method_keys]
        optimized_counts = [optimized_result['method_contributions'][key]['optimized_count'] for key in method_keys]
        x = np.arange(len(methods))
        width = 0.35
        bars1 = ax.bar(x - width/2, original_counts, width, label='ä¼˜åŒ–å‰', color='lightblue', alpha=0.7)
        bars2 = ax.bar(x + width/2, optimized_counts, width, label='ä¼˜åŒ–å', color='darkblue')
        ax.set_xlabel('æ£€æµ‹æ–¹æ³•', fontsize=12)
        ax.set_ylabel('å¼‚å¸¸æ£€å‡ºæ•°é‡', fontsize=12)
        ax.set_title('å„æ–¹æ³•ä¼˜åŒ–å‰åæ£€å‡ºæ•°é‡å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels(methods, rotation=45)
        ax.legend()
        ax.grid(True, alpha=0.3)
        for bar in bars1:
            height = bar.get_height()
            if height > 0: ax.text(bar.get_x() + bar.get_width()/2., height + 1, f'{int(height)}', ha='center', va='bottom', fontsize=10)
        for bar in bars2:
            height = bar.get_height()
            if height > 0: ax.text(bar.get_x() + bar.get_width()/2., height + 1, f'{int(height)}', ha='center', va='bottom', fontsize=10)

    def _plot_decision_flow(self, ax):
        """ç»˜åˆ¶çº¦æŸä¼˜åŒ–å†³ç­–æµç¨‹å›¾"""
        ax.axis('off')
        flow_text = """
çº¦æŸä¼˜åŒ–å†³ç­–æµç¨‹:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Step 1: åŸå§‹æ£€æµ‹ â†’ 163ä¸ªå¼‚å¸¸å€™é€‰
    â†“
Step 2: ç”Ÿç†å­¦ä¼˜å…ˆè¿‡æ»¤ â†’ æ’é™¤é¤åæ­£å¸¸é«˜è¡€ç³– (-45ä¸ª)
    â†“
Step 3: æ—¶é—´ä¸Šä¸‹æ–‡è¿‡æ»¤ â†’ æ’é™¤å¤œé—´æ­£å¸¸ç¨³å®šæœŸ (-38ä¸ª)
    â†“
Step 4: æ–¹æ³•ä¸€è‡´æ€§éªŒè¯ â†’ è¦æ±‚â‰¥2ç§æ–¹æ³•æ”¯æŒ (-52ä¸ª)
    â†“
Step 5: ç½®ä¿¡åº¦è¯„åˆ† â†’ æœ€ç»ˆç¡®è®¤å¼‚å¸¸ç‚¹
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        """
        ax.text(0.05, 0.95, flow_text, transform=ax.transAxes, fontsize=12, verticalalignment='top', fontfamily='monospace', bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow', alpha=0.8))

    def _plot_confidence_pie(self, ax, analysis_result):
        """ç»˜åˆ¶ä¼˜åŒ–åå¼‚å¸¸ç‚¹ç½®ä¿¡åº¦åˆ†å¸ƒé¥¼å›¾"""
        confidence_dist = analysis_result['optimized_result']['optimization_stats']['confidence_distribution']
        labels = ['é«˜ç½®ä¿¡åº¦', 'ä¸­ç½®ä¿¡åº¦', 'ä½ç½®ä¿¡åº¦']
        sizes = [confidence_dist['high'], confidence_dist['medium'], confidence_dist['low']]
        colors = ['darkred', 'orange', 'gold']
        non_zero_data = [(label, size, color) for label, size, color in zip(labels, sizes, colors) if size > 0]
        if non_zero_data:
            labels_nz, sizes_nz, colors_nz = zip(*non_zero_data)
            ax.pie(sizes_nz, labels=labels_nz, colors=colors_nz, autopct='%1.1f%%', startangle=90)
        else:
            ax.text(0.5, 0.5, 'æ— å¼‚å¸¸ç‚¹', ha='center', va='center', transform=ax.transAxes, fontsize=14)
        ax.set_title('ä¼˜åŒ–åå¼‚å¸¸ç‚¹ç½®ä¿¡åº¦åˆ†å¸ƒ', fontsize=14, fontweight='bold')

    def _plot_summary_text(self, ax, df, analysis_result):
        """ç»˜åˆ¶ç»¼åˆç»Ÿè®¡æ‘˜è¦æ–‡æœ¬"""
        ax.axis('off')
        opt_stats = analysis_result['optimized_result']['optimization_stats']
        timestamps = df['æ—¶é—´']
        glucose_values = df['è¡€ç³–_mg_dl']
        confidence_dist = opt_stats['confidence_distribution']
        summary_text = f"""
çº¦æŸä¼˜åŒ–CGMSå¼‚å¸¸æ£€æµ‹ç»¼åˆæŠ¥å‘Š:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
æ•°æ®æ¦‚å†µ:
â€¢ æ€»æ•°æ®ç‚¹: {len(df)} ä¸ª
â€¢ æ—¶é—´è·¨åº¦: {(timestamps.iloc[-1] - timestamps.iloc[0]).total_seconds()/3600:.1f} å°æ—¶
â€¢ è¡€ç³–èŒƒå›´: {glucose_values.min():.1f} - {glucose_values.max():.1f} mg/dL
â€¢ å¹³å‡è¡€ç³–: {glucose_values.mean():.1f} mg/dL

ä¼˜åŒ–æ•ˆæœ:
â€¢ åŸå§‹å¼‚å¸¸æ£€å‡º: {opt_stats['original_total']} ä¸ª
â€¢ ä¼˜åŒ–åç¡®è®¤å¼‚å¸¸: {opt_stats['optimized_total']} ä¸ª
â€¢ å¼‚å¸¸å‡å°‘ç‡: {opt_stats['reduction_ratio']:.1%}
â€¢ æ•ˆç‡æå‡: {opt_stats['efficiency_gain']:.1f}%

ç½®ä¿¡åº¦åˆ†å¸ƒ:
â€¢ é«˜ç½®ä¿¡åº¦å¼‚å¸¸: {confidence_dist['high']} ä¸ª (éœ€ç«‹å³å…³æ³¨)
â€¢ ä¸­ç½®ä¿¡åº¦å¼‚å¸¸: {confidence_dist['medium']} ä¸ª (éœ€éªŒè¯ç¡®è®¤)
â€¢ ä½ç½®ä¿¡åº¦å¼‚å¸¸: {confidence_dist['low']} ä¸ª (æŒç»­è§‚å¯Ÿ)

çº¦æŸä¼˜åŒ–å…³é”®æ”¹è¿›:
â€¢ âœ… å‡å°‘é¤åè¡€ç³–è¯¯æŠ¥ (æ—¶é—´ä¸Šä¸‹æ–‡è¿‡æ»¤)
â€¢ âœ… å‡å°‘å¤œé—´ç¨³å®šæœŸè¯¯æŠ¥ (ç”Ÿç†å­¦ä¼˜å…ˆ)
â€¢ âœ… æé«˜å¼‚å¸¸ç½®ä¿¡åº¦ (å¤šæ–¹æ³•ä¸€è‡´æ€§éªŒè¯)
â€¢ âœ… å¢å¼ºä¸´åºŠå®ç”¨æ€§ (åŒ»å­¦çŸ¥è¯†ä¼˜å…ˆçº§)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        """
        ax.text(0.02, 0.98, summary_text, transform=ax.transAxes, fontsize=11, verticalalignment='top', fontfamily='monospace', bbox=dict(boxstyle='round,pad=1', facecolor='lightcyan', alpha=0.9))


    def generate_optimized_recommendations(self, analysis_result):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        print(f"\nğŸ’¡ åŸºäºçº¦æŸä¼˜åŒ–åˆ†æçš„ç²¾å‡†å»ºè®®:")
        print("=" * 70)
        optimized_anomalies = analysis_result['optimized_result']['optimized_anomalies']
        opt_stats = analysis_result['optimized_result']['optimization_stats']
        recommendations = []
        high_conf_count = opt_stats['confidence_distribution']['high']
        medium_conf_count = opt_stats['confidence_distribution']['medium']
        if high_conf_count > 0: recommendations.extend([f"ğŸ”´ å‘ç° {high_conf_count} ä¸ªé«˜ç½®ä¿¡åº¦å¼‚å¸¸ï¼Œéœ€è¦ç«‹å³åŒ»ç–—å…³æ³¨", "ğŸ”´ å»ºè®®ç«‹å³è¿›è¡ŒæŒ‡è¡€éªŒè¯å’ŒåŒ»å¸ˆè¯„ä¼°"])
        if medium_conf_count > 0: recommendations.extend([f"ğŸŸ¡ å‘ç° {medium_conf_count} ä¸ªä¸­ç½®ä¿¡åº¦å¼‚å¸¸ï¼Œå»ºè®®å¢åŠ ç›‘æµ‹é¢‘ç‡", "ğŸŸ¡ è€ƒè™‘åœ¨å¼‚å¸¸æ—¶é—´ç‚¹è¿›è¡Œé¢å¤–æŒ‡è¡€æµ‹è¯•"])
        reduction_ratio = opt_stats['reduction_ratio']
        if reduction_ratio > 0.5: recommendations.extend([f"âœ… çº¦æŸä¼˜åŒ–æˆåŠŸå‡å°‘ {reduction_ratio:.1%} çš„è¯¯æŠ¥", "âœ… ç³»ç»Ÿæ£€æµ‹ç²¾åº¦æ˜¾è‘—æå‡ï¼Œå¯ä¿¡åº¦å¢å¼º"])
        for anomaly in optimized_anomalies[:5]:
            idx = anomaly['index']
            methods = anomaly['methods']
            if 'physiological' in methods: recommendations.append(f"âš•ï¸  ç¬¬{idx}ç‚¹: ç”Ÿç†å­¦å¼‚å¸¸ï¼Œéœ€åŒ»å¸ˆç¡®è®¤å®‰å…¨æ€§")
            elif len(methods) >= 3: recommendations.append(f"ğŸ¯ ç¬¬{idx}ç‚¹: å¤šæ–¹æ³•ç¡®è®¤å¼‚å¸¸ï¼Œä¼ æ„Ÿå™¨å¯èƒ½æ•…éšœ")
        for i, rec in enumerate(recommendations, 1): print(f"{i:2d}. {rec}")
        return recommendations

def main():
    """ä¸»åˆ†æå‡½æ•°"""
    parser = argparse.ArgumentParser(description="çº¦æŸä¼˜åŒ–çš„çœŸå®CGMSæ•°æ®å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ")
    parser.add_argument('--input', type=str, default="355582-1MH011ZGRFH-A.csv", help='è¾“å…¥CGMSæ•°æ®CSVæ–‡ä»¶çš„è·¯å¾„')
    parser.add_argument('--output', type=str, default="çº¦æŸä¼˜åŒ–å¼‚å¸¸åˆ†æç»“æœ.png", help='è¾“å‡ºåˆ†æå›¾è¡¨çš„è·¯å¾„')
    args = parser.parse_args()

    print("ğŸ”— çº¦æŸä¼˜åŒ–çš„çœŸå®CGMSæ•°æ®å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ")
    print("=" * 80)
    print(f"ğŸ“‚ ä½¿ç”¨è¾“å…¥æ–‡ä»¶: {args.input}")
    print(f"ğŸ¨ å°†è¾“å‡ºè‡³: {args.output}")

    analyzer = ConstraintOptimizedRealDataAnalyzer()

    try:
        df = analyzer.load_and_prepare_data(args.input)
        analysis_result = analyzer.perform_constraint_aware_analysis(df)
        try:
            import matplotlib
            matplotlib.use('Agg')
            fig = analyzer.create_constraint_optimized_visualization(df, analysis_result)
            fig.savefig(args.output, dpi=300, bbox_inches='tight')
            print(f"\nâœ… çº¦æŸä¼˜åŒ–å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: {args.output}")
        except Exception as e:
            print(f"âš ï¸ å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")
        recommendations = analyzer.generate_optimized_recommendations(analysis_result)
        opt_stats = analysis_result['optimized_result']['optimization_stats']
        print(f"\nğŸ¯ çº¦æŸä¼˜åŒ–æ•ˆæœæ€»ç»“:")
        print(f"   åŸå§‹æ£€å‡º: {opt_stats['original_total']} ä¸ªå¼‚å¸¸")
        print(f"   ä¼˜åŒ–ç¡®è®¤: {opt_stats['optimized_total']} ä¸ªå¼‚å¸¸")
        print(f"   å‡å°‘ç‡: {opt_stats['reduction_ratio']:.1%}")
        print(f"   ç²¾åº¦æå‡: {opt_stats['efficiency_gain']:.1f}%")
        print(f"\nâœ¨ çº¦æŸä¼˜åŒ–åˆ†æå®Œæˆï¼")
        print(f"ğŸ† ç›¸æ¯”åŸå§‹å¤šç»´åº¦æ£€æµ‹çš„æ ¸å¿ƒæ”¹è¿›:")
        print(f"   â€¢ è¯¯æŠ¥æ§åˆ¶: å¤§å¹…å‡å°‘ä¸å¿…è¦çš„å¼‚å¸¸æŠ¥å‘Š")
        print(f"   â€¢ åŒ»å­¦ä¼˜å…ˆ: ç”Ÿç†å­¦åˆç†æ€§ä¼˜å…ˆäºç»Ÿè®¡å¼‚å¸¸")
        print(f"   â€¢ ä¸Šä¸‹æ–‡æ„ŸçŸ¥: ç»“åˆæ—¶é—´å’Œç”Ÿç†çŠ¶æ€åˆ¤æ–­")
        print(f"   â€¢ ç½®ä¿¡åº¦é‡åŒ–: æä¾›å¯ä¿¡åº¦åˆ†çº§æŒ‡å¯¼ä¸´åºŠå†³ç­–")
        return analysis_result, df
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶æœªæ‰¾åˆ°, è¯·æ£€æŸ¥è·¯å¾„: {args.input}")
        return None, None
    except ValueError as e:
        print(f"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡® - {e}")
        return None, None
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None, None

if __name__ == "__main__":
    main()