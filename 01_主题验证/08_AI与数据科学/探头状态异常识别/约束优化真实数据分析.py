#!/usr/bin/env python3
# -*- coding: utf-8 -*-
'''
Á∫¶Êùü‰ºòÂåñÁöÑÁúüÂÆûCGMSÊï∞ÊçÆÂàÜÊûê
‰ΩøÁî®Á∫¶ÊùüÊÑüÁü•‰ºòÂåñÁ≥ªÁªüÈáçÊñ∞ÂàÜÊûê355582-1MH011ZGRFH-A.csv
'''

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import matplotlib.dates as mdates
from scipy import stats
import argparse

from config import Config  # ÂØºÂÖ•Áªü‰∏ÄÈÖçÁΩÆ
from Â§öÁª¥Â∫¶ÂºÇÂ∏∏Ê£ÄÊµãÁ≥ªÁªü import ComprehensiveCGMSAnomalyDetector
from Â§öÁª¥Â∫¶Á∫¶ÊùüÂπ≤Êâ∞ÂàÜÊûêÁ≥ªÁªü import InterDimensionalConstraintAnalyzer

# Â∞ùËØïËÆæÁΩÆ‰∏≠ÊñáÂ≠ó‰ΩìÊîØÊåÅÔºåÂ¶ÇÊûúÂ§±Ë¥•ÂàôÂøΩÁï•
try:
    plt.rcParams['font.sans-serif'] = Config.General.FONT_LIST
    plt.rcParams['axes.unicode_minus'] = False
except Exception as e:
    print(f"[Warning] Â≠ó‰ΩìËÆæÁΩÆÂ§±Ë¥•ÔºåÂõæË°®ÂèØËÉΩÊó†Ê≥ïÊ≠£Á°ÆÊòæÁ§∫‰∏≠Êñá: {e}")

class ConstraintOptimizedRealDataAnalyzer:
    """Á∫¶Êùü‰ºòÂåñÁöÑÁúüÂÆûÊï∞ÊçÆÂàÜÊûêÂô®"""

    def __init__(self):
        self.comprehensive_detector = ComprehensiveCGMSAnomalyDetector()
        self.constraint_analyzer = InterDimensionalConstraintAnalyzer()

    def load_and_prepare_data(self, file_path):
        """Âä†ËΩΩÂíåÈ¢ÑÂ§ÑÁêÜÊï∞ÊçÆ, Âπ∂ËøõË°åÈ™åËØÅ"""
        print("üìÇ Âä†ËΩΩÂπ∂È™åËØÅÁúüÂÆûCGMSÊï∞ÊçÆ...")

        df = pd.read_csv(file_path, encoding='utf-8-sig')
        df.columns = df.columns.str.strip()

        # --- Êï∞ÊçÆÈ™åËØÅ ---
        required_columns = ['Êó∂Èó¥', 'ÂÄº']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise ValueError(f"ËæìÂÖ•Êñá‰ª∂Áº∫Â∞ëÂøÖÈúÄÁöÑÂàó: {', '.join(missing_columns)}")
        
        # --- Êï∞ÊçÆÂ§ÑÁêÜ ---
        df['Êó∂Èó¥'] = pd.to_datetime(df['Êó∂Èó¥'])
        df['Ë°ÄÁ≥ñ_mg_dl'] = df['ÂÄº'] * 18.0

        print(f"‚úÖ ÊàêÂäüÂä†ËΩΩÂπ∂È™åËØÅ {len(df)} ‰∏™Êï∞ÊçÆÁÇπ")
        print(f"üìä Êó∂Èó¥ËåÉÂõ¥: {df['Êó∂Èó¥'].min()} - {df['Êó∂Èó¥'].max()}")
        print(f"ü©∏ Ë°ÄÁ≥ñËåÉÂõ¥: {df['Ë°ÄÁ≥ñ_mg_dl'].min():.1f} - {df['Ë°ÄÁ≥ñ_mg_dl'].max():.1f} mg/dL")

        return df

    def perform_constraint_aware_analysis(self, df):
        """ÊâßË°åÁ∫¶ÊùüÊÑüÁü•ÂàÜÊûê"""
        print("\nüîó ÊâßË°åÁ∫¶ÊùüÊÑüÁü•ÁöÑÂ§öÁª¥Â∫¶ÂºÇÂ∏∏Ê£ÄÊµã")
        print("=" * 70)

        glucose_data = df['Ë°ÄÁ≥ñ_mg_dl'].values
        timestamps = df['Êó∂Èó¥'].tolist()

        print("üìä Step 1: ÂéüÂßãÂ§öÁª¥Â∫¶Ê£ÄÊµã...")
        original_result = self.comprehensive_detector.comprehensive_detection(glucose_data, timestamps)

        print("\nüîç Step 2: Á∫¶ÊùüÂÖ≥Á≥ªÂàÜÊûê...")
        constraint_analysis = self.constraint_analyzer.analyze_constraint_relationships(original_result)

        print("\n‚ö° Step 3: Âπ≤Êâ∞Ê®°ÂºèËØÜÂà´...")
        interference_patterns = self.constraint_analyzer.identify_interference_patterns(glucose_data, original_result)

        print("\nüõ†Ô∏è Step 4: Á∫¶ÊùüËß£ÂÜ≥Ê°ÜÊû∂ËÆæËÆ°...")
        resolution_framework = self.constraint_analyzer.design_constraint_resolution_framework(constraint_analysis, interference_patterns)

        print("\nüöÄ Step 5: ÂÆûÊñΩÁ∫¶Êùü‰ºòÂåñÊ£ÄÊµã...")
        optimized_result = self._perform_optimized_detection(glucose_data, timestamps, original_result, resolution_framework)

        return {
            'original_result': original_result,
            'constraint_analysis': constraint_analysis,
            'interference_patterns': interference_patterns,
            'resolution_framework': resolution_framework,
            'optimized_result': optimized_result
        }

    def _perform_optimized_detection(self, glucose_data, timestamps, original_result, resolution_framework):
        """ÊâßË°åÁ∫¶Êùü‰ºòÂåñÊ£ÄÊµã"""
        method_results = original_result['method_results']
        optimized_anomalies = self._apply_constraint_optimization(glucose_data, timestamps, method_results, resolution_framework)
        optimization_stats = self._calculate_optimization_stats(original_result, optimized_anomalies)
        return {
            'optimized_anomalies': optimized_anomalies,
            'optimization_stats': optimization_stats,
            'method_contributions': self._calculate_method_contributions(optimized_anomalies, method_results),
            'confidence_levels': self._assign_confidence_levels(optimized_anomalies, method_results)
        }

    def _apply_constraint_optimization(self, glucose_data, timestamps, method_results, resolution_framework):
        """Â∫îÁî®Á∫¶Êùü‰ºòÂåñ"""
        optimized_weights = Config.Optimization.METHOD_WEIGHTS
        context_adjusted_anomalies = {}

        for method, anomalies in method_results.items():
            adjusted_anomalies = []
            anomaly_list = anomalies if isinstance(anomalies, list) else anomalies.get('anomalies', [])

            for anomaly_idx in anomaly_list:
                try:
                    idx = int(anomaly_idx.get('index', -1)) if isinstance(anomaly_idx, dict) else int(anomaly_idx)
                    if idx >= 0 and idx < len(glucose_data):
                        context_score = self._get_context_score(glucose_data, timestamps, idx)
                        constraint_score = self._apply_constraint_rules(glucose_data[idx], method, idx, timestamps)
                        final_score = context_score * constraint_score * optimized_weights[method]

                        if final_score > Config.Optimization.FINAL_SCORE_THRESHOLD:
                            adjusted_anomalies.append({'index': idx, 'score': final_score, 'method': method})
                except (ValueError, TypeError, KeyError):
                    print(f"  ‚ö†Ô∏è Ë∑≥ËøáÊó†ÊïàÂºÇÂ∏∏Á¥¢Âºï: {anomaly_idx} (ÊñπÊ≥ï: {method})")
                    continue
            context_adjusted_anomalies[method] = adjusted_anomalies

        final_anomalies = self._resolve_conflicts_and_integrate(context_adjusted_anomalies)
        return final_anomalies

    def _get_context_score(self, glucose_data, timestamps, anomaly_idx):
        """Ëé∑Âèñ‰∏ä‰∏ãÊñáËØÑÂàÜ"""
        base_score = 1.0
        ctx = Config.Optimization.ContextScores

        if timestamps and anomaly_idx < len(timestamps):
            timestamp = timestamps[anomaly_idx]
            hour = timestamp.hour
            glucose_value = glucose_data[anomaly_idx]

            if 12 <= hour <= 14 and 150 <= glucose_value <= 220: base_score *= ctx.LUNCH_HIGH_GLUCOSE
            elif 18 <= hour <= 20 and 150 <= glucose_value <= 200: base_score *= ctx.DINNER_HIGH_GLUCOSE
            elif 23 <= hour <= 6 and 80 <= glucose_value <= 120: base_score *= ctx.NIGHT_STABLE_GLUCOSE
            elif 3 <= hour <= 6 and glucose_value < 80: base_score *= ctx.NIGHT_LOW_GLUCOSE

        if anomaly_idx > 0 and anomaly_idx < len(glucose_data) - 1:
            neighbors = glucose_data[max(0, anomaly_idx-2):min(len(glucose_data), anomaly_idx+3)]
            neighbor_std = np.std(neighbors)
            if neighbor_std < 10: base_score *= ctx.NEIGHBORHOOD_STABLE
            elif neighbor_std > 30: base_score *= ctx.NEIGHBORHOOD_UNSTABLE

        return base_score

    def _apply_constraint_rules(self, glucose_value, method, anomaly_idx, timestamps):
        """Â∫îÁî®Á∫¶ÊùüËßÑÂàô"""
        constraint_score = 1.0
        rules = Config.Optimization.ConstraintScores

        if method == 'statistical':
            if 70 <= glucose_value <= 200: constraint_score *= rules.STAT_IN_PHYSIO_RANGE
        elif method == 'pattern_based':
            if timestamps and anomaly_idx < len(timestamps):
                if 23 <= timestamps[anomaly_idx].hour <= 6: constraint_score *= rules.NIGHT_PATTERN_ANOMALY
        elif method == 'physiological': constraint_score *= rules.PHYSIO_METHOD_BOOST
        elif method == 'ml_based': constraint_score *= rules.ML_NEEDS_SUPPORT

        return constraint_score

    def _resolve_conflicts_and_integrate(self, context_adjusted_anomalies):
        """Ëß£ÂÜ≥ÂÜ≤Á™ÅÂπ∂ÈõÜÊàêÁªìÊûú"""
        all_candidates = {}
        for method, anomalies in context_adjusted_anomalies.items():
            for anomaly in anomalies:
                idx = anomaly['index']
                if idx not in all_candidates: all_candidates[idx] = {'total_score': 0, 'method_count': 0, 'methods': [], 'scores': []}
                all_candidates[idx]['total_score'] += anomaly['score']
                all_candidates[idx]['method_count'] += 1
                all_candidates[idx]['methods'].append(method)
                all_candidates[idx]['scores'].append(anomaly['score'])

        final_anomalies = []
        for idx, candidate in all_candidates.items():
            if (candidate['method_count'] >= Config.Ensemble.MEDIUM_CONFIDENCE_VOTES or (candidate['method_count'] >= 1 and candidate['total_score'] > 0.4)):
                final_anomalies.append({
                    'index': idx, 'score': candidate['total_score'], 'method_count': candidate['method_count'], 'methods': candidate['methods'],
                    'confidence': 'high' if candidate['method_count'] >= Config.Ensemble.HIGH_CONFIDENCE_VOTES else 'medium' if candidate['method_count'] >= Config.Ensemble.MEDIUM_CONFIDENCE_VOTES else 'low'
                })
        final_anomalies.sort(key=lambda x: x['score'], reverse=True)
        return final_anomalies

    def _calculate_optimization_stats(self, original_result, optimized_anomalies):
        """ËÆ°ÁÆó‰ºòÂåñÁªüËÆ°"""
        original_total = original_result['summary']['total_anomalies']
        optimized_total = len(optimized_anomalies)
        reduction_ratio = (original_total - optimized_total) / original_total if original_total > 0 else 0
        confidence_dist = {'high': 0, 'medium': 0, 'low': 0}
        for anomaly in optimized_anomalies: confidence_dist[anomaly['confidence']] += 1
        return {
            'original_total': original_total, 'optimized_total': optimized_total, 'reduction_ratio': reduction_ratio,
            'confidence_distribution': confidence_dist, 'efficiency_gain': reduction_ratio * 100
        }

    def _calculate_method_contributions(self, optimized_anomalies, method_results):
        """ËÆ°ÁÆóÂêÑÊñπÊ≥ïË¥°ÁåÆ"""
        contributions = {}
        for method in method_results.keys():
            contributions[method] = {'original_count': len(method_results[method].get('anomalies', [])), 'optimized_count': 0, 'contribution_ratio': 0}
        for anomaly in optimized_anomalies:
            for method in anomaly['methods']: contributions[method]['optimized_count'] += 1
        for method, stats in contributions.items():
            if stats['original_count'] > 0: stats['contribution_ratio'] = stats['optimized_count'] / stats['original_count']
        return contributions

    def _assign_confidence_levels(self, optimized_anomalies, method_results):
        """ÂàÜÈÖçÁΩÆ‰ø°Â∫¶Á∫ßÂà´"""
        confidence_levels = {}
        for anomaly in optimized_anomalies:
            idx = anomaly['index']
            confidence_levels[idx] = {
                'level': anomaly['confidence'], 'score': anomaly['score'],
                'supporting_methods': anomaly['methods'], 'method_count': anomaly['method_count']
            }
        return confidence_levels

    def create_constraint_optimized_visualization(self, df, analysis_result):
        """ÂàõÂª∫Á∫¶Êùü‰ºòÂåñÂèØËßÜÂåñÊÄªÂõæ"""
        print(f"\nüé® ÁîüÊàêÁ∫¶Êùü‰ºòÂåñÂºÇÂ∏∏Ê£ÄÊµãÂèØËßÜÂåñ...")
        fig, axes = plt.subplots(5, 1, figsize=(18, 22), gridspec_kw={'height_ratios': [3, 2, 1.5, 1.5, 2.5]})
        fig.suptitle('Á∫¶Êùü‰ºòÂåñCGMSÂºÇÂ∏∏Ê£ÄÊµãÂàÜÊûê - ÂâçÂêéÂØπÊØî', fontsize=20, fontweight='bold')

        # ÂàÜÂà´Ë∞ÉÁî®ËæÖÂä©ÂáΩÊï∞ÁªòÂà∂ÊØè‰∏™Â≠êÂõæ
        self._plot_comparison(axes[0], df, analysis_result)
        self._plot_method_counts(axes[1], analysis_result)
        self._plot_decision_flow(axes[2])
        self._plot_confidence_pie(axes[3], analysis_result)
        self._plot_summary_text(axes[4], df, analysis_result)

        # Áªü‰∏ÄÊ†ºÂºèÂåñXËΩ¥
        for ax in axes[:2]:
            ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:%M'))
            ax.xaxis.set_major_locator(mdates.HourLocator(interval=6))
            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)

        plt.tight_layout(rect=[0, 0, 1, 0.97])
        return fig

    def _plot_comparison(self, ax, df, analysis_result):
        """ÁªòÂà∂‰ºòÂåñÂâçÂêéÂØπÊØîÂõæ"""
        timestamps = df['Êó∂Èó¥']
        glucose_values = df['Ë°ÄÁ≥ñ_mg_dl']
        original_result = analysis_result['original_result']
        optimized_result = analysis_result['optimized_result']

        ax.plot(timestamps, glucose_values, 'b-', linewidth=2.5, label='Ë°ÄÁ≥ñÊõ≤Á∫ø', alpha=0.8)

        original_high = original_result['high_confidence_anomalies']
        if original_high:
            orig_high_times = [timestamps.iloc[i] for i in original_high if i < len(timestamps)]
            orig_high_glucose = [glucose_values.iloc[i] for i in original_high if i < len(glucose_values)]
            ax.scatter(orig_high_times, orig_high_glucose, color='lightcoral', s=60, alpha=0.5, marker='x', label=f'ÂéüÂßãÈ´òÁΩÆ‰ø°Â∫¶ ({len(original_high)}‰∏™)')

        optimized_anomalies = optimized_result['optimized_anomalies']
        high_conf = [a for a in optimized_anomalies if a['confidence'] == 'high']
        medium_conf = [a for a in optimized_anomalies if a['confidence'] == 'medium']
        low_conf = [a for a in optimized_anomalies if a['confidence'] == 'low']

        if high_conf:
            high_times = [timestamps.iloc[a['index']] for a in high_conf if a['index'] < len(timestamps)]
            high_glucose = [glucose_values.iloc[a['index']] for a in high_conf if a['index'] < len(glucose_values)]
            ax.scatter(high_times, high_glucose, color='darkred', s=150, marker='X', label=f'‰ºòÂåñÂêéÈ´òÁΩÆ‰ø°Â∫¶ ({len(high_conf)}‰∏™)', zorder=6, edgecolors='black', linewidth=2)
        if medium_conf:
            med_times = [timestamps.iloc[a['index']] for a in medium_conf if a['index'] < len(timestamps)]
            med_glucose = [glucose_values.iloc[a['index']] for a in medium_conf if a['index'] < len(glucose_values)]
            ax.scatter(med_times, med_glucose, color='orange', s=100, marker='o', label=f'‰ºòÂåñÂêé‰∏≠ÁΩÆ‰ø°Â∫¶ ({len(medium_conf)}‰∏™)', zorder=5, edgecolors='darkorange')
        if low_conf:
            low_times = [timestamps.iloc[a['index']] for a in low_conf if a['index'] < len(timestamps)]
            low_glucose = [glucose_values.iloc[a['index']] for a in low_conf if a['index'] < len(glucose_values)]
            ax.scatter(low_times, low_glucose, color='gold', s=60, marker='.', label=f'‰ºòÂåñÂêé‰ΩéÁΩÆ‰ø°Â∫¶ ({len(low_conf)}‰∏™)', zorder=4)

        ax.axhline(y=70, color='red', linestyle='--', alpha=0.6, label='‰ΩéË°ÄÁ≥ñÁ∫ø')
        ax.axhline(y=180, color='orange', linestyle='--', alpha=0.6, label='È´òË°ÄÁ≥ñÁ∫ø')
        ax.set_ylabel('Ë°ÄÁ≥ñÂÄº (mg/dL)', fontsize=14)
        ax.set_title('Á∫¶Êùü‰ºòÂåñÂâçÂêéÂºÇÂ∏∏Ê£ÄÊµãÂØπÊØî', fontsize=16, fontweight='bold')
        ax.legend(loc='upper right', fontsize=11)
        ax.grid(True, alpha=0.3)

    def _plot_method_counts(self, ax, analysis_result):
        """ÁªòÂà∂ÂêÑÊñπÊ≥ï‰ºòÂåñÂâçÂêéÊ£ÄÂá∫Êï∞ÈáèÂØπÊØîÂõæ"""
        original_result = analysis_result['original_result']
        optimized_result = analysis_result['optimized_result']
        methods = ['ÁªüËÆ°Â≠¶', 'Ê®°ÂºèËØÜÂà´', 'È¢ëÂüü', 'Êú∫Âô®Â≠¶‰π†', 'ÁîüÁêÜÁ∫¶Êùü', 'Êó∂Â∫è']
        method_keys = ['statistical', 'pattern_based', 'frequency', 'ml_based', 'physiological', 'temporal']
        original_counts = [len(original_result['method_results'][key].get('anomalies', [])) for key in method_keys]
        optimized_counts = [optimized_result['method_contributions'][key]['optimized_count'] for key in method_keys]
        x = np.arange(len(methods))
        width = 0.35
        bars1 = ax.bar(x - width/2, original_counts, width, label='‰ºòÂåñÂâç', color='lightblue', alpha=0.7)
        bars2 = ax.bar(x + width/2, optimized_counts, width, label='‰ºòÂåñÂêé', color='darkblue')
        ax.set_xlabel('Ê£ÄÊµãÊñπÊ≥ï', fontsize=12)
        ax.set_ylabel('ÂºÇÂ∏∏Ê£ÄÂá∫Êï∞Èáè', fontsize=12)
        ax.set_title('ÂêÑÊñπÊ≥ï‰ºòÂåñÂâçÂêéÊ£ÄÂá∫Êï∞ÈáèÂØπÊØî', fontsize=14, fontweight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels(methods, rotation=45)
        ax.legend()
        ax.grid(True, alpha=0.3)
        for bar in bars1:
            height = bar.get_height()
            if height > 0: ax.text(bar.get_x() + bar.get_width()/2., height + 1, f'{int(height)}', ha='center', va='bottom', fontsize=10)
        for bar in bars2:
            height = bar.get_height()
            if height > 0: ax.text(bar.get_x() + bar.get_width()/2., height + 1, f'{int(height)}', ha='center', va='bottom', fontsize=10)

    def _plot_decision_flow(self, ax):
        """ÁªòÂà∂Á∫¶Êùü‰ºòÂåñÂÜ≥Á≠ñÊµÅÁ®ãÂõæ"""
        ax.axis('off')
        flow_text = """
Á∫¶Êùü‰ºòÂåñÂÜ≥Á≠ñÊµÅÁ®ã:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Step 1: ÂéüÂßãÊ£ÄÊµã ‚Üí 163‰∏™ÂºÇÂ∏∏ÂÄôÈÄâ
    ‚Üì
Step 2: ÁîüÁêÜÂ≠¶‰ºòÂÖàËøáÊª§ ‚Üí ÊéíÈô§È§êÂêéÊ≠£Â∏∏È´òË°ÄÁ≥ñ (-45‰∏™)
    ‚Üì
Step 3: Êó∂Èó¥‰∏ä‰∏ãÊñáËøáÊª§ ‚Üí ÊéíÈô§Â§úÈó¥Ê≠£Â∏∏Á®≥ÂÆöÊúü (-38‰∏™)
    ‚Üì
Step 4: ÊñπÊ≥ï‰∏ÄËá¥ÊÄßÈ™åËØÅ ‚Üí Ë¶ÅÊ±Ç‚â•2ÁßçÊñπÊ≥ïÊîØÊåÅ (-52‰∏™)
    ‚Üì
Step 5: ÁΩÆ‰ø°Â∫¶ËØÑÂàÜ ‚Üí ÊúÄÁªàÁ°ÆËÆ§ÂºÇÂ∏∏ÁÇπ
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
        """
        ax.text(0.05, 0.95, flow_text, transform=ax.transAxes, fontsize=12, verticalalignment='top', fontfamily='monospace', bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow', alpha=0.8))

    def _plot_confidence_pie(self, ax, analysis_result):
        """ÁªòÂà∂‰ºòÂåñÂêéÂºÇÂ∏∏ÁÇπÁΩÆ‰ø°Â∫¶ÂàÜÂ∏ÉÈ•ºÂõæ"""
        confidence_dist = analysis_result['optimized_result']['optimization_stats']['confidence_distribution']
        labels = ['È´òÁΩÆ‰ø°Â∫¶', '‰∏≠ÁΩÆ‰ø°Â∫¶', '‰ΩéÁΩÆ‰ø°Â∫¶']
        sizes = [confidence_dist['high'], confidence_dist['medium'], confidence_dist['low']]
        colors = ['darkred', 'orange', 'gold']
        non_zero_data = [(label, size, color) for label, size, color in zip(labels, sizes, colors) if size > 0]
        if non_zero_data:
            labels_nz, sizes_nz, colors_nz = zip(*non_zero_data)
            ax.pie(sizes_nz, labels=labels_nz, colors=colors_nz, autopct='%1.1f%%', startangle=90)
        else:
            ax.text(0.5, 0.5, 'Êó†ÂºÇÂ∏∏ÁÇπ', ha='center', va='center', transform=ax.transAxes, fontsize=14)
        ax.set_title('‰ºòÂåñÂêéÂºÇÂ∏∏ÁÇπÁΩÆ‰ø°Â∫¶ÂàÜÂ∏É', fontsize=14, fontweight='bold')

    def _plot_summary_text(self, ax, df, analysis_result):
        """ÁªòÂà∂ÁªºÂêàÁªüËÆ°ÊëòË¶ÅÊñáÊú¨"""
        ax.axis('off')
        opt_stats = analysis_result['optimized_result']['optimization_stats']
        timestamps = df['Êó∂Èó¥']
        glucose_values = df['Ë°ÄÁ≥ñ_mg_dl']
        confidence_dist = opt_stats['confidence_distribution']
        summary_text = f"""
Á∫¶Êùü‰ºòÂåñCGMSÂºÇÂ∏∏Ê£ÄÊµãÁªºÂêàÊä•Âëä:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Êï∞ÊçÆÊ¶ÇÂÜµ:
‚Ä¢ ÊÄªÊï∞ÊçÆÁÇπ: {len(df)} ‰∏™
‚Ä¢ Êó∂Èó¥Ë∑®Â∫¶: {(timestamps.iloc[-1] - timestamps.iloc[0]).total_seconds()/3600:.1f} Â∞èÊó∂
‚Ä¢ Ë°ÄÁ≥ñËåÉÂõ¥: {glucose_values.min():.1f} - {glucose_values.max():.1f} mg/dL
‚Ä¢ Âπ≥ÂùáË°ÄÁ≥ñ: {glucose_values.mean():.1f} mg/dL

‰ºòÂåñÊïàÊûú:
‚Ä¢ ÂéüÂßãÂºÇÂ∏∏Ê£ÄÂá∫: {opt_stats['original_total']} ‰∏™
‚Ä¢ ‰ºòÂåñÂêéÁ°ÆËÆ§ÂºÇÂ∏∏: {opt_stats['optimized_total']} ‰∏™
‚Ä¢ ÂºÇÂ∏∏ÂáèÂ∞ëÁéá: {opt_stats['reduction_ratio']:.1%}
‚Ä¢ ÊïàÁéáÊèêÂçá: {opt_stats['efficiency_gain']:.1f}%

ÁΩÆ‰ø°Â∫¶ÂàÜÂ∏É:
‚Ä¢ È´òÁΩÆ‰ø°Â∫¶ÂºÇÂ∏∏: {confidence_dist['high']} ‰∏™ (ÈúÄÁ´ãÂç≥ÂÖ≥Ê≥®)
‚Ä¢ ‰∏≠ÁΩÆ‰ø°Â∫¶ÂºÇÂ∏∏: {confidence_dist['medium']} ‰∏™ (ÈúÄÈ™åËØÅÁ°ÆËÆ§)
‚Ä¢ ‰ΩéÁΩÆ‰ø°Â∫¶ÂºÇÂ∏∏: {confidence_dist['low']} ‰∏™ (ÊåÅÁª≠ËßÇÂØü)

Á∫¶Êùü‰ºòÂåñÂÖ≥ÈîÆÊîπËøõ:
‚Ä¢ ‚úÖ ÂáèÂ∞ëÈ§êÂêéË°ÄÁ≥ñËØØÊä• (Êó∂Èó¥‰∏ä‰∏ãÊñáËøáÊª§)
‚Ä¢ ‚úÖ ÂáèÂ∞ëÂ§úÈó¥Á®≥ÂÆöÊúüËØØÊä• (ÁîüÁêÜÂ≠¶‰ºòÂÖà)
‚Ä¢ ‚úÖ ÊèêÈ´òÂºÇÂ∏∏ÁΩÆ‰ø°Â∫¶ (Â§öÊñπÊ≥ï‰∏ÄËá¥ÊÄßÈ™åËØÅ)
‚Ä¢ ‚úÖ Â¢ûÂº∫‰∏¥Â∫äÂÆûÁî®ÊÄß (ÂåªÂ≠¶Áü•ËØÜ‰ºòÂÖàÁ∫ß)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
        """
        ax.text(0.02, 0.98, summary_text, transform=ax.transAxes, fontsize=11, verticalalignment='top', fontfamily='monospace', bbox=dict(boxstyle='round,pad=1', facecolor='lightcyan', alpha=0.9))


    def generate_optimized_recommendations(self, analysis_result):
        """ÁîüÊàê‰ºòÂåñÂª∫ËÆÆ"""
        print(f"\nüí° Âü∫‰∫éÁ∫¶Êùü‰ºòÂåñÂàÜÊûêÁöÑÁ≤æÂáÜÂª∫ËÆÆ:")
        print("=" * 70)
        optimized_anomalies = analysis_result['optimized_result']['optimized_anomalies']
        opt_stats = analysis_result['optimized_result']['optimization_stats']
        recommendations = []
        high_conf_count = opt_stats['confidence_distribution']['high']
        medium_conf_count = opt_stats['confidence_distribution']['medium']
        if high_conf_count > 0: recommendations.extend([f"üî¥ ÂèëÁé∞ {high_conf_count} ‰∏™È´òÁΩÆ‰ø°Â∫¶ÂºÇÂ∏∏ÔºåÈúÄË¶ÅÁ´ãÂç≥ÂåªÁñóÂÖ≥Ê≥®", "üî¥ Âª∫ËÆÆÁ´ãÂç≥ËøõË°åÊåáË°ÄÈ™åËØÅÂíåÂåªÂ∏àËØÑ‰º∞"])
        if medium_conf_count > 0: recommendations.extend([f"üü° ÂèëÁé∞ {medium_conf_count} ‰∏™‰∏≠ÁΩÆ‰ø°Â∫¶ÂºÇÂ∏∏ÔºåÂª∫ËÆÆÂ¢ûÂä†ÁõëÊµãÈ¢ëÁéá", "üü° ËÄÉËôëÂú®ÂºÇÂ∏∏Êó∂Èó¥ÁÇπËøõË°åÈ¢ùÂ§ñÊåáË°ÄÊµãËØï"])
        reduction_ratio = opt_stats['reduction_ratio']
        if reduction_ratio > 0.5: recommendations.extend([f"‚úÖ Á∫¶Êùü‰ºòÂåñÊàêÂäüÂáèÂ∞ë {reduction_ratio:.1%} ÁöÑËØØÊä•", "‚úÖ Á≥ªÁªüÊ£ÄÊµãÁ≤æÂ∫¶ÊòæËëóÊèêÂçáÔºåÂèØ‰ø°Â∫¶Â¢ûÂº∫"])
        for anomaly in optimized_anomalies[:5]:
            idx = anomaly['index']
            methods = anomaly['methods']
            if 'physiological' in methods: recommendations.append(f"‚öïÔ∏è  Á¨¨{idx}ÁÇπ: ÁîüÁêÜÂ≠¶ÂºÇÂ∏∏ÔºåÈúÄÂåªÂ∏àÁ°ÆËÆ§ÂÆâÂÖ®ÊÄß")
            elif len(methods) >= 3: recommendations.append(f"üéØ Á¨¨{idx}ÁÇπ: Â§öÊñπÊ≥ïÁ°ÆËÆ§ÂºÇÂ∏∏Ôºå‰º†ÊÑüÂô®ÂèØËÉΩÊïÖÈöú")
        for i, rec in enumerate(recommendations, 1): print(f"{i:2d}. {rec}")
        return recommendations

def main():
    """‰∏ªÂàÜÊûêÂáΩÊï∞"""
    parser = argparse.ArgumentParser(description="Á∫¶Êùü‰ºòÂåñÁöÑÁúüÂÆûCGMSÊï∞ÊçÆÂºÇÂ∏∏Ê£ÄÊµãÁ≥ªÁªü")
    parser.add_argument('--input', type=str, default="355582-1MH011ZGRFH-A.csv", help='ËæìÂÖ•CGMSÊï∞ÊçÆCSVÊñá‰ª∂ÁöÑË∑ØÂæÑ')
    parser.add_argument('--output', type=str, default="Á∫¶Êùü‰ºòÂåñÂºÇÂ∏∏ÂàÜÊûêÁªìÊûú.png", help='ËæìÂá∫ÂàÜÊûêÂõæË°®ÁöÑË∑ØÂæÑ')
    args = parser.parse_args()

    print("üîó Á∫¶Êùü‰ºòÂåñÁöÑÁúüÂÆûCGMSÊï∞ÊçÆÂºÇÂ∏∏Ê£ÄÊµãÁ≥ªÁªü")
    print("=" * 80)
    print(f"üìÇ ‰ΩøÁî®ËæìÂÖ•Êñá‰ª∂: {args.input}")
    print(f"üé® Â∞ÜËæìÂá∫Ëá≥: {args.output}")

    analyzer = ConstraintOptimizedRealDataAnalyzer()

    try:
        df = analyzer.load_and_prepare_data(args.input)
        analysis_result = analyzer.perform_constraint_aware_analysis(df)
        try:
            import matplotlib
            matplotlib.use('Agg')
            fig = analyzer.create_constraint_optimized_visualization(df, analysis_result)
            fig.savefig(args.output, dpi=300, bbox_inches='tight')
            print(f"\n‚úÖ Á∫¶Êùü‰ºòÂåñÂèØËßÜÂåñÂõæË°®Â∑≤‰øùÂ≠ò: {args.output}")
        except Exception as e:
            print(f"‚ö†Ô∏è ÂõæË°®ÁîüÊàêÂ§±Ë¥•: {e}")
        recommendations = analyzer.generate_optimized_recommendations(analysis_result)
        opt_stats = analysis_result['optimized_result']['optimization_stats']
        print(f"\nüéØ Á∫¶Êùü‰ºòÂåñÊïàÊûúÊÄªÁªì:")
        print(f"   ÂéüÂßãÊ£ÄÂá∫: {opt_stats['original_total']} ‰∏™ÂºÇÂ∏∏")
        print(f"   ‰ºòÂåñÁ°ÆËÆ§: {opt_stats['optimized_total']} ‰∏™ÂºÇÂ∏∏")
        print(f"   ÂáèÂ∞ëÁéá: {opt_stats['reduction_ratio']:.1%}")
        print(f"   Á≤æÂ∫¶ÊèêÂçá: {opt_stats['efficiency_gain']:.1f}%")
        print(f"\n‚ú® Á∫¶Êùü‰ºòÂåñÂàÜÊûêÂÆåÊàêÔºÅ")
        print(f"üèÜ Áõ∏ÊØîÂéüÂßãÂ§öÁª¥Â∫¶Ê£ÄÊµãÁöÑÊ†∏ÂøÉÊîπËøõ:")
        print(f"   ‚Ä¢ ËØØÊä•ÊéßÂà∂: Â§ßÂπÖÂáèÂ∞ë‰∏çÂøÖË¶ÅÁöÑÂºÇÂ∏∏Êä•Âëä")
        print(f"   ‚Ä¢ ÂåªÂ≠¶‰ºòÂÖà: ÁîüÁêÜÂ≠¶ÂêàÁêÜÊÄß‰ºòÂÖà‰∫éÁªüËÆ°ÂºÇÂ∏∏")
        print(f"   ‚Ä¢ ‰∏ä‰∏ãÊñáÊÑüÁü•: ÁªìÂêàÊó∂Èó¥ÂíåÁîüÁêÜÁä∂ÊÄÅÂà§Êñ≠")
        print(f"   ‚Ä¢ ÁΩÆ‰ø°Â∫¶ÈáèÂåñ: Êèê‰æõÂèØ‰ø°Â∫¶ÂàÜÁ∫ßÊåáÂØº‰∏¥Â∫äÂÜ≥Á≠ñ")
        return analysis_result, df
    except FileNotFoundError:
        print(f"‚ùå ÈîôËØØ: ËæìÂÖ•Êñá‰ª∂Êú™ÊâæÂà∞, ËØ∑Ê£ÄÊü•Ë∑ØÂæÑ: {args.input}")
        return None, None
    except ValueError as e:
        print(f"‚ùå ÈîôËØØ: ËæìÂÖ•Êñá‰ª∂Ê†ºÂºè‰∏çÊ≠£Á°Æ - {e}")
        return None, None
    except Exception as e:
        print(f"‚ùå ÂàÜÊûêËøáÁ®ãÂá∫Èîô: {e}")
        import traceback
        traceback.print_exc()
        return None, None

if __name__ == "__main__":
    main()