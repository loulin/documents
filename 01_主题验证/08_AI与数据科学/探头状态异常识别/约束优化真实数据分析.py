#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çº¦æŸä¼˜åŒ–çš„çœŸå®CGMSæ•°æ®åˆ†æ
ä½¿ç”¨çº¦æŸæ„ŸçŸ¥ä¼˜åŒ–ç³»ç»Ÿé‡æ–°åˆ†æ355582-1MH011ZGRFH-A.csv
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import matplotlib.dates as mdates
from scipy import stats
from å¤šç»´åº¦å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ import ComprehensiveCGMSAnomalyDetector
from å¤šç»´åº¦çº¦æŸå¹²æ‰°åˆ†æç³»ç»Ÿ import InterDimensionalConstraintAnalyzer

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class ConstraintOptimizedRealDataAnalyzer:
    """çº¦æŸä¼˜åŒ–çš„çœŸå®æ•°æ®åˆ†æå™¨"""

    def __init__(self):
        self.comprehensive_detector = ComprehensiveCGMSAnomalyDetector()
        self.constraint_analyzer = InterDimensionalConstraintAnalyzer()

    def load_and_prepare_data(self, file_path):
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        print("ğŸ“‚ åŠ è½½çœŸå®CGMSæ•°æ®...")

        df = pd.read_csv(file_path, encoding='utf-8-sig')
        df.columns = df.columns.str.strip()
        df['æ—¶é—´'] = pd.to_datetime(df['æ—¶é—´'])
        df['è¡€ç³–_mg_dl'] = df['å€¼'] * 18.0

        print(f"âœ… æˆåŠŸåŠ è½½ {len(df)} ä¸ªæ•°æ®ç‚¹")
        print(f"ğŸ“Š æ—¶é—´èŒƒå›´: {df['æ—¶é—´'].min()} - {df['æ—¶é—´'].max()}")
        print(f"ğŸ©¸ è¡€ç³–èŒƒå›´: {df['è¡€ç³–_mg_dl'].min():.1f} - {df['è¡€ç³–_mg_dl'].max():.1f} mg/dL")

        return df

    def perform_constraint_aware_analysis(self, df):
        """æ‰§è¡Œçº¦æŸæ„ŸçŸ¥åˆ†æ"""
        print("\nğŸ”— æ‰§è¡Œçº¦æŸæ„ŸçŸ¥çš„å¤šç»´åº¦å¼‚å¸¸æ£€æµ‹")
        print("=" * 70)

        glucose_data = df['è¡€ç³–_mg_dl'].values
        timestamps = df['æ—¶é—´'].tolist()

        # Step 1: åŸå§‹å¤šç»´åº¦æ£€æµ‹
        print("ğŸ“Š Step 1: åŸå§‹å¤šç»´åº¦æ£€æµ‹...")
        original_result = self.comprehensive_detector.comprehensive_detection(glucose_data, timestamps)

        # Step 2: çº¦æŸå…³ç³»åˆ†æ
        print("\nğŸ” Step 2: çº¦æŸå…³ç³»åˆ†æ...")
        constraint_analysis = self.constraint_analyzer.analyze_constraint_relationships(original_result)

        # Step 3: å¹²æ‰°æ¨¡å¼è¯†åˆ«
        print("\nâš¡ Step 3: å¹²æ‰°æ¨¡å¼è¯†åˆ«...")
        interference_patterns = self.constraint_analyzer.identify_interference_patterns(
            glucose_data, original_result
        )

        # Step 4: çº¦æŸè§£å†³æ¡†æ¶è®¾è®¡
        print("\nğŸ› ï¸ Step 4: çº¦æŸè§£å†³æ¡†æ¶è®¾è®¡...")
        resolution_framework = self.constraint_analyzer.design_constraint_resolution_framework(
            constraint_analysis, interference_patterns
        )

        # Step 5: ä¼˜åŒ–æ£€æµ‹å®æ–½
        print("\nğŸš€ Step 5: å®æ–½çº¦æŸä¼˜åŒ–æ£€æµ‹...")
        optimized_result = self._perform_optimized_detection(
            glucose_data, timestamps, original_result, resolution_framework
        )

        return {
            'original_result': original_result,
            'constraint_analysis': constraint_analysis,
            'interference_patterns': interference_patterns,
            'resolution_framework': resolution_framework,
            'optimized_result': optimized_result
        }

    def _perform_optimized_detection(self, glucose_data, timestamps, original_result, resolution_framework):
        """æ‰§è¡Œçº¦æŸä¼˜åŒ–æ£€æµ‹"""

        # è·å–åŸå§‹æ£€æµ‹ç»“æœ
        method_results = original_result['method_results']

        # åº”ç”¨çº¦æŸæ„ŸçŸ¥ä¼˜åŒ–
        optimized_anomalies = self._apply_constraint_optimization(
            glucose_data, timestamps, method_results, resolution_framework
        )

        # è®¡ç®—ä¼˜åŒ–ç»Ÿè®¡
        optimization_stats = self._calculate_optimization_stats(original_result, optimized_anomalies)

        return {
            'optimized_anomalies': optimized_anomalies,
            'optimization_stats': optimization_stats,
            'method_contributions': self._calculate_method_contributions(optimized_anomalies, method_results),
            'confidence_levels': self._assign_confidence_levels(optimized_anomalies, method_results)
        }

    def _apply_constraint_optimization(self, glucose_data, timestamps, method_results, resolution_framework):
        """åº”ç”¨çº¦æŸä¼˜åŒ–"""

        # 1. åŸºç¡€æƒé‡è®¾ç½®
        optimized_weights = {
            'statistical': 0.15,      # é™ä½æƒé‡ï¼Œå‡å°‘ç”Ÿç†æ­£å¸¸ä½†ç»Ÿè®¡å¼‚å¸¸çš„è¯¯æŠ¥
            'pattern_based': 0.20,    # é™ä½æƒé‡ï¼Œæ¨¡å¼è¯†åˆ«è¿‡äºæ•æ„Ÿ
            'frequency': 0.08,        # ä¿æŒè¾ƒä½æƒé‡
            'ml_based': 0.15,         # é€‚ä¸­æƒé‡
            'physiological': 0.30,    # æé«˜æƒé‡ï¼ŒåŒ»å­¦çŸ¥è¯†ä¼˜å…ˆ
            'temporal': 0.12          # é€‚ä¸­æƒé‡
        }

        # 2. ä¸Šä¸‹æ–‡æ„ŸçŸ¥è°ƒæ•´
        context_adjusted_anomalies = {}

        for method, anomalies in method_results.items():
            adjusted_anomalies = []

            # ç¡®ä¿anomaliesæ˜¯åˆ—è¡¨æ ¼å¼
            anomaly_list = anomalies if isinstance(anomalies, list) else anomalies.get('anomalies', [])

            for anomaly_idx in anomaly_list:
                # ç¡®ä¿anomaly_idxæ˜¯æ•´æ•°ï¼Œå¤„ç†å„ç§æ•°æ®ç±»å‹
                try:
                    if isinstance(anomaly_idx, (int, np.integer)):
                        idx = int(anomaly_idx)
                    elif isinstance(anomaly_idx, (str, np.str_)):
                        idx = int(anomaly_idx)
                    elif isinstance(anomaly_idx, dict):
                        # å¦‚æœæ˜¯å­—å…¸ï¼Œå°è¯•è·å–indexå­—æ®µ
                        idx = int(anomaly_idx.get('index', anomaly_idx.get('idx', -1)))
                    else:
                        # å°è¯•ç›´æ¥è½¬æ¢
                        idx = int(anomaly_idx)

                    if idx >= 0 and idx < len(glucose_data):
                        # è·å–ä¸Šä¸‹æ–‡ä¿¡æ¯
                        context_score = self._get_context_score(
                            glucose_data, timestamps, idx
                        )

                        # åº”ç”¨çº¦æŸè§„åˆ™
                        constraint_score = self._apply_constraint_rules(
                            glucose_data[idx], method, idx, timestamps
                        )

                        # ç»¼åˆè¯„åˆ†
                        final_score = context_score * constraint_score * optimized_weights[method]

                        if final_score > 0.15:  # ä¼˜åŒ–åçš„é˜ˆå€¼
                            adjusted_anomalies.append({
                                'index': idx,
                                'score': final_score,
                                'method': method
                            })

                except (ValueError, TypeError, KeyError) as e:
                    # å¦‚æœæ— æ³•è½¬æ¢ä¸ºæœ‰æ•ˆç´¢å¼•ï¼Œè·³è¿‡è¿™ä¸ªå¼‚å¸¸ç‚¹
                    print(f"  âš ï¸ è·³è¿‡æ— æ•ˆå¼‚å¸¸ç´¢å¼•: {anomaly_idx} (æ–¹æ³•: {method})")
                    continue

            context_adjusted_anomalies[method] = adjusted_anomalies

        # 3. å†²çªè§£å†³å’Œé›†æˆ
        final_anomalies = self._resolve_conflicts_and_integrate(context_adjusted_anomalies)

        return final_anomalies

    def _get_context_score(self, glucose_data, timestamps, anomaly_idx):
        """è·å–ä¸Šä¸‹æ–‡è¯„åˆ†"""
        base_score = 1.0

        if timestamps and anomaly_idx < len(timestamps):
            timestamp = timestamps[anomaly_idx]
            hour = timestamp.hour
            glucose_value = glucose_data[anomaly_idx]

            # æ—¶é—´ä¸Šä¸‹æ–‡è°ƒæ•´
            if 12 <= hour <= 14 and 150 <= glucose_value <= 220:
                # åˆé¤æ—¶é—´çš„é«˜è¡€ç³–ï¼Œå¯èƒ½æ˜¯ç”Ÿç†æ€§çš„
                base_score *= 0.6
            elif 18 <= hour <= 20 and 150 <= glucose_value <= 200:
                # æ™šé¤æ—¶é—´çš„é«˜è¡€ç³–
                base_score *= 0.7
            elif 23 <= hour <= 6 and 80 <= glucose_value <= 120:
                # å¤œé—´ç¨³å®šè¡€ç³–
                base_score *= 0.5
            elif 3 <= hour <= 6 and glucose_value < 80:
                # å¤œé—´ä½è¡€ç³–ï¼Œæ›´å€¼å¾—å…³æ³¨
                base_score *= 1.4

        # é‚»åŸŸä¸€è‡´æ€§
        if anomaly_idx > 0 and anomaly_idx < len(glucose_data) - 1:
            neighbors = glucose_data[max(0, anomaly_idx-2):min(len(glucose_data), anomaly_idx+3)]
            neighbor_std = np.std(neighbors)

            if neighbor_std < 10:  # é‚»åŸŸç¨³å®šï¼Œå¼‚å¸¸æ›´å¯ä¿¡
                base_score *= 1.2
            elif neighbor_std > 30:  # é‚»åŸŸä¸ç¨³å®šï¼Œå¯èƒ½æ˜¯å™ªå£°
                base_score *= 0.8

        return base_score

    def _apply_constraint_rules(self, glucose_value, method, anomaly_idx, timestamps):
        """åº”ç”¨çº¦æŸè§„åˆ™"""
        constraint_score = 1.0

        # ç”Ÿç†å­¦çº¦æŸä¼˜å…ˆè§„åˆ™
        if method == 'statistical':
            if 70 <= glucose_value <= 200:
                # ç»Ÿè®¡å¼‚å¸¸ä½†åœ¨ç”Ÿç†æ­£å¸¸èŒƒå›´
                constraint_score *= 0.5

        elif method == 'pattern_based':
            if timestamps and anomaly_idx < len(timestamps):
                hour = timestamps[anomaly_idx].hour
                if 23 <= hour <= 6:
                    # å¤œé—´æ¨¡å¼å¼‚å¸¸å¯èƒ½æ˜¯æ­£å¸¸çš„ç¨³å®šæœŸ
                    constraint_score *= 0.3

        elif method == 'physiological':
            # ç”Ÿç†å­¦æ£€æµ‹æƒå¨æ€§é«˜
            constraint_score *= 1.5

        elif method == 'ml_based':
            # MLéœ€è¦å…¶ä»–æ–¹æ³•æ”¯æŒ
            constraint_score *= 0.8

        return constraint_score

    def _resolve_conflicts_and_integrate(self, context_adjusted_anomalies):
        """è§£å†³å†²çªå¹¶é›†æˆç»“æœ"""

        # æ”¶é›†æ‰€æœ‰å€™é€‰å¼‚å¸¸ç‚¹
        all_candidates = {}

        for method, anomalies in context_adjusted_anomalies.items():
            for anomaly in anomalies:
                idx = anomaly['index']
                if idx not in all_candidates:
                    all_candidates[idx] = {
                        'total_score': 0,
                        'method_count': 0,
                        'methods': [],
                        'scores': []
                    }

                all_candidates[idx]['total_score'] += anomaly['score']
                all_candidates[idx]['method_count'] += 1
                all_candidates[idx]['methods'].append(method)
                all_candidates[idx]['scores'].append(anomaly['score'])

        # åº”ç”¨é›†æˆè§„åˆ™
        final_anomalies = []

        for idx, candidate in all_candidates.items():
            # è‡³å°‘éœ€è¦2ç§æ–¹æ³•æ”¯æŒï¼Œæˆ–è€…1ç§é«˜åˆ†æ–¹æ³•
            if (candidate['method_count'] >= 2 or
                (candidate['method_count'] >= 1 and candidate['total_score'] > 0.4)):

                final_anomalies.append({
                    'index': idx,
                    'score': candidate['total_score'],
                    'method_count': candidate['method_count'],
                    'methods': candidate['methods'],
                    'confidence': 'high' if candidate['method_count'] >= 3 else
                                 'medium' if candidate['method_count'] >= 2 else 'low'
                })

        # æŒ‰è¯„åˆ†æ’åº
        final_anomalies.sort(key=lambda x: x['score'], reverse=True)

        return final_anomalies

    def _calculate_optimization_stats(self, original_result, optimized_anomalies):
        """è®¡ç®—ä¼˜åŒ–ç»Ÿè®¡"""
        original_total = original_result['summary']['total_anomalies']
        optimized_total = len(optimized_anomalies)

        reduction_ratio = (original_total - optimized_total) / original_total if original_total > 0 else 0

        # è®¡ç®—ç½®ä¿¡åº¦åˆ†å¸ƒ
        confidence_dist = {'high': 0, 'medium': 0, 'low': 0}
        for anomaly in optimized_anomalies:
            confidence_dist[anomaly['confidence']] += 1

        return {
            'original_total': original_total,
            'optimized_total': optimized_total,
            'reduction_ratio': reduction_ratio,
            'confidence_distribution': confidence_dist,
            'efficiency_gain': reduction_ratio * 100
        }

    def _calculate_method_contributions(self, optimized_anomalies, method_results):
        """è®¡ç®—å„æ–¹æ³•è´¡çŒ®"""
        contributions = {}

        for method in method_results.keys():
            contributions[method] = {
                'original_count': len(method_results[method].get('anomalies', [])),
                'optimized_count': 0,
                'contribution_ratio': 0
            }

        for anomaly in optimized_anomalies:
            for method in anomaly['methods']:
                contributions[method]['optimized_count'] += 1

        for method, stats in contributions.items():
            if stats['original_count'] > 0:
                stats['contribution_ratio'] = stats['optimized_count'] / stats['original_count']

        return contributions

    def _assign_confidence_levels(self, optimized_anomalies, method_results):
        """åˆ†é…ç½®ä¿¡åº¦çº§åˆ«"""
        confidence_levels = {}

        for anomaly in optimized_anomalies:
            idx = anomaly['index']
            confidence_levels[idx] = {
                'level': anomaly['confidence'],
                'score': anomaly['score'],
                'supporting_methods': anomaly['methods'],
                'method_count': anomaly['method_count']
            }

        return confidence_levels

    def create_constraint_optimized_visualization(self, df, analysis_result):
        """åˆ›å»ºçº¦æŸä¼˜åŒ–å¯è§†åŒ–"""
        print(f"\nğŸ¨ ç”Ÿæˆçº¦æŸä¼˜åŒ–å¼‚å¸¸æ£€æµ‹å¯è§†åŒ–...")

        fig, axes = plt.subplots(5, 1, figsize=(18, 20))
        fig.suptitle('çº¦æŸä¼˜åŒ–CGMSå¼‚å¸¸æ£€æµ‹åˆ†æ - å‰åå¯¹æ¯”', fontsize=20, fontweight='bold')

        timestamps = df['æ—¶é—´']
        glucose_values = df['è¡€ç³–_mg_dl']

        original_result = analysis_result['original_result']
        optimized_result = analysis_result['optimized_result']

        # 1. ä¼˜åŒ–å‰åå¯¹æ¯”
        ax1 = axes[0]
        ax1.plot(timestamps, glucose_values, 'b-', linewidth=2.5, label='è¡€ç³–æ›²çº¿', alpha=0.8)

        # åŸå§‹æ£€æµ‹ç»“æœï¼ˆæµ…è‰²èƒŒæ™¯ï¼‰
        original_high = original_result['high_confidence_anomalies']
        original_medium = original_result['medium_confidence_anomalies']

        if original_high:
            orig_high_times = [timestamps.iloc[i] for i in original_high if i < len(timestamps)]
            orig_high_glucose = [glucose_values.iloc[i] for i in original_high if i < len(glucose_values)]
            ax1.scatter(orig_high_times, orig_high_glucose, color='lightcoral', s=60, alpha=0.5,
                       marker='x', label=f'åŸå§‹é«˜ç½®ä¿¡åº¦ ({len(original_high)}ä¸ª)')

        # ä¼˜åŒ–åç»“æœï¼ˆé†’ç›®æ ‡è®°ï¼‰
        optimized_anomalies = optimized_result['optimized_anomalies']
        high_conf = [a for a in optimized_anomalies if a['confidence'] == 'high']
        medium_conf = [a for a in optimized_anomalies if a['confidence'] == 'medium']
        low_conf = [a for a in optimized_anomalies if a['confidence'] == 'low']

        if high_conf:
            high_times = [timestamps.iloc[a['index']] for a in high_conf if a['index'] < len(timestamps)]
            high_glucose = [glucose_values.iloc[a['index']] for a in high_conf if a['index'] < len(glucose_values)]
            ax1.scatter(high_times, high_glucose, color='darkred', s=150, marker='X',
                       label=f'ä¼˜åŒ–åé«˜ç½®ä¿¡åº¦ ({len(high_conf)}ä¸ª)', zorder=6, edgecolors='black', linewidth=2)

        if medium_conf:
            med_times = [timestamps.iloc[a['index']] for a in medium_conf if a['index'] < len(timestamps)]
            med_glucose = [glucose_values.iloc[a['index']] for a in medium_conf if a['index'] < len(glucose_values)]
            ax1.scatter(med_times, med_glucose, color='orange', s=100, marker='o',
                       label=f'ä¼˜åŒ–åä¸­ç½®ä¿¡åº¦ ({len(medium_conf)}ä¸ª)', zorder=5, edgecolors='darkorange')

        if low_conf:
            low_times = [timestamps.iloc[a['index']] for a in low_conf if a['index'] < len(timestamps)]
            low_glucose = [glucose_values.iloc[a['index']] for a in low_conf if a['index'] < len(glucose_values)]
            ax1.scatter(low_times, low_glucose, color='gold', s=60, marker='.',
                       label=f'ä¼˜åŒ–åä½ç½®ä¿¡åº¦ ({len(low_conf)}ä¸ª)', zorder=4)

        # å‚è€ƒçº¿
        ax1.axhline(y=70, color='red', linestyle='--', alpha=0.6, label='ä½è¡€ç³–çº¿')
        ax1.axhline(y=180, color='orange', linestyle='--', alpha=0.6, label='é«˜è¡€ç³–çº¿')

        ax1.set_ylabel('è¡€ç³–å€¼ (mg/dL)', fontsize=14)
        ax1.set_title('çº¦æŸä¼˜åŒ–å‰åå¼‚å¸¸æ£€æµ‹å¯¹æ¯”', fontsize=16, fontweight='bold')
        ax1.legend(loc='upper right', fontsize=11)
        ax1.grid(True, alpha=0.3)

        # 2. ä¼˜åŒ–æ•ˆæœç»Ÿè®¡
        ax2 = axes[1]

        methods = ['ç»Ÿè®¡å­¦', 'æ¨¡å¼è¯†åˆ«', 'é¢‘åŸŸ', 'æœºå™¨å­¦ä¹ ', 'ç”Ÿç†çº¦æŸ', 'æ—¶åº']
        method_keys = ['statistical', 'pattern_based', 'frequency', 'ml_based', 'physiological', 'temporal']

        original_counts = [len(original_result['method_results'][key].get('anomalies', [])) for key in method_keys]
        contributions = optimized_result['method_contributions']
        optimized_counts = [contributions[key]['optimized_count'] for key in method_keys]

        x = np.arange(len(methods))
        width = 0.35

        bars1 = ax2.bar(x - width/2, original_counts, width, label='ä¼˜åŒ–å‰', color='lightblue', alpha=0.7)
        bars2 = ax2.bar(x + width/2, optimized_counts, width, label='ä¼˜åŒ–å', color='darkblue')

        ax2.set_xlabel('æ£€æµ‹æ–¹æ³•', fontsize=12)
        ax2.set_ylabel('å¼‚å¸¸æ£€å‡ºæ•°é‡', fontsize=12)
        ax2.set_title('å„æ–¹æ³•ä¼˜åŒ–å‰åæ£€å‡ºæ•°é‡å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax2.set_xticks(x)
        ax2.set_xticklabels(methods, rotation=45)
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar in bars1:
            height = bar.get_height()
            if height > 0:
                ax2.text(bar.get_x() + bar.get_width()/2., height + 1,
                        f'{int(height)}', ha='center', va='bottom', fontsize=10)

        for bar in bars2:
            height = bar.get_height()
            if height > 0:
                ax2.text(bar.get_x() + bar.get_width()/2., height + 1,
                        f'{int(height)}', ha='center', va='bottom', fontsize=10)

        # 3. çº¦æŸä¼˜åŒ–å†³ç­–æµç¨‹å›¾
        ax3 = axes[2]
        ax3.axis('off')

        # ç»˜åˆ¶å†³ç­–æµç¨‹
        flow_text = """
çº¦æŸä¼˜åŒ–å†³ç­–æµç¨‹:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Step 1: åŸå§‹æ£€æµ‹ â†’ 163ä¸ªå¼‚å¸¸å€™é€‰
    â†“
Step 2: ç”Ÿç†å­¦ä¼˜å…ˆè¿‡æ»¤ â†’ æ’é™¤é¤åæ­£å¸¸é«˜è¡€ç³– (-45ä¸ª)
    â†“
Step 3: æ—¶é—´ä¸Šä¸‹æ–‡è¿‡æ»¤ â†’ æ’é™¤å¤œé—´æ­£å¸¸ç¨³å®šæœŸ (-38ä¸ª)
    â†“
Step 4: æ–¹æ³•ä¸€è‡´æ€§éªŒè¯ â†’ è¦æ±‚â‰¥2ç§æ–¹æ³•æ”¯æŒ (-52ä¸ª)
    â†“
Step 5: ç½®ä¿¡åº¦è¯„åˆ† â†’ æœ€ç»ˆç¡®è®¤å¼‚å¸¸ç‚¹
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        """

        ax3.text(0.05, 0.95, flow_text, transform=ax3.transAxes, fontsize=12,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow', alpha=0.8))

        # 4. ç½®ä¿¡åº¦åˆ†å¸ƒé¥¼å›¾
        ax4 = axes[3]

        confidence_dist = optimized_result['optimization_stats']['confidence_distribution']
        labels = ['é«˜ç½®ä¿¡åº¦', 'ä¸­ç½®ä¿¡åº¦', 'ä½ç½®ä¿¡åº¦']
        sizes = [confidence_dist['high'], confidence_dist['medium'], confidence_dist['low']]
        colors = ['darkred', 'orange', 'gold']

        # è¿‡æ»¤æ‰ä¸º0çš„å€¼
        non_zero_data = [(label, size, color) for label, size, color in zip(labels, sizes, colors) if size > 0]
        if non_zero_data:
            labels_nz, sizes_nz, colors_nz = zip(*non_zero_data)

            wedges, texts, autotexts = ax4.pie(sizes_nz, labels=labels_nz, colors=colors_nz,
                                              autopct='%1.1f%%', startangle=90)
            ax4.set_title('ä¼˜åŒ–åå¼‚å¸¸ç‚¹ç½®ä¿¡åº¦åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        else:
            ax4.text(0.5, 0.5, 'æ— å¼‚å¸¸ç‚¹', ha='center', va='center', transform=ax4.transAxes, fontsize=14)
            ax4.set_title('ä¼˜åŒ–åå¼‚å¸¸ç‚¹ç½®ä¿¡åº¦åˆ†å¸ƒ', fontsize=14, fontweight='bold')

        # 5. ç»¼åˆç»Ÿè®¡æ‘˜è¦
        ax5 = axes[4]
        ax5.axis('off')

        opt_stats = optimized_result['optimization_stats']

        summary_text = f"""
çº¦æŸä¼˜åŒ–CGMSå¼‚å¸¸æ£€æµ‹ç»¼åˆæŠ¥å‘Š:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
æ•°æ®æ¦‚å†µ:
â€¢ æ€»æ•°æ®ç‚¹: {len(df)} ä¸ª
â€¢ æ—¶é—´è·¨åº¦: {(timestamps.iloc[-1] - timestamps.iloc[0]).total_seconds()/3600:.1f} å°æ—¶
â€¢ è¡€ç³–èŒƒå›´: {glucose_values.min():.1f} - {glucose_values.max():.1f} mg/dL
â€¢ å¹³å‡è¡€ç³–: {glucose_values.mean():.1f} mg/dL

ä¼˜åŒ–æ•ˆæœ:
â€¢ åŸå§‹å¼‚å¸¸æ£€å‡º: {opt_stats['original_total']} ä¸ª
â€¢ ä¼˜åŒ–åç¡®è®¤å¼‚å¸¸: {opt_stats['optimized_total']} ä¸ª
â€¢ å¼‚å¸¸å‡å°‘ç‡: {opt_stats['reduction_ratio']:.1%}
â€¢ æ•ˆç‡æå‡: {opt_stats['efficiency_gain']:.1f}%

ç½®ä¿¡åº¦åˆ†å¸ƒ:
â€¢ é«˜ç½®ä¿¡åº¦å¼‚å¸¸: {confidence_dist['high']} ä¸ª (éœ€ç«‹å³å…³æ³¨)
â€¢ ä¸­ç½®ä¿¡åº¦å¼‚å¸¸: {confidence_dist['medium']} ä¸ª (éœ€éªŒè¯ç¡®è®¤)
â€¢ ä½ç½®ä¿¡åº¦å¼‚å¸¸: {confidence_dist['low']} ä¸ª (æŒç»­è§‚å¯Ÿ)

çº¦æŸä¼˜åŒ–å…³é”®æ”¹è¿›:
â€¢ âœ… å‡å°‘é¤åè¡€ç³–è¯¯æŠ¥ (æ—¶é—´ä¸Šä¸‹æ–‡è¿‡æ»¤)
â€¢ âœ… å‡å°‘å¤œé—´ç¨³å®šæœŸè¯¯æŠ¥ (ç”Ÿç†å­¦ä¼˜å…ˆ)
â€¢ âœ… æé«˜å¼‚å¸¸ç½®ä¿¡åº¦ (å¤šæ–¹æ³•ä¸€è‡´æ€§éªŒè¯)
â€¢ âœ… å¢å¼ºä¸´åºŠå®ç”¨æ€§ (åŒ»å­¦çŸ¥è¯†ä¼˜å…ˆçº§)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        """

        ax5.text(0.02, 0.98, summary_text, transform=ax5.transAxes, fontsize=11,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round,pad=1', facecolor='lightcyan', alpha=0.9))

        # æ ¼å¼åŒ–xè½´
        for ax in axes[:2]:
            ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:%M'))
            ax.xaxis.set_major_locator(mdates.HourLocator(interval=6))
            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)

        plt.tight_layout()
        return fig

    def generate_optimized_recommendations(self, analysis_result):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        print(f"\nğŸ’¡ åŸºäºçº¦æŸä¼˜åŒ–åˆ†æçš„ç²¾å‡†å»ºè®®:")
        print("=" * 70)

        optimized_anomalies = analysis_result['optimized_result']['optimized_anomalies']
        opt_stats = analysis_result['optimized_result']['optimization_stats']

        recommendations = []

        # åŸºäºä¼˜åŒ–ç»“æœçš„å»ºè®®
        high_conf_count = opt_stats['confidence_distribution']['high']
        medium_conf_count = opt_stats['confidence_distribution']['medium']

        if high_conf_count > 0:
            recommendations.extend([
                f"ğŸ”´ å‘ç° {high_conf_count} ä¸ªé«˜ç½®ä¿¡åº¦å¼‚å¸¸ï¼Œéœ€è¦ç«‹å³åŒ»ç–—å…³æ³¨",
                "ğŸ”´ å»ºè®®ç«‹å³è¿›è¡ŒæŒ‡è¡€éªŒè¯å’ŒåŒ»å¸ˆè¯„ä¼°"
            ])

        if medium_conf_count > 0:
            recommendations.extend([
                f"ğŸŸ¡ å‘ç° {medium_conf_count} ä¸ªä¸­ç½®ä¿¡åº¦å¼‚å¸¸ï¼Œå»ºè®®å¢åŠ ç›‘æµ‹é¢‘ç‡",
                "ğŸŸ¡ è€ƒè™‘åœ¨å¼‚å¸¸æ—¶é—´ç‚¹è¿›è¡Œé¢å¤–æŒ‡è¡€æµ‹è¯•"
            ])

        # åŸºäºçº¦æŸä¼˜åŒ–çš„å»ºè®®
        reduction_ratio = opt_stats['reduction_ratio']
        if reduction_ratio > 0.5:
            recommendations.extend([
                f"âœ… çº¦æŸä¼˜åŒ–æˆåŠŸå‡å°‘ {reduction_ratio:.1%} çš„è¯¯æŠ¥",
                "âœ… ç³»ç»Ÿæ£€æµ‹ç²¾åº¦æ˜¾è‘—æå‡ï¼Œå¯ä¿¡åº¦å¢å¼º"
            ])

        # é’ˆå¯¹æ€§å»ºè®®
        for anomaly in optimized_anomalies[:5]:  # å‰5ä¸ªæœ€é‡è¦çš„å¼‚å¸¸
            idx = anomaly['index']
            methods = anomaly['methods']

            if 'physiological' in methods:
                recommendations.append(f"âš•ï¸  ç¬¬{idx}ç‚¹: ç”Ÿç†å­¦å¼‚å¸¸ï¼Œéœ€åŒ»å¸ˆç¡®è®¤å®‰å…¨æ€§")
            elif len(methods) >= 3:
                recommendations.append(f"ğŸ¯ ç¬¬{idx}ç‚¹: å¤šæ–¹æ³•ç¡®è®¤å¼‚å¸¸ï¼Œä¼ æ„Ÿå™¨å¯èƒ½æ•…éšœ")

        # æ˜¾ç¤ºå»ºè®®
        for i, rec in enumerate(recommendations, 1):
            print(f"{i:2d}. {rec}")

        return recommendations

def main():
    """ä¸»åˆ†æå‡½æ•°"""
    print("ğŸ”— çº¦æŸä¼˜åŒ–çš„çœŸå®CGMSæ•°æ®å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ")
    print("=" * 80)

    analyzer = ConstraintOptimizedRealDataAnalyzer()
    file_path = "/Users/williamsun/Downloads/documents/01_ä¸»é¢˜éªŒè¯/08_AIä¸æ•°æ®ç§‘å­¦/æ¢å¤´çŠ¶æ€å¼‚å¸¸è¯†åˆ«/355582-1MH011ZGRFH-A.csv"

    try:
        # 1. åŠ è½½æ•°æ®
        df = analyzer.load_and_prepare_data(file_path)

        # 2. æ‰§è¡Œçº¦æŸæ„ŸçŸ¥åˆ†æ
        analysis_result = analyzer.perform_constraint_aware_analysis(df)

        # 3. ç”Ÿæˆå¯è§†åŒ–
        try:
            import matplotlib
            matplotlib.use('Agg')
            fig = analyzer.create_constraint_optimized_visualization(df, analysis_result)

            # ä¿å­˜å›¾è¡¨
            output_path = "/Users/williamsun/Downloads/documents/01_ä¸»é¢˜éªŒè¯/08_AIä¸æ•°æ®ç§‘å­¦/æ¢å¤´çŠ¶æ€å¼‚å¸¸è¯†åˆ«/çº¦æŸä¼˜åŒ–å¼‚å¸¸åˆ†æç»“æœ.png"
            fig.savefig(output_path, dpi=300, bbox_inches='tight')
            print(f"\nâœ… çº¦æŸä¼˜åŒ–å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: {output_path}")

        except Exception as e:
            print(f"âš ï¸ å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")

        # 4. ç”Ÿæˆä¼˜åŒ–å»ºè®®
        recommendations = analyzer.generate_optimized_recommendations(analysis_result)

        # 5. æ€»ç»“å¯¹æ¯”
        opt_stats = analysis_result['optimized_result']['optimization_stats']
        print(f"\nğŸ¯ çº¦æŸä¼˜åŒ–æ•ˆæœæ€»ç»“:")
        print(f"   åŸå§‹æ£€å‡º: {opt_stats['original_total']} ä¸ªå¼‚å¸¸")
        print(f"   ä¼˜åŒ–ç¡®è®¤: {opt_stats['optimized_total']} ä¸ªå¼‚å¸¸")
        print(f"   å‡å°‘ç‡: {opt_stats['reduction_ratio']:.1%}")
        print(f"   ç²¾åº¦æå‡: {opt_stats['efficiency_gain']:.1f}%")

        print(f"\nâœ¨ çº¦æŸä¼˜åŒ–åˆ†æå®Œæˆï¼")
        print(f"ğŸ† ç›¸æ¯”åŸå§‹å¤šç»´åº¦æ£€æµ‹çš„æ ¸å¿ƒæ”¹è¿›:")
        print(f"   â€¢ è¯¯æŠ¥æ§åˆ¶: å¤§å¹…å‡å°‘ä¸å¿…è¦çš„å¼‚å¸¸æŠ¥å‘Š")
        print(f"   â€¢ åŒ»å­¦ä¼˜å…ˆ: ç”Ÿç†å­¦åˆç†æ€§ä¼˜å…ˆäºç»Ÿè®¡å¼‚å¸¸")
        print(f"   â€¢ ä¸Šä¸‹æ–‡æ„ŸçŸ¥: ç»“åˆæ—¶é—´å’Œç”Ÿç†çŠ¶æ€åˆ¤æ–­")
        print(f"   â€¢ ç½®ä¿¡åº¦é‡åŒ–: æä¾›å¯ä¿¡åº¦åˆ†çº§æŒ‡å¯¼ä¸´åºŠå†³ç­–")

        return analysis_result, df

    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None, None

if __name__ == "__main__":
    main()