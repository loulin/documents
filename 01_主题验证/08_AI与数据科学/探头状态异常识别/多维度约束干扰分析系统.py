#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šç»´åº¦CGMSå¼‚å¸¸æ£€æµ‹æ–¹æ³•é—´çº¦æŸå¹²æ‰°åˆ†æç³»ç»Ÿ
åˆ†æå„æ£€æµ‹ç»´åº¦ä¹‹é—´çš„ç›¸äº’å½±å“ã€çº¦æŸå…³ç³»å’Œå†²çªè§£å†³
"""

import numpy as np
import pandas as pd
from scipy import stats
from collections import defaultdict
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

class InterDimensionalConstraintAnalyzer:
    """å¤šç»´åº¦çº¦æŸå¹²æ‰°åˆ†æå™¨"""

    def __init__(self):
        # å®šä¹‰å„ç»´åº¦ä¹‹é—´çš„ç†è®ºå…³ç³»
        self.interdependency_matrix = {
            'statistical': {
                'conflicts_with': ['pattern_based'],  # ç»Ÿè®¡å¼‚å¸¸vsæ¨¡å¼æ­£å¸¸
                'supports': ['physiological'],        # ç»Ÿè®¡å¼‚å¸¸é€šå¸¸æ”¯æŒç”Ÿç†å¼‚å¸¸
                'influenced_by': ['ml_based'],        # MLç»“æœå½±å“ç»Ÿè®¡åˆ¤æ–­
                'influences': ['temporal'],           # ç»Ÿè®¡ç‰¹å¾å½±å“æ—¶åºåˆ¤æ–­
            },
            'pattern_based': {
                'conflicts_with': ['statistical', 'frequency'],  # æ¨¡å¼vsç»Ÿè®¡/é¢‘åŸŸå†²çª
                'supports': ['temporal'],             # æ¨¡å¼æ”¯æŒæ—¶åºåˆ†æ
                'influenced_by': ['frequency'],       # é¢‘åŸŸå½±å“æ¨¡å¼è¯†åˆ«
                'influences': ['ml_based'],           # æ¨¡å¼ç‰¹å¾è®­ç»ƒML
            },
            'frequency': {
                'conflicts_with': ['pattern_based'],  # é¢‘åŸŸvsæ¨¡å¼å¯èƒ½å†²çª
                'supports': ['temporal'],             # é¢‘åŸŸæ”¯æŒæ—¶åº
                'influenced_by': [],                  # ç›¸å¯¹ç‹¬ç«‹
                'influences': ['pattern_based'],      # é¢‘åŸŸä¿¡æ¯å½±å“æ¨¡å¼
            },
            'ml_based': {
                'conflicts_with': [],                # MLç›¸å¯¹ä¸­æ€§
                'supports': ['statistical'],         # MLå¯ä»¥æ”¯æŒç»Ÿè®¡åˆ¤æ–­
                'influenced_by': ['all_methods'],     # å—æ‰€æœ‰æ–¹æ³•å½±å“
                'influences': ['final_decision'],     # å½±å“æœ€ç»ˆå†³ç­–
            },
            'physiological': {
                'conflicts_with': ['statistical'],   # ç”Ÿç†åˆç†vsç»Ÿè®¡å¼‚å¸¸
                'supports': [],                      # ç”Ÿç†çº¦æŸç›¸å¯¹ç‹¬ç«‹
                'influenced_by': [],                 # åŸºäºåŒ»å­¦çŸ¥è¯†
                'influences': ['all_methods'],       # å½±å“æ‰€æœ‰æ–¹æ³•çš„æƒé‡
            },
            'temporal': {
                'conflicts_with': [],               # æ—¶åºç›¸å¯¹ç‹¬ç«‹
                'supports': ['pattern_based'],      # æ”¯æŒæ¨¡å¼è¯†åˆ«
                'influenced_by': ['statistical', 'pattern_based', 'frequency'],
                'influences': ['ml_based'],         # æ—¶åºç‰¹å¾å½±å“ML
            }
        }

    def analyze_constraint_relationships(self, detection_results):
        """åˆ†æçº¦æŸå…³ç³»"""
        print("ğŸ”— åˆ†æå„æ£€æµ‹ç»´åº¦ä¹‹é—´çš„çº¦æŸå…³ç³»")
        print("=" * 60)

        constraint_analysis = {}
        method_results = detection_results['method_results']

        # 1. ç›´æ¥å†²çªåˆ†æ
        conflicts = self._analyze_direct_conflicts(method_results)
        constraint_analysis['direct_conflicts'] = conflicts

        # 2. æ”¯æŒ-åå¯¹å…³ç³»åˆ†æ
        support_analysis = self._analyze_support_relationships(method_results)
        constraint_analysis['support_relationships'] = support_analysis

        # 3. å½±å“å¼ºåº¦åˆ†æ
        influence_analysis = self._analyze_influence_strength(method_results)
        constraint_analysis['influence_strength'] = influence_analysis

        return constraint_analysis

    def _analyze_direct_conflicts(self, method_results):
        """åˆ†æç›´æ¥å†²çª"""
        conflicts = []

        # ç»Ÿè®¡å­¦ vs ç”Ÿç†å­¦å†²çª
        stat_anomalies = set(method_results.get('statistical', {}).get('anomalies', []))
        physio_anomalies = set(method_results.get('physiological', {}).get('anomalies', []))

        stat_only = stat_anomalies - physio_anomalies
        if len(stat_only) > 0:
            conflicts.append({
                'type': 'ç»Ÿè®¡-ç”Ÿç†å†²çª',
                'description': 'ç»Ÿè®¡å­¦æ£€æµ‹ä¸ºå¼‚å¸¸ä½†ç”Ÿç†å­¦è®¤ä¸ºæ­£å¸¸',
                'conflict_points': list(stat_only),
                'severity': 'medium',
                'resolution': 'ä¼˜å…ˆè€ƒè™‘ç”Ÿç†å­¦åˆç†æ€§'
            })

        # æ¨¡å¼è¯†åˆ« vs é¢‘åŸŸåˆ†æå†²çª
        pattern_anomalies = set(method_results.get('pattern_based', {}).get('anomalies', []))
        freq_anomalies = set(method_results.get('frequency', {}).get('anomalies', []))

        pattern_only = pattern_anomalies - freq_anomalies
        freq_only = freq_anomalies - pattern_anomalies

        if len(pattern_only) > 10:  # æ¨¡å¼æ£€æµ‹å¾ˆå¤šï¼Œé¢‘åŸŸå¾ˆå°‘
            conflicts.append({
                'type': 'æ¨¡å¼-é¢‘åŸŸå†²çª',
                'description': 'æ¨¡å¼è¯†åˆ«å‘ç°å¼‚å¸¸ä½†é¢‘åŸŸåˆ†ææ­£å¸¸',
                'conflict_points': list(pattern_only),
                'severity': 'low',
                'resolution': 'å¯èƒ½æ˜¯éå‘¨æœŸæ€§æ¨¡å¼å¼‚å¸¸ï¼Œä¼˜å…ˆæ¨¡å¼è¯†åˆ«'
            })

        return conflicts

    def _analyze_support_relationships(self, method_results):
        """åˆ†ææ”¯æŒå…³ç³»"""
        support_relationships = {}

        # æ£€æŸ¥å„æ–¹æ³•å¯¹çš„é‡å ç¨‹åº¦
        method_pairs = [
            ('statistical', 'physiological', 'ç»Ÿè®¡-ç”Ÿç†ä¸€è‡´æ€§'),
            ('pattern_based', 'temporal', 'æ¨¡å¼-æ—¶åºä¸€è‡´æ€§'),
            ('frequency', 'temporal', 'é¢‘åŸŸ-æ—¶åºä¸€è‡´æ€§'),
            ('ml_based', 'statistical', 'ML-ç»Ÿè®¡ä¸€è‡´æ€§')
        ]

        for method1, method2, name in method_pairs:
            anomalies1 = set(method_results.get(method1, {}).get('anomalies', []))
            anomalies2 = set(method_results.get(method2, {}).get('anomalies', []))

            if len(anomalies1) > 0 and len(anomalies2) > 0:
                overlap = anomalies1 & anomalies2
                overlap_ratio = len(overlap) / min(len(anomalies1), len(anomalies2))

                support_relationships[name] = {
                    'overlap_count': len(overlap),
                    'overlap_ratio': overlap_ratio,
                    'support_strength': 'high' if overlap_ratio > 0.7 else 'medium' if overlap_ratio > 0.3 else 'low',
                    'overlapping_points': list(overlap)
                }

        return support_relationships

    def _analyze_influence_strength(self, method_results):
        """åˆ†æå½±å“å¼ºåº¦"""
        influence_matrix = np.zeros((6, 6))  # 6x6 æ–¹æ³•å½±å“çŸ©é˜µ
        method_names = ['statistical', 'pattern_based', 'frequency', 'ml_based', 'physiological', 'temporal']

        for i, method_i in enumerate(method_names):
            anomalies_i = set(method_results.get(method_i, {}).get('anomalies', []))

            for j, method_j in enumerate(method_names):
                if i != j:
                    anomalies_j = set(method_results.get(method_j, {}).get('anomalies', []))

                    if len(anomalies_i) > 0 and len(anomalies_j) > 0:
                        # è®¡ç®—å½±å“å¼ºåº¦ï¼šiæ–¹æ³•æ£€å‡ºçš„å¼‚å¸¸æœ‰å¤šå°‘è¢«jæ–¹æ³•æ”¯æŒ
                        support_count = len(anomalies_i & anomalies_j)
                        influence_strength = support_count / len(anomalies_i)
                        influence_matrix[i, j] = influence_strength

        return {
            'influence_matrix': influence_matrix,
            'method_names': method_names,
            'dominant_influencer': method_names[np.argmax(np.sum(influence_matrix, axis=1))],
            'most_influenced': method_names[np.argmax(np.sum(influence_matrix, axis=0))]
        }

    def identify_interference_patterns(self, glucose_data, detection_results):
        """è¯†åˆ«å¹²æ‰°æ¨¡å¼"""
        print("\nâš¡ è¯†åˆ«å„ç»´åº¦é—´çš„å¹²æ‰°æ¨¡å¼")
        print("=" * 60)

        interference_patterns = {}

        # 1. æ•°æ®ä¾èµ–å¹²æ‰°
        data_interference = self._analyze_data_dependency_interference(glucose_data, detection_results)
        interference_patterns['data_dependency'] = data_interference

        # 2. é˜ˆå€¼ç«äº‰å¹²æ‰°
        threshold_interference = self._analyze_threshold_competition(detection_results)
        interference_patterns['threshold_competition'] = threshold_interference

        # 3. ç‰¹å¾ç©ºé—´é‡å å¹²æ‰°
        feature_interference = self._analyze_feature_overlap_interference(glucose_data)
        interference_patterns['feature_overlap'] = feature_interference

        return interference_patterns

    def _analyze_data_dependency_interference(self, glucose_data, detection_results):
        """åˆ†ææ•°æ®ä¾èµ–å¹²æ‰°"""
        interference = {}

        # é‡‡æ ·ç‡ä¾èµ–
        time_intervals = np.diff(range(len(glucose_data)))  # ç®€åŒ–æ—¶é—´é—´éš”
        irregular_sampling = np.std(time_intervals) > 0

        if irregular_sampling:
            interference['sampling_rate'] = {
                'affected_methods': ['frequency', 'temporal', 'pattern_based'],
                'severity': 'high',
                'description': 'ä¸è§„å¾‹é‡‡æ ·å½±å“é¢‘åŸŸå’Œæ—¶åºåˆ†æ',
                'solution': 'æ•°æ®é¢„å¤„ç†ï¼šæ’å€¼æˆ–é‡é‡‡æ ·'
            }

        # çª—å£å¤§å°ä¾èµ–
        short_data = len(glucose_data) < 50
        if short_data:
            interference['window_size'] = {
                'affected_methods': ['pattern_based', 'frequency', 'ml_based'],
                'severity': 'medium',
                'description': 'æ•°æ®é•¿åº¦ä¸è¶³å½±å“æ¨¡å¼è¯†åˆ«å’Œé¢‘åŸŸåˆ†æ',
                'solution': 'è°ƒæ•´çª—å£å¤§å°æˆ–ç­‰å¾…æ›´å¤šæ•°æ®'
            }

        return interference

    def _analyze_threshold_competition(self, detection_results):
        """åˆ†æé˜ˆå€¼ç«äº‰"""
        competition = {}
        method_results = detection_results['method_results']

        # è®¡ç®—å„æ–¹æ³•çš„æ£€å‡ºç‡
        total_points = max([len(result.get('anomalies', [])) for result in method_results.values()]) or 1

        detection_rates = {}
        for method, result in method_results.items():
            rate = len(result.get('anomalies', [])) / total_points
            detection_rates[method] = rate

        # è¯†åˆ«è¿‡æ•æ„Ÿå’Œè¿‡ä¿å®ˆçš„æ–¹æ³•
        mean_rate = np.mean(list(detection_rates.values()))
        std_rate = np.std(list(detection_rates.values()))

        over_sensitive = [method for method, rate in detection_rates.items()
                         if rate > mean_rate + std_rate]
        under_sensitive = [method for method, rate in detection_rates.items()
                          if rate < mean_rate - std_rate]

        competition['sensitivity_imbalance'] = {
            'over_sensitive_methods': over_sensitive,
            'under_sensitive_methods': under_sensitive,
            'balance_score': 1 - std_rate / (mean_rate + 1e-6),
            'recommendation': 'è°ƒæ•´è¿‡æ•æ„Ÿæ–¹æ³•çš„é˜ˆå€¼'
        }

        return competition

    def _analyze_feature_overlap_interference(self, glucose_data):
        """åˆ†æç‰¹å¾ç©ºé—´é‡å å¹²æ‰°"""
        overlap = {}

        # ç»Ÿè®¡å­¦å’ŒMLçš„ç‰¹å¾é‡å 
        # ä¸¤è€…éƒ½ä½¿ç”¨ç»Ÿè®¡ç‰¹å¾ï¼Œå¯èƒ½äº§ç”Ÿå†—ä½™
        overlap['statistical_ml'] = {
            'overlap_type': 'feature_redundancy',
            'severity': 'medium',
            'description': 'ç»Ÿè®¡å­¦ç‰¹å¾ä¸MLç‰¹å¾ç©ºé—´é‡å ',
            'impact': 'å¯èƒ½å¯¼è‡´è¿‡åº¦æƒé‡ç»Ÿè®¡å¼‚å¸¸',
            'solution': 'MLä¸­å‡å°‘ç»Ÿè®¡ç‰¹å¾æƒé‡æˆ–ä½¿ç”¨ç‰¹å¾é€‰æ‹©'
        }

        # æ¨¡å¼è¯†åˆ«å’Œæ—¶åºåˆ†æçš„é‡å 
        # ä¸¤è€…éƒ½åˆ†ææ—¶é—´ç›¸å…³æ¨¡å¼
        overlap['pattern_temporal'] = {
            'overlap_type': 'temporal_pattern_redundancy',
            'severity': 'low',
            'description': 'æ¨¡å¼è¯†åˆ«ä¸æ—¶åºåˆ†æåœ¨æ—¶é—´æ¨¡å¼ä¸Šé‡å ',
            'impact': 'æ—¶é—´ç›¸å…³å¼‚å¸¸å¯èƒ½è¢«åŒé‡è®¡ç®—',
            'solution': 'åœ¨é›†æˆæ—¶è°ƒæ•´è¿™ä¸¤ç§æ–¹æ³•çš„æƒé‡'
        }

        return overlap

    def design_constraint_resolution_framework(self, constraint_analysis, interference_patterns):
        """è®¾è®¡çº¦æŸè§£å†³æ¡†æ¶"""
        print("\nğŸ› ï¸ è®¾è®¡çº¦æŸè§£å†³æ¡†æ¶")
        print("=" * 60)

        resolution_framework = {
            'conflict_resolution_rules': {},
            'weight_adjustment_strategy': {},
            'decision_hierarchy': {},
            'adaptive_mechanisms': {}
        }

        # 1. å†²çªè§£å†³è§„åˆ™
        resolution_framework['conflict_resolution_rules'] = {
            'statistical_vs_physiological': {
                'rule': 'physiological_priority',
                'logic': 'ç”Ÿç†å­¦åˆç†æ€§ä¼˜å…ˆäºç»Ÿè®¡å¼‚å¸¸',
                'implementation': 'if physio_normal and stat_abnormal: reduce_stat_weight(0.5)'
            },
            'pattern_vs_frequency': {
                'rule': 'pattern_priority_for_non_periodic',
                'logic': 'éå‘¨æœŸæ€§å¼‚å¸¸ä¼˜å…ˆæ¨¡å¼è¯†åˆ«',
                'implementation': 'if freq_normal and pattern_abnormal: confirm_non_periodic()'
            },
            'ml_vs_rule_based': {
                'rule': 'interpretable_priority',
                'logic': 'å¯è§£é‡Šæ–¹æ³•ä¼˜å…ˆäºé»‘ç›’ML',
                'implementation': 'if ml_only_detection: require_supporting_evidence()'
            }
        }

        # 2. æƒé‡è°ƒæ•´ç­–ç•¥
        resolution_framework['weight_adjustment_strategy'] = {
            'conflict_based_adjustment': {
                'high_conflict_methods': 'reduce_weight(-0.2)',
                'supportive_methods': 'increase_weight(+0.1)',
                'independent_methods': 'maintain_weight()'
            },
            'performance_based_adjustment': {
                'high_false_positive': 'reduce_weight(-0.3)',
                'high_false_negative': 'increase_sensitivity()',
                'balanced_performance': 'maintain_weight()'
            }
        }

        # 3. å†³ç­–å±‚çº§
        resolution_framework['decision_hierarchy'] = {
            'tier_1_critical': ['physiological'],  # ç”Ÿç†å®‰å…¨æœ€ä¼˜å…ˆ
            'tier_2_reliable': ['statistical', 'pattern_based'],  # å¯è§£é‡Šæ–¹æ³•
            'tier_3_supportive': ['temporal', 'frequency'],  # æ”¯æŒæ€§æ–¹æ³•
            'tier_4_ml': ['ml_based']  # MLä½œä¸ºè¾…åŠ©å†³ç­–
        }

        # 4. è‡ªé€‚åº”æœºåˆ¶
        resolution_framework['adaptive_mechanisms'] = {
            'dynamic_threshold': 'adjust_based_on_conflict_history()',
            'method_reliability_tracking': 'update_weights_based_on_performance()',
            'context_aware_resolution': 'apply_clinical_context_rules()',
            'feedback_incorporation': 'learn_from_clinical_feedback()'
        }

        return resolution_framework

    def implement_optimized_ensemble(self, glucose_data, timestamps, resolution_framework):
        """å®ç°ä¼˜åŒ–çš„é›†æˆæ£€æµ‹å™¨"""
        print("\nğŸš€ å®ç°çº¦æŸæ„ŸçŸ¥çš„ä¼˜åŒ–é›†æˆæ£€æµ‹å™¨")
        print("=" * 60)

        # åŸºç¡€æƒé‡
        base_weights = {
            'statistical': 0.20,
            'pattern_based': 0.25,
            'frequency': 0.10,
            'ml_based': 0.15,
            'physiological': 0.25,
            'temporal': 0.05
        }

        # æ¨¡æ‹Ÿå„æ–¹æ³•çš„æ£€æµ‹ç»“æœ
        method_detections = self._simulate_method_detections(glucose_data)

        # åº”ç”¨çº¦æŸè§£å†³æ¡†æ¶
        optimized_results = self._apply_constraint_resolution(
            method_detections, base_weights, resolution_framework
        )

        # ç”Ÿæˆæœ€ç»ˆå¼‚å¸¸ç‚¹
        final_anomalies = self._generate_final_anomalies(optimized_results)

        print(f"ğŸ“Š ä¼˜åŒ–é›†æˆç»“æœ:")
        print(f"  åŸå§‹æ£€æµ‹æ€»æ•°: {sum(len(det) for det in method_detections.values())}")
        print(f"  å†²çªè§£å†³å: {len(final_anomalies)} ä¸ªç¡®è®¤å¼‚å¸¸")
        print(f"  çº¦æŸä¸€è‡´æ€§: {optimized_results['consistency_score']:.3f}")

        return {
            'final_anomalies': final_anomalies,
            'method_contributions': optimized_results['method_contributions'],
            'conflict_resolutions': optimized_results['conflict_resolutions'],
            'consistency_score': optimized_results['consistency_score']
        }

    def _simulate_method_detections(self, glucose_data):
        """æ¨¡æ‹Ÿå„æ–¹æ³•æ£€æµ‹ç»“æœ"""
        detections = {}

        # ç»Ÿè®¡å­¦æ£€æµ‹
        z_scores = np.abs(stats.zscore(glucose_data))
        detections['statistical'] = np.where(z_scores > 2.5)[0].tolist()

        # ç”Ÿç†å­¦æ£€æµ‹
        detections['physiological'] = np.where((glucose_data < 60) | (glucose_data > 250))[0].tolist()

        # æ¨¡å¼æ£€æµ‹ï¼ˆç®€åŒ–ï¼‰
        pattern_anomalies = []
        for i in range(1, len(glucose_data)):
            if abs(glucose_data[i] - glucose_data[i-1]) > 30:
                pattern_anomalies.append(i)
        detections['pattern_based'] = pattern_anomalies

        # å…¶ä»–æ–¹æ³•ç®€åŒ–å®ç°
        detections['frequency'] = []
        detections['ml_based'] = np.random.choice(len(glucose_data), size=5, replace=False).tolist()
        detections['temporal'] = []

        return detections

    def _apply_constraint_resolution(self, method_detections, base_weights, resolution_framework):
        """åº”ç”¨çº¦æŸè§£å†³"""
        resolved_weights = base_weights.copy()
        conflict_resolutions = []

        # æ£€æµ‹å†²çª
        stat_set = set(method_detections['statistical'])
        physio_set = set(method_detections['physiological'])

        # ç»Ÿè®¡-ç”Ÿç†å†²çªè§£å†³
        stat_only = stat_set - physio_set
        if len(stat_only) > 0:
            resolved_weights['statistical'] *= 0.7  # é™ä½ç»Ÿè®¡å­¦æƒé‡
            conflict_resolutions.append({
                'conflict': 'statistical_vs_physiological',
                'resolution': 'reduced_statistical_weight',
                'affected_points': list(stat_only)
            })

        # è®¡ç®—ä¸€è‡´æ€§è¯„åˆ†
        all_detections = set()
        for detections in method_detections.values():
            all_detections.update(detections)

        method_agreement = {}
        for point in all_detections:
            agreement_count = sum(1 for detections in method_detections.values()
                                if point in detections)
            method_agreement[point] = agreement_count

        consistency_score = np.mean(list(method_agreement.values())) / len(method_detections)

        return {
            'resolved_weights': resolved_weights,
            'method_contributions': method_agreement,
            'conflict_resolutions': conflict_resolutions,
            'consistency_score': consistency_score
        }

    def _generate_final_anomalies(self, optimized_results):
        """ç”Ÿæˆæœ€ç»ˆå¼‚å¸¸ç‚¹åˆ—è¡¨"""
        # æ ¹æ®æ–¹æ³•ä¸€è‡´æ€§å’Œæƒé‡å†³å®šæœ€ç»ˆå¼‚å¸¸
        final_anomalies = []
        contributions = optimized_results['method_contributions']

        for point, agreement_count in contributions.items():
            if agreement_count >= 2:  # è‡³å°‘ä¸¤ç§æ–¹æ³•åŒæ„
                final_anomalies.append(point)

        return sorted(final_anomalies)

def demonstrate_constraint_analysis():
    """æ¼”ç¤ºçº¦æŸåˆ†æç³»ç»Ÿ"""
    print("ğŸ”— å¤šç»´åº¦CGMSå¼‚å¸¸æ£€æµ‹çº¦æŸå¹²æ‰°åˆ†ææ¼”ç¤º")
    print("=" * 80)

    analyzer = InterDimensionalConstraintAnalyzer()

    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)
    glucose_data = 120 + np.cumsum(np.random.normal(0, 3, 100))

    # æ·»åŠ ä¸€äº›ç‰¹å®šæ¨¡å¼æ¥æ¼”ç¤ºçº¦æŸå…³ç³»
    glucose_data[30] = 300    # ç»Ÿè®¡å¼‚å¸¸ä½†å¯èƒ½ç”Ÿç†æ­£å¸¸ï¼ˆé¤åï¼‰
    glucose_data[50:55] = 95  # æ¨¡å¼å¹³ç¨³ä½†ç»Ÿè®¡æ­£å¸¸
    glucose_data[80] = 40     # ç”Ÿç†å¼‚å¸¸ï¼Œåº”è¯¥ç»Ÿè®¡ä¹Ÿå¼‚å¸¸

    # æ¨¡æ‹Ÿæ£€æµ‹ç»“æœ
    detection_results = {
        'method_results': {
            'statistical': {'anomalies': [30, 80]},
            'physiological': {'anomalies': [80]},  # åªæœ‰çœŸæ­£æå€¼
            'pattern_based': {'anomalies': [30, 50, 51, 52, 53, 54]},
            'frequency': {'anomalies': []},
            'ml_based': {'anomalies': [30, 45, 80]},
            'temporal': {'anomalies': [45, 80]}
        }
    }

    print(f"ğŸ“Š æ¨¡æ‹Ÿæ•°æ®å’Œæ£€æµ‹ç»“æœ:")
    for method, result in detection_results['method_results'].items():
        print(f"  {method}: {len(result['anomalies'])} ä¸ªå¼‚å¸¸")

    # 1. çº¦æŸå…³ç³»åˆ†æ
    constraint_analysis = analyzer.analyze_constraint_relationships(detection_results)

    print(f"\nğŸ“ˆ çº¦æŸå…³ç³»åˆ†æç»“æœ:")
    for conflict in constraint_analysis['direct_conflicts']:
        print(f"  ğŸ”¥ {conflict['type']}: {conflict['description']}")
        print(f"     è§£å†³æ–¹æ¡ˆ: {conflict['resolution']}")

    # 2. å¹²æ‰°æ¨¡å¼è¯†åˆ«
    interference_patterns = analyzer.identify_interference_patterns(glucose_data, detection_results)

    print(f"\nâš¡ å¹²æ‰°æ¨¡å¼è¯†åˆ«:")
    for interference_type, details in interference_patterns.items():
        print(f"  {interference_type}: {len(details)} ç§å¹²æ‰°")

    # 3. çº¦æŸè§£å†³æ¡†æ¶
    resolution_framework = analyzer.design_constraint_resolution_framework(
        constraint_analysis, interference_patterns
    )

    # 4. ä¼˜åŒ–é›†æˆå®ç°
    optimized_result = analyzer.implement_optimized_ensemble(
        glucose_data, None, resolution_framework
    )

    print(f"\nâœ¨ çº¦æŸæ„ŸçŸ¥ä¼˜åŒ–å®Œæˆï¼")
    print(f"æœ€ç»ˆå¼‚å¸¸ç‚¹æ•°: {len(optimized_result['final_anomalies'])}")
    print(f"æ–¹æ³•ä¸€è‡´æ€§è¯„åˆ†: {optimized_result['consistency_score']:.3f}")

    return optimized_result

if __name__ == "__main__":
    demonstrate_constraint_analysis()