#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CRFæ•°æ®æŒ–æ˜æ™ºèƒ½åˆ†æAgent - å¤šç»´åº¦å¢å¼ºç‰ˆ
Enhanced Clinical Research Data Mining & Analysis Agent

æ–°å¢åŠŸèƒ½ç‰¹ç‚¹:
1. å¤šç»´åº¦ä¸´åºŠå‚æ•°æ”¯æŒ (100+ ç”Ÿç‰©æ ‡è®°ç‰©)
2. é«˜çº§ç»Ÿè®¡åˆ†ææ–¹æ³• (æœºå™¨å­¦ä¹ ã€ç½‘ç»œåˆ†æã€ä¸­ä»‹åˆ†æ)
3. ç²¾å‡†åŒ»å­¦å’Œä¸ªæ€§åŒ–æ²»ç–—åˆ†æ
4. ç¤¾ä¼šå†³å®šå› ç´ å¥åº·ç»“å±€åˆ†æ
5. å¤šä¸­å¿ƒç ”ç©¶è´¨é‡æ§åˆ¶
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union
import json
import warnings
from dataclasses import dataclass
from enum import Enum
import os
import re

# --- å¯é€‰ä¾èµ–åŒ… ---
try:
    import yaml
    HAS_YAML = True
except ImportError:
    HAS_YAML = False

try:
    import seaborn as sns
    import matplotlib.pyplot as plt
    HAS_PLOTTING = True
except ImportError:
    HAS_PLOTTING = False

try:
    from scipy import stats
    from scipy.cluster.hierarchy import dendrogram, linkage
    HAS_SCIPY = True
except ImportError:
    HAS_SCIPY = False

try:
    from sklearn.cluster import KMeans, AgglomerativeClustering
    from sklearn.preprocessing import StandardScaler
    from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import roc_auc_score, silhouette_score
    from sklearn.model_selection import cross_val_score
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False

try:
    import networkx as nx
    HAS_NETWORKX = True
except ImportError:
    HAS_NETWORKX = False

try:
    from lifelines import CoxPHFitter, KaplanMeierFitter
    HAS_SURVIVAL = True
except ImportError:
    HAS_SURVIVAL = False

warnings.filterwarnings('ignore')

# --- æ‰©å±•çš„æ•°æ®ç»“æ„å®šä¹‰ ---
class ResearchValue(Enum):
    """ç ”ç©¶ä»·å€¼ç­‰çº§"""
    BREAKTHROUGH = "çªç ´æ€§å‘ç°"      # æ–°å¢ï¼šçªç ´æ€§å‘ç°
    VERY_HIGH = "æé«˜ä»·å€¼"
    HIGH = "é«˜ä»·å€¼"
    MODERATE = "ä¸­ç­‰ä»·å€¼"
    LOW = "ä½ä»·å€¼"
    INSUFFICIENT = "æ•°æ®ä¸è¶³"

class PublicationOpportunity(Enum):
    """å‘è¡¨æœºä¼šç±»å‹"""
    NATURE_MEDICINE = "Nature Medicineçº§åˆ«"  # æ–°å¢
    ORIGINAL_RESEARCH = "åŸåˆ›ç ”ç©¶"
    LONGITUDINAL = "çºµå‘ç ”ç©¶"
    CROSS_SECTIONAL = "æ¨ªæ–­é¢ç ”ç©¶"
    OBSERVATIONAL_STUDY = "è§‚å¯Ÿæ€§ç ”ç©¶"
    META_ANALYSIS = "èŸèƒåˆ†æ"
    CASE_SERIES = "ç—…ä¾‹ç³»åˆ—"
    PRECISION_MEDICINE = "ç²¾å‡†åŒ»å­¦ç ”ç©¶"      # æ–°å¢
    SOCIAL_EPIDEMIOLOGY = "ç¤¾ä¼šæµè¡Œç—…å­¦"    # æ–°å¢

class AnalysisType(Enum):
    """åˆ†æç±»å‹æšä¸¾"""
    CHI2_CONTINGENCY = "chi2_contingency"
    GROUP_BY_RATE_DIFF = "group_by_rate_diff"
    LOGISTIC_REGRESSION = "logistic_regression"           # æ–°å¢
    SURVIVAL_ANALYSIS = "survival_analysis"               # æ–°å¢
    K_MEANS_CLUSTERING = "k_means_clustering"             # æ–°å¢
    RANDOM_FOREST_ANALYSIS = "random_forest_analysis"     # æ–°å¢
    CORRELATION_NETWORK = "correlation_network"           # æ–°å¢
    MEDIATION_ANALYSIS = "mediation_analysis"             # æ–°å¢
    MULTILEVEL_REGRESSION = "multilevel_regression"       # æ–°å¢
    RISK_STRATIFICATION = "risk_stratification"           # æ–°å¢

@dataclass
class EnhancedResearchInsight:
    """å¢å¼ºçš„ç ”ç©¶å‘ç°"""
    title: str
    description: str
    value_level: ResearchValue
    publication_type: PublicationOpportunity
    statistical_power: float
    sample_size: int
    key_variables: List[str]
    statistical_methods: List[str]
    expected_impact_factor: str
    implementation_difficulty: str
    recommendations: List[str]
    # æ–°å¢å­—æ®µ
    clinical_significance: str
    precision_medicine_potential: str
    health_economics_impact: str
    regulatory_considerations: List[str]
    international_collaboration_potential: str
    data_sharing_requirements: List[str]

@dataclass
class MultiDimensionalQualityMetrics:
    """å¤šç»´åº¦æ•°æ®è´¨é‡æŒ‡æ ‡"""
    total_records: int
    total_features: int
    missing_rate: float
    duplicate_rate: float
    outlier_rates: Dict[str, float]
    consistency_violations: List[str]
    clinical_implausible_values: List[str]
    temporal_consistency_issues: List[str]
    cross_validation_errors: List[str]
    overall_grade: str
    recommendations: List[str]

# --- ä¸»Agentç±»å¢å¼ºç‰ˆ ---
class EnhancedCRFResearchMiningAgent:
    """CRFæ•°æ®æŒ–æ˜æ™ºèƒ½åˆ†æä»£ç† - å¤šç»´åº¦å¢å¼ºç‰ˆ"""
    
    def __init__(self, config_path: str, data_storage_path: str = "./crf_data/"):
        """åˆå§‹åŒ–å¢å¼ºç‰ˆCRFåˆ†æä»£ç†"""
        self.config_path = config_path
        self.data_storage_path = data_storage_path
        self.data_cache = {}
        self.analysis_history = []
        self.research_insights = []
        self.quality_metrics = {}
        
        if not HAS_YAML:
            print("âŒ å…³é”®ä¾èµ–åŒ… PyYAML æœªå®‰è£…ï¼ŒAgentæ— æ³•å¯åŠ¨ã€‚è¯·è¿è¡Œ: pip install pyyaml")
            raise ImportError("PyYAML not found.")

        self.config = self._load_enhanced_config()
        self.data_sources_config = self.config.get('data_sources', {})
        self.quality_checks = self.config.get('quality_checks', {})
        self.analysis_config = self.config.get('analyses', [])
        self.publication_strategy = self.config.get('publication_strategy', {})
        self.multicenter_settings = self.config.get('multicenter_settings', {})

        os.makedirs(data_storage_path, exist_ok=True)
        print("âœ… å¢å¼ºç‰ˆCRFæ•°æ®æŒ–æ˜Agentåˆå§‹åŒ–æˆåŠŸ")
        print(f"ğŸ“Š æ”¯æŒåˆ†æç±»å‹: {len(AnalysisType.__members__)}ç§")
        print(f"ğŸ”¬ æ”¯æŒå‚æ•°ç»´åº¦: {len(self.quality_checks.get('clinical_ranges', {}))}ä¸ª")

    def _load_enhanced_config(self) -> Dict:
        """åŠ è½½å¢å¼ºé…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            print(f"âœ… æˆåŠŸåŠ è½½å¢å¼ºé…ç½®æ–‡ä»¶: {self.config_path}")
            return config
        except Exception as e:
            print(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return {}

    def load_enhanced_crf_data(self) -> Dict[str, pd.DataFrame]:
        """æ ¹æ®é…ç½®æ–‡ä»¶åŠ è½½å¤šç»´åº¦CRFæ•°æ®"""
        datasets = {}
        if not self.data_sources_config:
            print("âš ï¸ é…ç½®æ–‡ä»¶ä¸­æ— æ•°æ®æºä¿¡æ¯ï¼Œè·³è¿‡åŠ è½½ã€‚")
            return datasets

        for data_type, file_path in self.data_sources_config.items():
            try:
                if os.path.exists(file_path):
                    if file_path.endswith('.csv'):
                        datasets[data_type] = pd.read_csv(file_path)
                    elif file_path.endswith('.json'):
                        with open(file_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                        datasets[data_type] = pd.json_normalize(data)
                    print(f"âœ… æˆåŠŸåŠ è½½ {data_type}: {len(datasets[data_type])} æ¡è®°å½•")
                else:
                    print(f"âš ï¸ è­¦å‘Š: æ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨ '{file_path}'ï¼Œè·³è¿‡åŠ è½½ {data_type}")
            except Exception as e:
                print(f"âŒ åŠ è½½ {data_type} å¤±è´¥: {e}")
        
        self.data_cache = datasets
        return datasets

    # --- å¢å¼ºçš„æ•°æ®è´¨é‡è¯„ä¼° ---
    def assess_multidimensional_data_quality(self) -> Dict[str, MultiDimensionalQualityMetrics]:
        """å¤šç»´åº¦æ•°æ®è´¨é‡è¯„ä¼°"""
        quality_report = {}
        
        for dataset_name, df in self.data_cache.items():
            # åŸºç¡€è´¨é‡æŒ‡æ ‡
            total_records = len(df)
            total_features = len(df.columns)
            missing_rate = df.isnull().mean().mean()
            duplicate_rate = df.duplicated().sum() / len(df) if len(df) > 0 else 0
            
            # å¼‚å¸¸å€¼æ£€æµ‹
            outlier_rates = self._detect_multidimensional_outliers(df, dataset_name)
            
            # ä¸´åºŠåˆç†æ€§æ£€æŸ¥
            clinical_implausible = self._check_clinical_plausibility(df, dataset_name)
            
            # é€»è¾‘ä¸€è‡´æ€§æ£€æŸ¥
            consistency_violations = self._check_enhanced_consistency(df, dataset_name)
            
            # æ—¶é—´ä¸€è‡´æ€§æ£€æŸ¥
            temporal_issues = self._check_temporal_consistency(df, dataset_name)
            
            # äº¤å‰éªŒè¯é”™è¯¯
            cross_validation_errors = self._cross_validate_measurements(df, dataset_name)
            
            # ç»¼åˆè´¨é‡ç­‰çº§è¯„å®š
            overall_grade = self._calculate_overall_quality_grade(
                missing_rate, duplicate_rate, len(clinical_implausible), 
                len(consistency_violations), len(outlier_rates)
            )
            
            # æ”¹è¿›å»ºè®®
            recommendations = self._generate_quality_recommendations(
                missing_rate, duplicate_rate, clinical_implausible, 
                consistency_violations, outlier_rates
            )
            
            quality_report[dataset_name] = MultiDimensionalQualityMetrics(
                total_records=total_records,
                total_features=total_features,
                missing_rate=missing_rate,
                duplicate_rate=duplicate_rate,
                outlier_rates=outlier_rates,
                consistency_violations=consistency_violations,
                clinical_implausible_values=clinical_implausible,
                temporal_consistency_issues=temporal_issues,
                cross_validation_errors=cross_validation_errors,
                overall_grade=overall_grade,
                recommendations=recommendations
            )
        
        self.quality_metrics = quality_report
        return quality_report

    def _detect_multidimensional_outliers(self, df: pd.DataFrame, dataset_name: str) -> Dict[str, float]:
        """å¤šç»´åº¦å¼‚å¸¸å€¼æ£€æµ‹"""
        outlier_rates = {}
        clinical_ranges = self.quality_checks.get('clinical_ranges', {})
        
        for col in df.select_dtypes(include=[np.number]).columns:
            if col in clinical_ranges:
                range_config = clinical_ranges[col]
                min_val, max_val = range_config['min'], range_config['max']
                
                # èŒƒå›´å¼‚å¸¸å€¼
                range_outliers = df[(df[col] < min_val) | (df[col] > max_val)]
                
                # IQRå¼‚å¸¸å€¼
                Q1, Q3 = df[col].quantile(0.25), df[col].quantile(0.75)
                IQR = Q3 - Q1
                if IQR > 0:
                    iqr_outliers = df[(df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)]
                    outlier_rates[col] = len(iqr_outliers) / len(df)
                else:
                    outlier_rates[col] = 0
                    
        return outlier_rates

    def _check_clinical_plausibility(self, df: pd.DataFrame, dataset_name: str) -> List[str]:
        """ä¸´åºŠåˆç†æ€§æ£€æŸ¥"""
        implausible_values = []
        clinical_ranges = self.quality_checks.get('clinical_ranges', {})
        
        for col, range_config in clinical_ranges.items():
            if col in df.columns:
                min_val, max_val = range_config['min'], range_config['max']
                implausible = df[(df[col] < min_val) | (df[col] > max_val)]
                
                if not implausible.empty:
                    implausible_values.append(
                        f"{col}: {len(implausible)}æ¡è®°å½•è¶…å‡ºä¸´åºŠåˆç†èŒƒå›´ ({min_val}-{max_val})"
                    )
        
        return implausible_values

    def _check_enhanced_consistency(self, df: pd.DataFrame, dataset_name: str) -> List[str]:
        """å¢å¼ºçš„é€»è¾‘ä¸€è‡´æ€§æ£€æŸ¥"""
        violations = []
        consistency_rules = self.quality_checks.get('logical_consistency', [])
        
        for rule in consistency_rules:
            try:
                rule_expr = rule['rule']
                description = rule['description']
                tolerance = rule.get('tolerance', 0)
                
                # è¯„ä¼°è§„åˆ™è¡¨è¾¾å¼
                if 'bmi' in rule_expr and all(col in df.columns for col in ['weight_kg', 'height_cm', 'bmi']):
                    # BMIä¸€è‡´æ€§æ£€æŸ¥
                    calculated_bmi = df['weight_kg'] / (df['height_cm']/100) ** 2
                    bmi_diff = abs(df['bmi'] - calculated_bmi)
                    inconsistent = df[bmi_diff > tolerance]
                    
                    if not inconsistent.empty:
                        violations.append(f"BMIè®¡ç®—ä¸ä¸€è‡´: {len(inconsistent)}æ¡è®°å½•")
                        
                else:
                    # é€šç”¨è§„åˆ™æ£€æŸ¥
                    try:
                        violated = df.query(f"not ({rule_expr})")
                        if not violated.empty:
                            violations.append(f"{description}: {len(violated)}æ¡è®°å½•")
                    except Exception:
                        continue
                        
            except Exception as e:
                violations.append(f"è§„åˆ™æ£€æŸ¥é”™è¯¯: {rule.get('description', 'Unknown rule')}")
                
        return violations

    def _check_temporal_consistency(self, df: pd.DataFrame, dataset_name: str) -> List[str]:
        """æ—¶é—´ä¸€è‡´æ€§æ£€æŸ¥"""
        temporal_issues = []
        
        # æ£€æŸ¥æ—¥æœŸåˆ—
        date_columns = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]
        
        for col in date_columns:
            try:
                # è½¬æ¢ä¸ºæ—¥æœŸç±»å‹
                df[col] = pd.to_datetime(df[col], errors='coerce')
                
                # æ£€æŸ¥æœªæ¥æ—¥æœŸ
                future_dates = df[df[col] > pd.Timestamp.now()]
                if not future_dates.empty:
                    temporal_issues.append(f"{col}: {len(future_dates)}æ¡è®°å½•ä¸ºæœªæ¥æ—¥æœŸ")
                    
                # æ£€æŸ¥è¿‡æ—©æ—¥æœŸ (1900å¹´ä¹‹å‰)
                too_early = df[df[col] < pd.Timestamp('1900-01-01')]
                if not too_early.empty:
                    temporal_issues.append(f"{col}: {len(too_early)}æ¡è®°å½•æ—¥æœŸè¿‡æ—©")
                    
            except Exception:
                continue
                
        return temporal_issues

    def _cross_validate_measurements(self, df: pd.DataFrame, dataset_name: str) -> List[str]:
        """äº¤å‰éªŒè¯æµ‹é‡å€¼"""
        validation_errors = []
        
        # è¡€å‹ä¸€è‡´æ€§
        if all(col in df.columns for col in ['systolic_bp', 'diastolic_bp']):
            invalid_bp = df[df['systolic_bp'] <= df['diastolic_bp']]
            if not invalid_bp.empty:
                validation_errors.append(f"è¡€å‹å€¼ä¸åˆç†: {len(invalid_bp)}æ¡è®°å½•æ”¶ç¼©å‹â‰¤èˆ’å¼ å‹")
        
        # BMIä¸èº«é«˜ä½“é‡ä¸€è‡´æ€§
        if all(col in df.columns for col in ['bmi', 'weight_kg', 'height_cm']):
            calculated_bmi = df['weight_kg'] / (df['height_cm']/100) ** 2
            bmi_discrepancy = abs(df['bmi'] - calculated_bmi) > 1.0  # å…è®¸1.0çš„è¯¯å·®
            if bmi_discrepancy.any():
                validation_errors.append(f"BMIè®¡ç®—ä¸ä¸€è‡´: {bmi_discrepancy.sum()}æ¡è®°å½•")
        
        # èƒ†å›ºé†‡æˆåˆ†ä¸€è‡´æ€§
        if all(col in df.columns for col in ['total_cholesterol', 'ldl_cholesterol', 'hdl_cholesterol']):
            ldl_hdl_sum = df['ldl_cholesterol'] + df['hdl_cholesterol']
            implausible_lipid = df[ldl_hdl_sum > df['total_cholesterol'] + 1.0]  # å…è®¸1.0çš„è¯¯å·®
            if not implausible_lipid.empty:
                validation_errors.append(f"è¡€è„‚æˆåˆ†ä¸åˆç†: {len(implausible_lipid)}æ¡è®°å½•")
        
        return validation_errors

    def _calculate_overall_quality_grade(self, missing_rate: float, duplicate_rate: float, 
                                       clinical_issues: int, consistency_issues: int, 
                                       outlier_count: int) -> str:
        """è®¡ç®—ç»¼åˆè´¨é‡ç­‰çº§"""
        score = 100
        
        # ç¼ºå¤±ç‡æ‰£åˆ†
        if missing_rate > 0.20:
            score -= 30
        elif missing_rate > 0.10:
            score -= 15
        elif missing_rate > 0.05:
            score -= 5
            
        # é‡å¤ç‡æ‰£åˆ†  
        if duplicate_rate > 0.05:
            score -= 20
        elif duplicate_rate > 0.02:
            score -= 10
            
        # ä¸´åºŠé—®é¢˜æ‰£åˆ†
        score -= min(clinical_issues * 5, 25)
        
        # ä¸€è‡´æ€§é—®é¢˜æ‰£åˆ†
        score -= min(consistency_issues * 5, 25)
        
        # å¼‚å¸¸å€¼æ‰£åˆ†
        score -= min(outlier_count * 2, 15)
        
        if score >= 90:
            return "ä¼˜ç§€"
        elif score >= 80:
            return "è‰¯å¥½"
        elif score >= 70:
            return "å¯æ¥å—"
        elif score >= 60:
            return "ä¸€èˆ¬"
        else:
            return "å·®"

    def _generate_quality_recommendations(self, missing_rate: float, duplicate_rate: float,
                                        clinical_issues: List[str], consistency_violations: List[str],
                                        outlier_rates: Dict[str, float]) -> List[str]:
        """ç”Ÿæˆæ•°æ®è´¨é‡æ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if missing_rate > 0.10:
            recommendations.append(f"ç¼ºå¤±ç‡({missing_rate:.1%})è¾ƒé«˜ï¼Œå»ºè®®åŠ å¼ºæ•°æ®æ”¶é›†å®Œæ•´æ€§")
        
        if duplicate_rate > 0.02:
            recommendations.append(f"é‡å¤è®°å½•ç‡({duplicate_rate:.1%})åé«˜ï¼Œå»ºè®®æ¸…ç†é‡å¤æ•°æ®")
            
        if clinical_issues:
            recommendations.append("å­˜åœ¨ä¸´åºŠä¸åˆç†å€¼ï¼Œå»ºè®®æ ¸å®æ•°æ®å½•å…¥å‡†ç¡®æ€§")
            
        if consistency_violations:
            recommendations.append("å­˜åœ¨é€»è¾‘ä¸ä¸€è‡´é—®é¢˜ï¼Œå»ºè®®å»ºç«‹æ•°æ®éªŒè¯è§„åˆ™")
            
        high_outlier_fields = [field for field, rate in outlier_rates.items() if rate > 0.05]
        if high_outlier_fields:
            recommendations.append(f"ä»¥ä¸‹å­—æ®µå¼‚å¸¸å€¼è¾ƒå¤š: {', '.join(high_outlier_fields[:3])}")
            
        if not recommendations:
            recommendations.append("æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå¯è¿›è¡Œåç»­åˆ†æ")
            
        return recommendations

    # --- å¢å¼ºçš„ç ”ç©¶æœºä¼šå‘ç° ---
    def discover_enhanced_research_opportunities(self) -> List[EnhancedResearchInsight]:
        """å¢å¼ºç‰ˆç ”ç©¶æœºä¼šå‘ç°"""
        self.research_insights = []
        if not self.analysis_config:
            return []

        for i, analysis_def in enumerate(self.analysis_config, 1):
            try:
                print(f"\nğŸš€ [åˆ†æ {i}/{len(self.analysis_config)}] {analysis_def.get('title')}")
                
                # å‡†å¤‡åˆ†ææ•°æ®
                df = self._prepare_enhanced_analysis_data(analysis_def.get('datasets', []))
                if df is None or df.empty:
                    print(f"âš ï¸ æ•°æ®å‡†å¤‡å¤±è´¥æˆ–ä¸ºç©ºï¼Œè·³è¿‡åˆ†æ: {analysis_def.get('title')}")
                    continue

                # åˆ›å»ºåˆ†æå˜é‡
                df = self._create_enhanced_analysis_variables(df, analysis_def.get('variables', {}))
                
                # æ‰§è¡Œåˆ†æ
                analysis_type = analysis_def['analysis']['type']
                analysis_func = self._get_enhanced_analysis_function(analysis_type)
                analysis_results = analysis_func(df, analysis_def['analysis'])
                analysis_results['sample_size'] = len(df)

                # æ£€æŸ¥è§¦å‘æ¡ä»¶
                if self._check_enhanced_trigger(analysis_results, analysis_def.get('trigger', {})):
                    print(f"âœ… å‘ç°æ½œåœ¨ç ”ç©¶æœºä¼š: {analysis_def.get('title')}")
                    insight = self._generate_enhanced_insight(analysis_def, analysis_results)
                    self.research_insights.append(insight)
                else:
                    print(f"â„¹ï¸ åˆ†æå®Œæˆï¼Œæœªè¾¾åˆ°è§¦å‘æ¡ä»¶ã€‚")
                    
            except Exception as e:
                print(f"âŒ åˆ†æ '{analysis_def.get('id', 'N/A')}' æ‰§è¡Œå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()

        return self.research_insights

    def _prepare_enhanced_analysis_data(self, dataset_defs: List[Dict]) -> Optional[pd.DataFrame]:
        """å‡†å¤‡å¢å¼ºåˆ†ææ•°æ®"""
        if not dataset_defs:
            return None
            
        # è·å–ä¸»æ•°æ®é›†
        primary_dataset = dataset_defs[0]
        if isinstance(primary_dataset, str):
            primary_dataset = {'source': primary_dataset}
            
        if primary_dataset['source'] not in self.data_cache:
            return None
            
        df = self.data_cache[primary_dataset['source']].copy()
        
        # å¤„ç†å¤šæ•°æ®é›†åˆå¹¶
        if len(dataset_defs) > 1:
            merge_key = primary_dataset.get('merge_key', 'patient_id')
            
            for dataset_def in dataset_defs[1:]:
                if isinstance(dataset_def, str):
                    dataset_def = {'source': dataset_def}
                    
                source_name = dataset_def['source']
                if source_name not in self.data_cache:
                    print(f"âš ï¸ æ•°æ®æº {source_name} æœªæ‰¾åˆ°ï¼Œæ— æ³•åˆå¹¶")
                    continue
                    
                right_df = self.data_cache[source_name]
                how = dataset_def.get('how', 'inner')
                
                # æ£€æŸ¥åˆå¹¶é”®
                if merge_key not in df.columns or merge_key not in right_df.columns:
                    # å°è¯•å¤‡ç”¨é”®
                    fallback_keys = ['subject_id', 'id']
                    found_key = None
                    for key in fallback_keys:
                        if key in df.columns and key in right_df.columns:
                            found_key = key
                            break
                    if found_key:
                        merge_key = found_key
                    else:
                        print(f"âš ï¸ æ— æ³•æ‰¾åˆ°åˆé€‚çš„åˆå¹¶é”®ï¼Œè·³è¿‡ {source_name}")
                        continue
                
                df = pd.merge(df, right_df, on=merge_key, how=how, suffixes=('', f'_{source_name}'))
        
        return df

    def _create_enhanced_analysis_variables(self, df: pd.DataFrame, variables: Dict) -> pd.DataFrame:
        """åˆ›å»ºå¢å¼ºåˆ†æå˜é‡"""
        for var_name, var_def in variables.items():
            try:
                var_type = var_def['type']
                
                if var_type == 'cut':
                    source_col = var_def['source_column']
                    df[var_name] = pd.cut(df[source_col], **var_def['params'])
                    
                elif var_type == 'expression':
                    df[var_name] = df.eval(var_def['expression'])
                    
                elif var_type == 'composite_score':
                    # å¤åˆè¯„åˆ†
                    components = var_def['components']
                    score = pd.Series(0, index=df.index)
                    
                    for component, weight in components.items():
                        if component in df.columns:
                            # æ ‡å‡†åŒ–ååŠ æƒ
                            normalized = (df[component] - df[component].mean()) / df[component].std()
                            score += normalized * weight
                            
                    df[var_name] = score
                    
                elif var_type == 'composite_z_score':
                    # æ ‡å‡†åŒ–å¤åˆè¯„åˆ†
                    components = var_def['components']
                    z_scores = []
                    
                    for component in components:
                        if component in df.columns:
                            z_score = (df[component] - df[component].mean()) / df[component].std()
                            z_scores.append(z_score)
                    
                    if z_scores:
                        df[var_name] = pd.concat(z_scores, axis=1).mean(axis=1)
                        
            except Exception as e:
                print(f"âš ï¸ åˆ›å»ºå˜é‡ '{var_name}' å¤±è´¥: {e}")
                
        return df

    def _get_enhanced_analysis_function(self, analysis_type: str) -> callable:
        """è·å–å¢å¼ºåˆ†æå‡½æ•°"""
        analysis_map = {
            'chi2_contingency': self._run_chi2_contingency,
            'group_by_rate_diff': self._run_group_by_rate_diff,
            'logistic_regression': self._run_logistic_regression,
            'survival_analysis': self._run_survival_analysis,
            'k_means_clustering': self._run_k_means_clustering,
            'random_forest_analysis': self._run_random_forest_analysis,
            'correlation_network': self._run_correlation_network,
            'mediation_analysis': self._run_mediation_analysis,
            'multilevel_regression': self._run_multilevel_regression,
            'risk_stratification': self._run_risk_stratification,
            'prevalence': self._run_prevalence_analysis,
        }
        
        if analysis_type not in analysis_map:
            raise ValueError(f"æœªçŸ¥çš„åˆ†æç±»å‹: {analysis_type}")
            
        return analysis_map[analysis_type]

    # --- æ–°å¢åˆ†ææ–¹æ³• ---
    def _run_logistic_regression(self, df: pd.DataFrame, analysis_config: Dict) -> Dict:
        """é€»è¾‘å›å½’åˆ†æ"""
        if not HAS_SKLEARN:
            return {'auc': 0, 'error': 'sklearn not available'}
            
        try:
            dependent = analysis_config['dependent']
            independent = analysis_config['independent']
            
            # å‡†å¤‡æ•°æ®
            X = df[independent].fillna(0)
            y = df[dependent].fillna(0)
            
            # æ‹Ÿåˆæ¨¡å‹
            model = LogisticRegression(random_state=42)
            model.fit(X, y)
            
            # é¢„æµ‹å’Œè¯„ä¼°
            y_pred_proba = model.predict_proba(X)[:, 1]
            auc = roc_auc_score(y, y_pred_proba)
            
            # ç‰¹å¾é‡è¦æ€§ï¼ˆç³»æ•°ï¼‰
            feature_importance = dict(zip(independent, abs(model.coef_[0])))
            
            return {
                'model_auc': auc,
                'feature_importance': feature_importance,
                'coefficients': dict(zip(independent, model.coef_[0])),
                'sample_size': len(df)
            }
            
        except Exception as e:
            return {'auc': 0, 'error': str(e)}

    def _run_survival_analysis(self, df: pd.DataFrame, analysis_config: Dict) -> Dict:
        """ç”Ÿå­˜åˆ†æ"""
        if not HAS_SURVIVAL:
            return {'hazard_ratio': 1.0, 'error': 'lifelines not available'}
            
        try:
            duration_col = analysis_config['time_to_event']
            event_col = analysis_config['event']
            
            # Kaplan-Meierä¼°è®¡
            kmf = KaplanMeierFitter()
            kmf.fit(df[duration_col], df[event_col])
            
            # Coxæ¯”ä¾‹é£é™©æ¨¡å‹ï¼ˆå¦‚æœæœ‰åå˜é‡ï¼‰
            if 'covariates' in analysis_config:
                cph = CoxPHFitter()
                covariates = analysis_config['covariates']
                data = df[[duration_col, event_col] + covariates].dropna()
                cph.fit(data, duration_col=duration_col, event_col=event_col)
                
                hazard_ratios = cph.hazard_ratios_.to_dict()
                p_values = cph.summary['p'].to_dict()
                
                return {
                    'hazard_ratios': hazard_ratios,
                    'p_values': p_values,
                    'median_survival': kmf.median_survival_time_,
                    'sample_size': len(data)
                }
            else:
                return {
                    'median_survival': kmf.median_survival_time_,
                    'sample_size': len(df)
                }
                
        except Exception as e:
            return {'hazard_ratio': 1.0, 'error': str(e)}

    def _run_k_means_clustering(self, df: pd.DataFrame, analysis_config: Dict) -> Dict:
        """Kå‡å€¼èšç±»åˆ†æ"""
        if not HAS_SKLEARN:
            return {'silhouette_score': 0, 'error': 'sklearn not available'}
            
        try:
            features = analysis_config['features']
            n_clusters = analysis_config.get('n_clusters', 3)
            
            # å‡†å¤‡æ•°æ®
            X = df[features].fillna(0)
            
            # æ ‡å‡†åŒ–
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            # Kå‡å€¼èšç±»
            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            cluster_labels = kmeans.fit_predict(X_scaled)
            
            # è¯„ä¼°èšç±»è´¨é‡
            silhouette_avg = silhouette_score(X_scaled, cluster_labels)
            
            # èšç±»ä¸­å¿ƒ
            cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)
            
            return {
                'silhouette_score': silhouette_avg,
                'cluster_labels': cluster_labels.tolist(),
                'cluster_centers': cluster_centers.tolist(),
                'inertia': kmeans.inertia_,
                'sample_size': len(X)
            }
            
        except Exception as e:
            return {'silhouette_score': 0, 'error': str(e)}

    def _run_random_forest_analysis(self, df: pd.DataFrame, analysis_config: Dict) -> Dict:
        """éšæœºæ£®æ—åˆ†æ"""
        if not HAS_SKLEARN:
            return {'importance_score': 0, 'error': 'sklearn not available'}
            
        try:
            target = analysis_config['target']
            predictors = analysis_config['predictors']
            
            # å‡†å¤‡æ•°æ®
            X = df[predictors].fillna(0)
            y = df[target].fillna(0)
            
            # åˆ¤æ–­æ˜¯åˆ†ç±»è¿˜æ˜¯å›å½’
            if y.nunique() <= 10 and y.dtype in ['object', 'bool', 'category']:
                model = RandomForestClassifier(n_estimators=100, random_state=42)
            else:
                model = RandomForestRegressor(n_estimators=100, random_state=42)
            
            # æ‹Ÿåˆæ¨¡å‹
            model.fit(X, y)
            
            # ç‰¹å¾é‡è¦æ€§
            feature_importance = dict(zip(predictors, model.feature_importances_))
            
            # äº¤å‰éªŒè¯è¯„åˆ†
            cv_scores = cross_val_score(model, X, y, cv=5)
            
            return {
                'feature_importance': feature_importance,
                'cv_score_mean': cv_scores.mean(),
                'cv_score_std': cv_scores.std(),
                'oob_score': getattr(model, 'oob_score_', None),
                'sample_size': len(X)
            }
            
        except Exception as e:
            return {'importance_score': 0, 'error': str(e)}

    def _run_correlation_network(self, df: pd.DataFrame, analysis_config: Dict) -> Dict:
        """ç›¸å…³æ€§ç½‘ç»œåˆ†æ"""
        if not HAS_NETWORKX or not HAS_SCIPY:
            return {'network_density': 0, 'error': 'networkx or scipy not available'}
            
        try:
            variables = analysis_config.get('variables', df.select_dtypes(include=[np.number]).columns.tolist())
            correlation_threshold = analysis_config.get('correlation_threshold', 0.3)
            
            # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
            corr_matrix = df[variables].corr()
            
            # åˆ›å»ºç½‘ç»œå›¾
            G = nx.Graph()
            
            # æ·»åŠ èŠ‚ç‚¹
            G.add_nodes_from(variables)
            
            # æ·»åŠ è¾¹ï¼ˆç›¸å…³æ€§è¶…è¿‡é˜ˆå€¼ï¼‰
            for i, var1 in enumerate(variables):
                for j, var2 in enumerate(variables):
                    if i < j:  # é¿å…é‡å¤
                        corr_val = abs(corr_matrix.loc[var1, var2])
                        if corr_val > correlation_threshold:
                            G.add_edge(var1, var2, weight=corr_val)
            
            # ç½‘ç»œæŒ‡æ ‡
            network_metrics = {
                'network_density': nx.density(G),
                'average_clustering': nx.average_clustering(G),
                'number_of_nodes': G.number_of_nodes(),
                'number_of_edges': G.number_of_edges()
            }
            
            # ä¸­å¿ƒæ€§æŒ‡æ ‡
            centrality = nx.degree_centrality(G)
            
            return {
                **network_metrics,
                'centrality': centrality,
                'correlation_matrix': corr_matrix.to_dict(),
                'sample_size': len(df)
            }
            
        except Exception as e:
            return {'network_density': 0, 'error': str(e)}

    def _run_mediation_analysis(self, df: pd.DataFrame, analysis_config: Dict) -> Dict:
        """ä¸­ä»‹åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        if not HAS_SKLEARN or not HAS_SCIPY:
            return {'mediation_effect': 0, 'error': 'sklearn or scipy not available'}
            
        try:
            independent = analysis_config['independent']
            mediator = analysis_config['mediator']
            dependent = analysis_config['dependent']
            
            # å‡†å¤‡æ•°æ®
            X = df[independent].fillna(0)
            M = df[mediator].fillna(0)
            Y = df[dependent].fillna(0)
            
            # è·¯å¾„åˆ†æ
            # Path a: X -> M
            from sklearn.linear_model import LinearRegression
            model_a = LinearRegression()
            model_a.fit(X.values.reshape(-1, 1), M)
            a_coef = model_a.coef_[0]
            
            # Path b: M -> Y (controlling for X)
            model_b = LinearRegression()
            XM = np.column_stack([X, M])
            model_b.fit(XM, Y)
            b_coef = model_b.coef_[1]  # Mçš„ç³»æ•°
            
            # Path c: X -> Y (total effect)
            model_c = LinearRegression()
            model_c.fit(X.values.reshape(-1, 1), Y)
            c_coef = model_c.coef_[0]
            
            # Path c': X -> Y (controlling for M, direct effect)
            c_prime_coef = model_b.coef_[0]  # Xçš„ç³»æ•°
            
            # ä¸­ä»‹æ•ˆåº”
            mediation_effect = a_coef * b_coef
            mediation_proportion = mediation_effect / c_coef if c_coef != 0 else 0
            
            return {
                'mediation_effect': mediation_effect,
                'mediation_proportion': mediation_proportion,
                'direct_effect': c_prime_coef,
                'total_effect': c_coef,
                'a_path': a_coef,
                'b_path': b_coef,
                'sample_size': len(df)
            }
            
        except Exception as e:
            return {'mediation_effect': 0, 'error': str(e)}

    def _run_multilevel_regression(self, df: pd.DataFrame, analysis_config: Dict) -> Dict:
        """å¤šå±‚å›å½’åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # ç®€åŒ–å®ç°ï¼Œå®é™…éœ€è¦ä¸“é—¨çš„å¤šå±‚å»ºæ¨¡åŒ…å¦‚statsmodels
        if not HAS_SKLEARN:
            return {'variance_explained_community': 0, 'error': 'sklearn not available'}
            
        try:
            dependent = analysis_config['dependent']
            independent = analysis_config['independent']
            group_var = analysis_config.get('group_variable', 'community_id')
            
            # å¦‚æœæ²¡æœ‰åˆ†ç»„å˜é‡ï¼Œé™çº§ä¸ºæ™®é€šå›å½’
            if group_var not in df.columns:
                model = LinearRegression()
                X = df[independent].fillna(0)
                y = df[dependent].fillna(0)
                model.fit(X, y)
                
                return {
                    'r_squared': model.score(X, y),
                    'coefficients': dict(zip(independent, model.coef_)),
                    'variance_explained_community': 0,
                    'sample_size': len(df)
                }
            
            # ç®€åŒ–çš„ç»„é—´å·®å¼‚åˆ†æ
            group_means = df.groupby(group_var)[dependent].mean()
            overall_mean = df[dependent].mean()
            
            # ç»„é—´æ–¹å·®å æ¯”ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
            between_group_var = ((group_means - overall_mean) ** 2).mean()
            total_var = df[dependent].var()
            variance_explained_community = between_group_var / total_var if total_var > 0 else 0
            
            return {
                'variance_explained_community': variance_explained_community,
                'group_means': group_means.to_dict(),
                'sample_size': len(df)
            }
            
        except Exception as e:
            return {'variance_explained_community': 0, 'error': str(e)}

    def _run_risk_stratification(self, df: pd.DataFrame, analysis_config: Dict) -> Dict:
        """é£é™©åˆ†å±‚åˆ†æ"""
        if not HAS_SKLEARN:
            return {'risk_auc': 0, 'error': 'sklearn not available'}
            
        try:
            risk_factors = analysis_config.get('risk_factors', [])
            outcome = analysis_config.get('outcome')
            
            # åˆ›å»ºé£é™©è¯„åˆ†
            X = df[risk_factors].fillna(0)
            y = df[outcome].fillna(0)
            
            # ä½¿ç”¨é€»è¾‘å›å½’åˆ›å»ºé£é™©æ¨¡å‹
            model = LogisticRegression(random_state=42)
            model.fit(X, y)
            
            # è®¡ç®—é£é™©æ¦‚ç‡
            risk_probabilities = model.predict_proba(X)[:, 1]
            
            # é£é™©åˆ†å±‚
            risk_quantiles = pd.qcut(risk_probabilities, q=4, labels=['ä½é£é™©', 'ä¸­ä½é£é™©', 'ä¸­é«˜é£é™©', 'é«˜é£é™©'])
            
            # å„é£é™©å±‚çš„ç»“å±€ç‡
            risk_strata_outcomes = df.groupby(risk_quantiles)[outcome].mean()
            
            # AUC
            auc = roc_auc_score(y, risk_probabilities)
            
            return {
                'risk_auc': auc,
                'risk_strata_outcomes': risk_strata_outcomes.to_dict(),
                'feature_weights': dict(zip(risk_factors, model.coef_[0])),
                'sample_size': len(df)
            }
            
        except Exception as e:
            return {'risk_auc': 0, 'error': str(e)}

    def _run_prevalence_analysis(self, df: pd.DataFrame, analysis_config: Dict) -> Dict:
        """æ‚£ç—…ç‡åˆ†æ"""
        try:
            condition_col = analysis_config.get('condition_column')
            threshold = analysis_config.get('threshold', 0)
            operator = analysis_config.get('operator', '>=')
            
            # æ ¹æ®æ“ä½œç¬¦è®¡ç®—æ‚£ç—…ç‡
            if operator == '>=':
                condition = df[condition_col] >= threshold
            elif operator == '>':
                condition = df[condition_col] > threshold
            elif operator == '<=':
                condition = df[condition_col] <= threshold
            elif operator == '<':
                condition = df[condition_col] < threshold
            else:
                condition = df[condition_col] >= threshold
                
            prevalence_rate = condition.mean()
            
            # æŒ‰åˆ†ç»„è®¡ç®—æ‚£ç—…ç‡
            if 'stratify_by' in analysis_config:
                stratify_col = analysis_config['stratify_by']
                if stratify_col in df.columns:
                    stratified_prevalence = df.groupby(stratify_col).apply(
                        lambda x: (x[condition_col] >= threshold).mean()
                    ).to_dict()
                else:
                    stratified_prevalence = {}
            else:
                stratified_prevalence = {}
            
            return {
                'prevalence_rate': prevalence_rate,
                'stratified_prevalence': stratified_prevalence,
                'affected_count': condition.sum(),
                'sample_size': len(df)
            }
            
        except Exception as e:
            return {'prevalence_rate': 0, 'error': str(e)}

    # --- ä¿æŒåŸæœ‰æ–¹æ³• ---
    def _run_chi2_contingency(self, df: pd.DataFrame, analysis_config: Dict) -> Dict:
        """å¡æ–¹æ£€éªŒåˆ†æ"""
        if not HAS_SCIPY:
            return {'p_value': 1.0, 'chi2': 0}
            
        try:
            params = analysis_config.get('params', analysis_config)
            ct = pd.crosstab(df[params['index']], df[params['columns']])
            if ct.empty or ct.shape[0] < 2 or ct.shape[1] < 2:
                return {'p_value': 1.0, 'chi2': 0, 'error': 'Contingency table is too small'}
            chi2, p, _, _ = stats.chi2_contingency(ct)
            return {'chi2': chi2, 'p_value': p}
        except Exception as e:
            return {'p_value': 1.0, 'chi2': 0, 'error': str(e)}

    def _run_group_by_rate_diff(self, df: pd.DataFrame, analysis_config: Dict) -> Dict:
        """ç»„é—´ç‡å·®åˆ†æ"""
        try:
            params = analysis_config.get('params', analysis_config)
            group_by_col = params['group_by_col']
            rate_col = params['rate_col']
            
            if group_by_col not in df.columns or rate_col not in df.columns:
                return {'rate_difference': 0, 'error': 'Column not found'}
            
            # ç¡®ä¿rate_colæ˜¯äºŒè¿›åˆ¶çš„
            if df[rate_col].nunique() <= 2:
                df[rate_col] = df[rate_col].astype(bool)
            else:
                return {'rate_difference': 0, 'error': 'Rate column is not binary'}
            
            rates = df.groupby(group_by_col)[rate_col].mean()
            
            if len(rates) != 2:
                return {'rate_difference': 0, 'error': f'Expected 2 groups, found {len(rates)}'}
            
            rate1, rate2 = rates.values
            group1, group2 = rates.index
            
            return {
                'rate_difference': abs(rate1 - rate2),
                f'{group1}_rate': rate1,
                f'{group2}_rate': rate2,
                'sample_size': len(df)
            }
            
        except Exception as e:
            return {'rate_difference': 0, 'error': str(e)}

    def _check_enhanced_trigger(self, results: Dict, trigger_def: Dict) -> bool:
        """å¢å¼ºç‰ˆè§¦å‘æ¡ä»¶æ£€æŸ¥"""
        if not trigger_def:
            return True
            
        metric = trigger_def.get('metric')
        operator = trigger_def.get('operator')
        value = trigger_def.get('value')
        
        if not all([metric, operator, value is not None]):
            return False
            
        result_val = results.get(metric)
        if result_val is None:
            return False
            
        op_map = {
            'lt': lambda a, b: a < b,
            'gt': lambda a, b: a > b,
            'lte': lambda a, b: a <= b,
            'gte': lambda a, b: a >= b,
            'eq': lambda a, b: abs(a - b) < 1e-6,
            'ne': lambda a, b: abs(a - b) >= 1e-6
        }
        
        if operator not in op_map:
            return False
            
        return op_map[operator](result_val, value)

    def _generate_enhanced_insight(self, analysis_def: Dict, results: Dict) -> EnhancedResearchInsight:
        """ç”Ÿæˆå¢å¼ºç ”ç©¶æ´å¯Ÿ"""
        insight_template = analysis_def.get('insight_template', {})
        
        # åŸºç¡€ä¿¡æ¯
        title = analysis_def.get('title', 'Untitled Research')
        description = analysis_def.get('description_template', '').format(**results)
        
        # ç ”ç©¶ä»·å€¼å’Œå‘è¡¨ç±»å‹
        value_level = ResearchValue[insight_template.get('value_level', 'MODERATE')]
        pub_type = PublicationOpportunity[insight_template.get('publication_type', 'CROSS_SECTIONAL')]
        
        # ç»Ÿè®¡åŠŸæ•ˆè¯„ä¼°
        sample_size = results.get('sample_size', 0)
        statistical_power = self._estimate_statistical_power(results, sample_size)
        
        # ç”Ÿæˆæ¨èå»ºè®®
        recommendations = insight_template.get('recommendations', [])
        recommendations.extend(self._generate_advanced_recommendations(analysis_def, results))
        
        return EnhancedResearchInsight(
            title=title,
            description=description,
            value_level=value_level,
            publication_type=pub_type,
            statistical_power=statistical_power,
            sample_size=sample_size,
            key_variables=insight_template.get('key_variables', []),
            statistical_methods=insight_template.get('statistical_methods', []),
            expected_impact_factor=insight_template.get('expected_impact_factor', 'N/A'),
            implementation_difficulty=insight_template.get('implementation_difficulty', 'ä¸­ç­‰'),
            recommendations=recommendations,
            clinical_significance=self._assess_clinical_significance(results),
            precision_medicine_potential=self._assess_precision_medicine_potential(analysis_def, results),
            health_economics_impact=self._assess_health_economics_impact(results),
            regulatory_considerations=self._identify_regulatory_considerations(analysis_def),
            international_collaboration_potential=self._assess_collaboration_potential(analysis_def, results),
            data_sharing_requirements=self._identify_data_sharing_requirements(analysis_def)
        )

    def _estimate_statistical_power(self, results: Dict, sample_size: int) -> float:
        """ä¼°ç®—ç»Ÿè®¡åŠŸæ•ˆ"""
        if 'p_value' in results and results['p_value'] < 0.05:
            if sample_size >= 500:
                return 0.95
            elif sample_size >= 300:
                return 0.85
            elif sample_size >= 100:
                return 0.75
            else:
                return 0.60
        else:
            return 0.50

    def _generate_advanced_recommendations(self, analysis_def: Dict, results: Dict) -> List[str]:
        """ç”Ÿæˆé«˜çº§æ¨èå»ºè®®"""
        recommendations = []
        analysis_type = analysis_def.get('analysis', {}).get('type', '')
        
        if 'machine_learning' in analysis_type or 'random_forest' in analysis_type:
            recommendations.append("è€ƒè™‘ä½¿ç”¨å¤–éƒ¨æ•°æ®é›†è¿›è¡Œæ¨¡å‹éªŒè¯")
            recommendations.append("æ¢ç´¢ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–æ¨¡å‹æ€§èƒ½")
            
        if 'network' in analysis_type:
            recommendations.append("è¿›ä¸€æ­¥åˆ†æå…³é”®èŠ‚ç‚¹çš„ç”Ÿç‰©å­¦æ„ä¹‰")
            recommendations.append("è€ƒè™‘åŠ¨æ€ç½‘ç»œåˆ†ææ•è·æ—¶é—´å˜åŒ–")
            
        if results.get('sample_size', 0) > 1000:
            recommendations.append("æ ·æœ¬é‡å……è¶³ï¼Œå¯è€ƒè™‘å¤šä¸­å¿ƒéªŒè¯")
            
        return recommendations

    def _assess_clinical_significance(self, results: Dict) -> str:
        """è¯„ä¼°ä¸´åºŠæ„ä¹‰"""
        if 'auc' in results and results['auc'] > 0.80:
            return "é¢„æµ‹æ¨¡å‹å…·æœ‰è‰¯å¥½çš„ä¸´åºŠåº”ç”¨ä»·å€¼"
        elif 'hazard_ratio' in results and results['hazard_ratio'] > 2.0:
            return "é£é™©æ¯”æç¤ºå¼ºçƒˆçš„ä¸´åºŠå…³è”"
        elif 'rate_difference' in results and results['rate_difference'] > 0.20:
            return "ç»„é—´å·®å¼‚å…·æœ‰ä¸´åºŠæ„ä¹‰"
        else:
            return "éœ€è¦è¿›ä¸€æ­¥è¯„ä¼°ä¸´åºŠæ„ä¹‰"

    def _assess_precision_medicine_potential(self, analysis_def: Dict, results: Dict) -> str:
        """è¯„ä¼°ç²¾å‡†åŒ»å­¦æ½œåŠ›"""
        analysis_type = analysis_def.get('analysis', {}).get('type', '')
        
        if 'random_forest' in analysis_type or 'clustering' in analysis_type:
            return "é«˜ - å¯ç”¨äºæ‚£è€…åˆ†å±‚å’Œä¸ªæ€§åŒ–æ²»ç–—"
        elif 'network' in analysis_type:
            return "ä¸­ç­‰ - å¯è¯†åˆ«ä¸ªæ€§åŒ–é¶ç‚¹"
        else:
            return "ä½ - ä¸»è¦ç”¨äºäººç¾¤å±‚é¢æŒ‡å¯¼"

    def _assess_health_economics_impact(self, results: Dict) -> str:
        """è¯„ä¼°å«ç”Ÿç»æµå­¦å½±å“"""
        if 'prevalence_rate' in results and results['prevalence_rate'] > 0.30:
            return "é«˜æ‚£ç—…ç‡æç¤ºé‡å¤§ç»æµè´Ÿæ‹…ï¼Œå¹²é¢„å…·æœ‰æˆæœ¬æ•ˆç›Š"
        elif 'risk_auc' in results and results['risk_auc'] > 0.75:
            return "é£é™©é¢„æµ‹æ¨¡å‹å¯ä¼˜åŒ–èµ„æºé…ç½®ï¼Œé™ä½åŒ»ç–—æˆæœ¬"
        else:
            return "éœ€è¦ä¸“é—¨çš„å«ç”Ÿç»æµå­¦è¯„ä»·"

    def _identify_regulatory_considerations(self, analysis_def: Dict) -> List[str]:
        """è¯†åˆ«ç›‘ç®¡è€ƒé‡"""
        considerations = []
        
        if 'prediction' in analysis_def.get('title', '').lower():
            considerations.append("AIåŒ»ç–—å™¨æ¢°ç›‘ç®¡è¦æ±‚")
            considerations.append("ä¸´åºŠéªŒè¯å’Œå®‰å…¨æ€§è¯„ä¼°")
            
        if 'biomarker' in str(analysis_def).lower():
            considerations.append("ç”Ÿç‰©æ ‡å¿—ç‰©éªŒè¯è¦æ±‚")
            considerations.append("è¯Šæ–­è¯•å‰‚ç›’æ³¨å†Œ")
            
        if not considerations:
            considerations.append("éµå¾ªåŒ»å­¦ç ”ç©¶ä¼¦ç†è¦æ±‚")
            
        return considerations

    def _assess_collaboration_potential(self, analysis_def: Dict, results: Dict) -> str:
        """è¯„ä¼°å›½é™…åˆä½œæ½œåŠ›"""
        if results.get('sample_size', 0) > 1000:
            return "é«˜ - é€‚åˆå›½é™…å¤šä¸­å¿ƒåˆä½œç ”ç©¶"
        elif 'network' in analysis_def.get('analysis', {}).get('type', ''):
            return "ä¸­ç­‰ - å¯ä¸ç³»ç»Ÿç”Ÿç‰©å­¦å›¢é˜Ÿåˆä½œ"
        else:
            return "ä½ - ä¸»è¦ä¸ºæœ¬åœ°ç ”ç©¶"

    def _identify_data_sharing_requirements(self, analysis_def: Dict) -> List[str]:
        """è¯†åˆ«æ•°æ®å…±äº«è¦æ±‚"""
        requirements = []
        
        if 'genomic' in str(analysis_def).lower():
            requirements.append("éµå¾ªåŸºå› ç»„æ•°æ®å…±äº«åè®®")
            requirements.append("æ‚£è€…çŸ¥æƒ…åŒæ„æ›´æ–°")
        
        requirements.extend([
            "æ•°æ®å»æ ‡è¯†åŒ–å¤„ç†",
            "éšç§ä¿æŠ¤æŠ€æœ¯åº”ç”¨",
            "æ•°æ®ä½¿ç”¨åè®®ç­¾ç½²"
        ])
        
        return requirements

    # --- æŠ¥å‘Šç”Ÿæˆå¢å¼º ---
    def generate_comprehensive_enhanced_report(self, output_path: str) -> str:
        """ç”Ÿæˆå…¨é¢çš„å¢å¼ºç‰ˆåˆ†ææŠ¥å‘Š"""
        if not self.quality_metrics:
            self.assess_multidimensional_data_quality()
            
        if not self.research_insights:
            self.discover_enhanced_research_opportunities()
            
        report_content = self._build_enhanced_report_content()
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            print(f"âœ… å¢å¼ºç‰ˆç»¼åˆæŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")
        except Exception as e:
            print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
            
        return report_content

    def _build_enhanced_report_content(self) -> str:
        """æ„å»ºå¢å¼ºç‰ˆæŠ¥å‘Šå†…å®¹"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        report = f"""
# CRFæ•°æ®æŒ–æ˜æ™ºèƒ½åˆ†ææŠ¥å‘Š - å¢å¼ºç‰ˆ

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´:** {timestamp}  
**åˆ†æç»´åº¦:** å¤šç»´åº¦ä¸´åºŠç ”ç©¶æ•°æ®æŒ–æ˜  
**Agentç‰ˆæœ¬:** Enhanced v3.0

---

## ğŸ¥ æ‰§è¡Œæ‘˜è¦

æœ¬æŠ¥å‘ŠåŸºäº{len(self.data_cache)}ä¸ªæ•°æ®æºï¼Œæ¶µç›–{sum(len(df) for df in self.data_cache.values())}æ¡æ‚£è€…è®°å½•ï¼Œé‡‡ç”¨{len(self.analysis_config)}ç§é«˜çº§ç»Ÿè®¡åˆ†ææ–¹æ³•ï¼Œè¯†åˆ«å‡º**{len(self.research_insights)}ä¸ªé«˜ä»·å€¼ç ”ç©¶æœºä¼š**ã€‚

### ğŸ“Š æ•°æ®è´¨é‡æ¦‚å†µ
"""
        
        # æ•°æ®è´¨é‡éƒ¨åˆ†
        for dataset_name, metrics in self.quality_metrics.items():
            report += f"""
#### æ•°æ®é›†: `{dataset_name}`
- **è®°å½•æ•°**: {metrics.total_records:,}
- **ç‰¹å¾ç»´åº¦**: {metrics.total_features}
- **æ•°æ®è´¨é‡ç­‰çº§**: {metrics.overall_grade}
- **ç¼ºå¤±ç‡**: {metrics.missing_rate:.1%}
- **é‡å¤ç‡**: {metrics.duplicate_rate:.1%}
"""

        # ç ”ç©¶å‘ç°éƒ¨åˆ†
        report += "\n## ğŸ¯ ç ”ç©¶æœºä¼šå‘ç°\n"
        
        if self.research_insights:
            # æŒ‰ä»·å€¼ç­‰çº§åˆ†ç»„
            value_groups = {}
            for insight in self.research_insights:
                level = insight.value_level.value
                if level not in value_groups:
                    value_groups[level] = []
                value_groups[level].append(insight)
            
            report += f"å…±è¯†åˆ«å‡º **{len(self.research_insights)}** ä¸ªç ”ç©¶æœºä¼šï¼ŒæŒ‰ä»·å€¼ç­‰çº§åˆ†å¸ƒï¼š\n\n"
            for level, insights in value_groups.items():
                report += f"- **{level}**: {len(insights)}ä¸ª\n"
            
            # è¯¦ç»†ç ”ç©¶æœºä¼š
            report += "\n### ğŸ† é«˜ä»·å€¼ç ”ç©¶å‘ç°è¯¦æƒ…\n"
            
            for i, insight in enumerate(self.research_insights, 1):
                report += f"""
#### {i}. {insight.title}

**ç ”ç©¶ä»·å€¼**: {insight.value_level.value}  
**å‘è¡¨ç±»å‹**: {insight.publication_type.value}  
**ç»Ÿè®¡åŠŸæ•ˆ**: {insight.statistical_power:.2f}  
**æ ·æœ¬é‡**: {insight.sample_size:,}  
**æœŸåˆŠæ¡£æ¬¡**: {insight.expected_impact_factor}

**ç ”ç©¶æè¿°**: {insight.description}

**å…³é”®å˜é‡**: `{', '.join(insight.key_variables[:5])}`  
**ç»Ÿè®¡æ–¹æ³•**: {', '.join(insight.statistical_methods)}

**ä¸´åºŠæ„ä¹‰**: {insight.clinical_significance}  
**ç²¾å‡†åŒ»å­¦æ½œåŠ›**: {insight.precision_medicine_potential}  
**å«ç”Ÿç»æµå­¦å½±å“**: {insight.health_economics_impact}

**å®æ–½å»ºè®®**:
"""
                for rec in insight.recommendations[:3]:  # é™åˆ¶æ˜¾ç¤ºæ¡æ•°
                    report += f"- {rec}\n"
                    
                report += f"""
**ç›‘ç®¡è€ƒé‡**: {'; '.join(insight.regulatory_considerations[:2])}  
**å›½é™…åˆä½œæ½œåŠ›**: {insight.international_collaboration_potential}

---
"""

        # å‘è¡¨ç­–ç•¥
        report += self._generate_publication_strategy_section()
        
        # è´¨é‡æ”¹è¿›å»ºè®®
        report += self._generate_quality_improvement_section()
        
        # æŠ€æœ¯é™„å½•
        report += self._generate_technical_appendix()
        
        return report

    def _generate_publication_strategy_section(self) -> str:
        """ç”Ÿæˆå‘è¡¨ç­–ç•¥éƒ¨åˆ†"""
        strategy_config = self.publication_strategy
        
        section = "\n## ğŸ“š å‘è¡¨ç­–ç•¥å»ºè®®\n"
        
        if not self.research_insights:
            return section + "æš‚æ— ç ”ç©¶å‘ç°ï¼Œæ— æ³•ç”Ÿæˆå‘è¡¨ç­–ç•¥ã€‚\n"
        
        # æŒ‰æœŸåˆŠæ¡£æ¬¡åˆ†ç»„
        high_impact = [i for i in self.research_insights if i.value_level in [ResearchValue.BREAKTHROUGH, ResearchValue.VERY_HIGH]]
        medium_impact = [i for i in self.research_insights if i.value_level == ResearchValue.HIGH]
        
        if high_impact:
            section += f"""
### ğŸŒŸ é«˜å½±å“å› å­æœŸåˆŠ (8+ IF)
**ç›®æ ‡æœŸåˆŠ**: Nature Medicine, Lancet Diabetes & Endocrinology, Diabetes Care
**ç ”ç©¶é¡¹ç›®**: {len(high_impact)}ä¸ª
**é¢„æœŸæ—¶é—´çº¿**: 18-24ä¸ªæœˆ

é‡ç‚¹é¡¹ç›®:
"""
            for insight in high_impact[:2]:  # æ˜¾ç¤ºå‰2ä¸ª
                section += f"- **{insight.title}** (ç»Ÿè®¡åŠŸæ•ˆ: {insight.statistical_power:.2f})\n"
        
        if medium_impact:
            section += f"""
### ğŸ“Š ä¸­ç­‰å½±å“å› å­æœŸåˆŠ (4-8 IF)  
**ç›®æ ‡æœŸåˆŠ**: Diabetologia, Cardiovascular Diabetology
**ç ”ç©¶é¡¹ç›®**: {len(medium_impact)}ä¸ª
**é¢„æœŸæ—¶é—´çº¿**: 12-18ä¸ªæœˆ
"""
        
        # æ—¶é—´è§„åˆ’
        section += """
### â° å‘è¡¨æ—¶é—´è§„åˆ’

**çŸ­æœŸ (6ä¸ªæœˆå†…)**:
- æ•°æ®æ¸…ç†å’Œè´¨é‡æ§åˆ¶å®Œå–„
- åˆæ­¥åˆ†æç»“æœéªŒè¯

**ä¸­æœŸ (6-12ä¸ªæœˆ)**:
- ä¸»è¦ç ”ç©¶å®Œæˆå’ŒæŠ•ç¨¿
- åŒè¡Œè¯„è®®å›åº”

**é•¿æœŸ (12-24ä¸ªæœˆ)**:
- é«˜å½±å“å› å­æœŸåˆŠæŠ•ç¨¿
- å›½é™…ä¼šè®®å±•ç¤º
"""
        
        return section

    def _generate_quality_improvement_section(self) -> str:
        """ç”Ÿæˆè´¨é‡æ”¹è¿›å»ºè®®éƒ¨åˆ†"""
        section = "\n## ğŸ”§ æ•°æ®è´¨é‡æ”¹è¿›å»ºè®®\n"
        
        all_recommendations = []
        for metrics in self.quality_metrics.values():
            all_recommendations.extend(metrics.recommendations)
        
        # å»é‡å¹¶åˆ†ç±»
        unique_recommendations = list(set(all_recommendations))
        
        if unique_recommendations:
            section += "### ğŸ“‹ ç»¼åˆæ”¹è¿›å»ºè®®\n"
            for i, rec in enumerate(unique_recommendations, 1):
                section += f"{i}. {rec}\n"
        
        # å¤šä¸­å¿ƒç ”ç©¶å»ºè®®
        if self.multicenter_settings:
            section += """
### ğŸŒ å¤šä¸­å¿ƒç ”ç©¶è´¨é‡æ§åˆ¶

**ä¸­å¿ƒé—´ä¸€è‡´æ€§è¦æ±‚**:
- å®éªŒå®¤æ ‡å‡†åŒ–å’Œè´¨æ§
- é—®å·è¯­è¨€ç‰ˆæœ¬éªŒè¯  
- å·¥ä½œäººå‘˜åŸ¹è®­è®¤è¯

**è´¨é‡ç›‘æ§æŒ‡æ ‡**:
- ä¸­å¿ƒé—´å˜å¼‚ç³»æ•° < 15%
- æ•°æ®å®Œæ•´åº¦ > 95%
- åè®®åç¦»ç‡ < 5%
"""
        
        return section

    def _generate_technical_appendix(self) -> str:
        """ç”ŸæˆæŠ€æœ¯é™„å½•"""
        section = "\n## ğŸ“ˆ æŠ€æœ¯é™„å½•\n"
        
        section += f"""
### ğŸ”¬ åˆ†ææ–¹æ³•ç»Ÿè®¡

**æ”¯æŒçš„åˆ†æç±»å‹**: {len(AnalysisType.__members__)}ç§
- ä¼ ç»Ÿç»Ÿè®¡æ–¹æ³•: å¡æ–¹æ£€éªŒ, tæ£€éªŒ, ç›¸å…³åˆ†æ
- æœºå™¨å­¦ä¹ æ–¹æ³•: éšæœºæ£®æ—, èšç±»åˆ†æ, é€»è¾‘å›å½’
- é«˜çº§åˆ†æ: ç”Ÿå­˜åˆ†æ, ä¸­ä»‹åˆ†æ, å¤šå±‚å›å½’, ç½‘ç»œåˆ†æ

**æ•°æ®è´¨é‡æŒ‡æ ‡**: {len(self.quality_checks.get('clinical_ranges', {}))}ä¸ªä¸´åºŠå‚æ•°èŒƒå›´æ£€æŸ¥

### ğŸ’» ç³»ç»Ÿé…ç½®

**Pythonä¾èµ–åŒ…çŠ¶æ€**:
- âœ… pandas, numpy: æ•°æ®å¤„ç†
- {'âœ…' if HAS_SCIPY else 'âŒ'} scipy: ç»Ÿè®¡åˆ†æ
- {'âœ…' if HAS_SKLEARN else 'âŒ'} scikit-learn: æœºå™¨å­¦ä¹ 
- {'âœ…' if HAS_NETWORKX else 'âŒ'} networkx: ç½‘ç»œåˆ†æ
- {'âœ…' if HAS_SURVIVAL else 'âŒ'} lifelines: ç”Ÿå­˜åˆ†æ
- {'âœ…' if HAS_PLOTTING else 'âŒ'} matplotlib, seaborn: å¯è§†åŒ–

### ğŸ¯ ç ”ç©¶è´¨é‡æ§åˆ¶

**ç»Ÿè®¡æ˜¾è‘—æ€§**: p < 0.05
**æœ€å°æ ·æœ¬é‡**: 100ä¾‹
**æ•°æ®å®Œæ•´åº¦è¦æ±‚**: > 80%
**æ•ˆåº”é‡é˜ˆå€¼**: å°(0.1), ä¸­(0.2), å¤§(0.35)
"""
        
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        section += f"""
---
**æŠ¥å‘Šç”Ÿæˆ**: {timestamp}  
**Agentç‰ˆæœ¬**: Enhanced CRF Research Mining Agent v3.0  
**æŠ€æœ¯æ”¯æŒ**: G+ Platform ä¸´åºŠç ”ç©¶å›¢é˜Ÿ
"""
        
        return section

# --- ä¸»æ‰§è¡Œå‡½æ•° ---
def main():
    """ä½¿ç”¨ç¤ºä¾‹"""
    print("=== CRFæ•°æ®æŒ–æ˜æ™ºèƒ½åˆ†æAgent - å¢å¼ºç‰ˆ ===")
    
    config_path = './Enhanced_Multi_Dimensional_Config.yaml'
    if not os.path.exists(config_path):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        print("è¯·ç¡®ä¿Enhanced_Multi_Dimensional_Config.yamlæ–‡ä»¶å­˜åœ¨")
        return

    agent = EnhancedCRFResearchMiningAgent(config_path=config_path)
    
    print("\n--- 1. åŠ è½½å¤šç»´åº¦æ•°æ® ---")
    datasets = agent.load_enhanced_crf_data()
    if not datasets:
        print("âš ï¸ æ— æ•°æ®åŠ è½½ï¼Œé€€å‡ºåˆ†æ")
        return

    print("\n--- 2. å¤šç»´åº¦æ•°æ®è´¨é‡è¯„ä¼° ---")
    quality_report = agent.assess_multidimensional_data_quality()
    
    print("\n--- 3. å‘ç°å¢å¼ºç ”ç©¶æœºä¼š ---")
    insights = agent.discover_enhanced_research_opportunities()
    
    print("\n--- 4. ç”Ÿæˆå¢å¼ºç‰ˆç»¼åˆæŠ¥å‘Š ---")
    report_path = './Enhanced_CRF_Research_Analysis_Report.md'
    agent.generate_comprehensive_enhanced_report(report_path)
    
    print(f"\nâœ… å¢å¼ºç‰ˆåˆ†æå®Œæˆï¼")
    print(f"ğŸ“Š å¤„ç†æ•°æ®: {sum(len(df) for df in datasets.values()):,}æ¡è®°å½•")
    print(f"ğŸ” å‘ç°ç ”ç©¶æœºä¼š: {len(insights)}ä¸ª")
    print(f"ğŸ“„ æŠ¥å‘Šè·¯å¾„: {report_path}")
    print("ğŸ¯ ç³»ç»Ÿæ”¯æŒ100+ä¸´åºŠå‚æ•°ï¼Œ10+é«˜çº§åˆ†ææ–¹æ³•")

if __name__ == "__main__":
    main()