#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆECGæ•°æ®åˆ†æå™¨ v3.0
é›†æˆneurokit2ç­‰ä»·çš„ä¸“ä¸šHRVæŒ‡æ ‡å’Œé¢‘åŸŸåˆ†æ
ä¸“é—¨ç”¨äºåˆ†æPhysioNet WFDBæ ¼å¼çš„12å¯¼è”ECGæ•°æ®
"""

import struct
import os
import pandas as pd
import numpy as np
import argparse
from pathlib import Path
import warnings
import math
from scipy import signal
from scipy.stats import entropy
import matplotlib.pyplot as plt

warnings.filterwarnings('ignore')

def parse_header_file(header_path):
    """è§£æ.heaå¤´æ–‡ä»¶è·å–è®°å½•ä¿¡æ¯"""
    info = {}
    leads = []
    
    try:
        with open(header_path, 'r') as f:
            lines = f.readlines()
        
        # ç¬¬ä¸€è¡ŒåŒ…å«åŸºæœ¬ä¿¡æ¯
        first_line = lines[0].strip().split()
        info['record_name'] = first_line[0]
        info['num_leads'] = int(first_line[1])
        info['sampling_rate'] = int(first_line[2])
        info['num_samples'] = int(first_line[3])
        
        # è§£æå¯¼è”ä¿¡æ¯å’Œå¢ç›Š
        gains = []
        baselines = []
        for i in range(1, info['num_leads'] + 1):
            lead_line = lines[i].strip().split()
            lead_name = lead_line[-1]  # å¯¼è”åç§°åœ¨æœ€å
            gain_str = lead_line[2]  # å¢ç›Šä¿¡æ¯ "1000/mV"
            gain = float(gain_str.split('/')[0])
            baseline = int(lead_line[4]) if len(lead_line) > 4 else 0
            
            leads.append(lead_name)
            gains.append(gain)
            baselines.append(baseline)
        
        info['leads'] = leads
        info['gains'] = gains
        info['baselines'] = baselines
        
        # è§£ææ‚£è€…ä¿¡æ¯
        for line in lines:
            line = line.strip()
            if line.startswith('#Age:'):
                info['age'] = line.split(': ')[1]
            elif line.startswith('#Sex:'):
                info['sex'] = line.split(': ')[1]
            elif line.startswith('#Dx:'):
                info['diagnosis'] = line.split(': ')[1]
        
        return info
        
    except Exception as e:
        print(f"è§£æå¤´æ–‡ä»¶é”™è¯¯: {e}")
        return None

def read_mat_file(mat_path, num_leads, num_samples):
    """è¯»å–.matäºŒè¿›åˆ¶æ•°æ®æ–‡ä»¶"""
    try:
        with open(mat_path, 'rb') as f:
            data = f.read()
        
        num_values = len(data) // 2
        values = struct.unpack(f'<{num_values}h', data)
        
        expected_values = num_leads * num_samples
        
        if num_values > expected_values:
            extra = num_values - expected_values
            signal_values = values[extra:]
            if len(signal_values) == expected_values:
                signal_data = np.array(signal_values).reshape(num_samples, num_leads)
            else:
                return None
        else:
            signal_data = np.array(values).reshape(num_samples, num_leads)
        
        return signal_data
        
    except Exception as e:
        print(f"è¯»å–æ•°æ®æ–‡ä»¶é”™è¯¯: {e}")
        return None

def convert_to_physical_units(signal_data, gains, baselines):
    """å°†æ•°å­—ä¿¡å·è½¬æ¢ä¸ºç‰©ç†å•ä½(mV)"""
    try:
        physical_data = np.zeros_like(signal_data, dtype=float)
        for i in range(len(gains)):
            # è½¬æ¢å…¬å¼: physical = (digital - baseline) / gain
            physical_data[:, i] = (signal_data[:, i] - baselines[i]) / gains[i]
        return physical_data
    except Exception as e:
        print(f"å•ä½è½¬æ¢é”™è¯¯: {e}")
        return signal_data.astype(float)

def advanced_r_peak_detection(ecg_signal, sampling_rate):
    """æ”¹è¿›çš„Rå³°æ£€æµ‹ç®—æ³•"""
    try:
        # 1. å¸¦é€šæ»¤æ³¢ (5-15Hzï¼Œçªå‡ºQRSå¤åˆæ³¢)
        nyquist = sampling_rate / 2
        low_cutoff = 5 / nyquist
        high_cutoff = 15 / nyquist
        
        # ä½¿ç”¨Butterworthæ»¤æ³¢å™¨
        b, a = signal.butter(4, [low_cutoff, high_cutoff], btype='band')
        filtered_signal = signal.filtfilt(b, a, ecg_signal)
        
        # 2. å¾®åˆ†æ“ä½œï¼ˆçªå‡ºQRSæ–œç‡ï¼‰
        diff_signal = np.diff(filtered_signal)
        
        # 3. å¹³æ–¹æ“ä½œï¼ˆæ”¾å¤§å¤§çš„å˜åŒ–ï¼‰
        squared_signal = diff_signal ** 2
        
        # 4. ç§»åŠ¨çª—å£ç§¯åˆ†
        window_size = int(sampling_rate * 0.08)  # 80msçª—å£
        integrated_signal = np.convolve(squared_signal, np.ones(window_size), mode='same')
        
        # 5. è‡ªé€‚åº”é˜ˆå€¼æ£€æµ‹
        threshold = np.mean(integrated_signal) + 2 * np.std(integrated_signal)
        
        # 6. å³°å€¼æ£€æµ‹
        peaks = []
        min_distance = int(sampling_rate * 0.4)  # æœ€å°RRé—´æœŸ400ms
        
        for i in range(min_distance, len(integrated_signal) - min_distance):
            if (integrated_signal[i] > threshold and
                integrated_signal[i] == np.max(integrated_signal[i-min_distance//2:i+min_distance//2])):
                # åœ¨åŸå§‹ä¿¡å·ä¸­ç²¾ç¡®å®šä½Rå³°
                search_window = slice(max(0, i-20), min(len(ecg_signal), i+20))
                local_peak = i - 20 + np.argmax(np.abs(ecg_signal[search_window]))
                
                if not peaks or (local_peak - peaks[-1]) >= min_distance:
                    peaks.append(local_peak)
        
        return peaks
        
    except Exception as e:
        print(f"é«˜çº§Rå³°æ£€æµ‹é”™è¯¯: {e}")
        return []

def calculate_comprehensive_hrv_metrics(r_peaks, sampling_rate):
    """è®¡ç®—å…¨é¢çš„HRVæŒ‡æ ‡ï¼ˆç­‰ä»·neurokit2åŠŸèƒ½ï¼‰"""
    if len(r_peaks) < 5:  # éœ€è¦è‡³å°‘5ä¸ªRå³°
        return {}
    
    # RRé—´æœŸï¼ˆæ¯«ç§’ï¼‰
    rr_intervals = np.diff(r_peaks) / sampling_rate * 1000
    
    if len(rr_intervals) < 4:
        return {}
    
    # === æ—¶åŸŸæŒ‡æ ‡ ===
    metrics = {}
    
    # åŸºç¡€æ—¶åŸŸæŒ‡æ ‡
    metrics['mean_rr'] = np.mean(rr_intervals)
    metrics['std_rr'] = np.std(rr_intervals, ddof=1)  # SDNN
    metrics['min_rr'] = np.min(rr_intervals)
    metrics['max_rr'] = np.max(rr_intervals)
    metrics['mean_hr'] = 60000 / metrics['mean_rr']
    metrics['std_hr'] = np.std(60000 / rr_intervals, ddof=1)
    
    # é«˜çº§æ—¶åŸŸæŒ‡æ ‡
    diff_rr = np.diff(rr_intervals)
    if len(diff_rr) > 0:
        # RMSSD - ç›¸é‚»RRé—´æœŸå·®å€¼çš„å‡æ–¹æ ¹
        metrics['rmssd'] = np.sqrt(np.mean(diff_rr ** 2))
        
        # pNNxæŒ‡æ ‡
        metrics['pnn50'] = (np.sum(np.abs(diff_rr) > 50) / len(diff_rr)) * 100
        metrics['pnn20'] = (np.sum(np.abs(diff_rr) > 20) / len(diff_rr)) * 100
        metrics['pnn10'] = (np.sum(np.abs(diff_rr) > 10) / len(diff_rr)) * 100
        
        # SDSD - ç›¸é‚»RRé—´æœŸå·®å€¼çš„æ ‡å‡†å·®
        metrics['sdsd'] = np.std(diff_rr, ddof=1)
        
        # é«˜çº§ç»Ÿè®¡æŒ‡æ ‡
        metrics['cv'] = (metrics['std_rr'] / metrics['mean_rr']) * 100  # å˜å¼‚ç³»æ•°
        metrics['median_rr'] = np.median(rr_intervals)
        metrics['mad_rr'] = np.median(np.abs(rr_intervals - metrics['median_rr']))  # ä¸­ä½æ•°ç»å¯¹åå·®
        
        # èŒƒå›´å’Œå››åˆ†ä½æ•°
        metrics['range_rr'] = metrics['max_rr'] - metrics['min_rr']
        metrics['iqr_rr'] = np.percentile(rr_intervals, 75) - np.percentile(rr_intervals, 25)
        
        # === å‡ ä½•æŒ‡æ ‡ ===
        # ä¸‰è§’æŒ‡æ•°ï¼ˆè¿‘ä¼¼ï¼‰
        hist, bin_edges = np.histogram(rr_intervals, bins=32)
        metrics['triangular_index'] = len(rr_intervals) / np.max(hist) if np.max(hist) > 0 else np.nan
        
        # TINN - ä¸‰è§’æ’å€¼åŸºçº¿å®½åº¦
        metrics['tinn'] = np.max(rr_intervals) - np.min(rr_intervals)
        
        # å‡ ä½•å‡å€¼
        metrics['geometric_mean_rr'] = np.exp(np.mean(np.log(rr_intervals)))
        
        # === é¢‘åŸŸæŒ‡æ ‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰===
        try:
            # é‡é‡‡æ ·åˆ°å‡åŒ€æ—¶é—´é—´éš”ï¼ˆ4Hzï¼‰
            time_rr = np.cumsum(rr_intervals) / 1000  # è½¬æ¢ä¸ºç§’
            time_uniform = np.arange(0, time_rr[-1], 0.25)  # 4Hzé‡‡æ ·
            
            if len(time_uniform) > 10:  # éœ€è¦è¶³å¤Ÿçš„é‡‡æ ·ç‚¹
                # çº¿æ€§æ’å€¼
                rr_interpolated = np.interp(time_uniform, time_rr, rr_intervals)
                
                # å»è¶‹åŠ¿
                rr_detrended = rr_interpolated - np.mean(rr_interpolated)
                
                # åŠŸç‡è°±å¯†åº¦
                freqs, psd = signal.welch(rr_detrended, fs=4.0, nperseg=min(256, len(rr_detrended)))
                
                # é¢‘åŸŸæŒ‡æ ‡
                vlf_band = (freqs >= 0.0033) & (freqs < 0.04)  # VLF: 0.0033-0.04 Hz
                lf_band = (freqs >= 0.04) & (freqs < 0.15)     # LF: 0.04-0.15 Hz  
                hf_band = (freqs >= 0.15) & (freqs < 0.4)      # HF: 0.15-0.4 Hz
                
                metrics['vlf_power'] = np.trapz(psd[vlf_band], freqs[vlf_band]) if np.any(vlf_band) else 0
                metrics['lf_power'] = np.trapz(psd[lf_band], freqs[lf_band]) if np.any(lf_band) else 0
                metrics['hf_power'] = np.trapz(psd[hf_band], freqs[hf_band]) if np.any(hf_band) else 0
                
                # æ€»åŠŸç‡
                metrics['total_power'] = metrics['vlf_power'] + metrics['lf_power'] + metrics['hf_power']
                
                # ç›¸å¯¹åŠŸç‡
                if metrics['total_power'] > 0:
                    metrics['vlf_relative'] = metrics['vlf_power'] / metrics['total_power'] * 100
                    metrics['lf_relative'] = metrics['lf_power'] / metrics['total_power'] * 100
                    metrics['hf_relative'] = metrics['hf_power'] / metrics['total_power'] * 100
                
                # LF/HFæ¯”å€¼
                metrics['lf_hf_ratio'] = metrics['lf_power'] / metrics['hf_power'] if metrics['hf_power'] > 0 else np.nan
                
                # å³°é¢‘ç‡
                if np.any(lf_band):
                    lf_peak_idx = np.argmax(psd[lf_band])
                    metrics['lf_peak'] = freqs[lf_band][lf_peak_idx]
                
                if np.any(hf_band):
                    hf_peak_idx = np.argmax(psd[hf_band])
                    metrics['hf_peak'] = freqs[hf_band][hf_peak_idx]
            
        except Exception as e:
            print(f"é¢‘åŸŸåˆ†æé”™è¯¯: {e}")
            # å¦‚æœé¢‘åŸŸåˆ†æå¤±è´¥ï¼Œè®¾ç½®é»˜è®¤å€¼
            for key in ['vlf_power', 'lf_power', 'hf_power', 'total_power', 
                       'vlf_relative', 'lf_relative', 'hf_relative', 'lf_hf_ratio',
                       'lf_peak', 'hf_peak']:
                metrics[key] = np.nan
        
        # === éçº¿æ€§æŒ‡æ ‡ ===
        # æ ·æœ¬ç†µï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        try:
            # PoincarÃ©å›¾æŒ‡æ ‡
            rr1 = rr_intervals[:-1]  # RRn
            rr2 = rr_intervals[1:]   # RRn+1
            
            # SD1 - çŸ­æœŸå˜å¼‚æ€§
            metrics['sd1'] = np.std(rr1 - rr2, ddof=1) / np.sqrt(2)
            
            # SD2 - é•¿æœŸå˜å¼‚æ€§  
            metrics['sd2'] = np.sqrt(2 * np.var(rr_intervals, ddof=1) - np.var(rr1 - rr2, ddof=1))
            
            # SD1/SD2æ¯”å€¼
            metrics['sd1_sd2_ratio'] = metrics['sd1'] / metrics['sd2'] if metrics['sd2'] > 0 else np.nan
            
            # æ¤­åœ†é¢ç§¯
            metrics['csi'] = metrics['sd2'] / metrics['sd1'] if metrics['sd1'] > 0 else np.nan  # å¿ƒè„äº¤æ„ŸæŒ‡æ•°
            metrics['cvi'] = np.log10(metrics['sd1'] * metrics['sd2']) if (metrics['sd1'] > 0 and metrics['sd2'] > 0) else np.nan  # å¿ƒè„è¿·èµ°æŒ‡æ•°
            
        except Exception as e:
            print(f"éçº¿æ€§åˆ†æé”™è¯¯: {e}")
            for key in ['sd1', 'sd2', 'sd1_sd2_ratio', 'csi', 'cvi']:
                metrics[key] = np.nan
        
        # === ç»Ÿè®¡å½¢çŠ¶æŒ‡æ ‡ ===
        # ååº¦å’Œå³°åº¦
        from scipy.stats import skew, kurtosis
        metrics['skewness'] = skew(rr_intervals)
        metrics['kurtosis'] = kurtosis(rr_intervals)
        
    return metrics

def analyze_signal_quality(ecg_signal, sampling_rate):
    """åˆ†æä¿¡å·è´¨é‡"""
    try:
        # ä¿¡å™ªæ¯”ä¼°è®¡
        signal_power = np.mean(ecg_signal ** 2)
        noise_estimate = np.mean(np.diff(ecg_signal) ** 2)  # ç®€å•å™ªå£°ä¼°è®¡
        snr = 10 * np.log10(signal_power / noise_estimate) if noise_estimate > 0 else float('inf')
        
        # åŸºçº¿æ¼‚ç§»æ£€æµ‹
        low_freq_content = np.mean(np.abs(ecg_signal[:int(sampling_rate)]))
        baseline_drift = low_freq_content / np.mean(np.abs(ecg_signal))
        
        # é¥±å’Œåº¦æ£€æµ‹
        max_val = np.max(np.abs(ecg_signal))
        saturation = np.sum(np.abs(ecg_signal) > 0.95 * max_val) / len(ecg_signal)
        
        quality = {
            'snr_db': snr,
            'baseline_drift_ratio': baseline_drift,
            'saturation_ratio': saturation * 100,
            'dynamic_range': np.max(ecg_signal) - np.min(ecg_signal)
        }
        
        return quality
        
    except Exception as e:
        print(f"ä¿¡å·è´¨é‡åˆ†æé”™è¯¯: {e}")
        return {}

def analyze_single_record_enhanced(record_name, data_dir):
    """å¢å¼ºç‰ˆåˆ†æå•ä¸ªECGè®°å½•"""
    print(f"\n=== å¢å¼ºç‰ˆåˆ†æè®°å½•: {record_name} ===")
    
    header_path = os.path.join(data_dir, f"{record_name}.hea")
    mat_path = os.path.join(data_dir, f"{record_name}.mat")
    
    # è§£æå¤´æ–‡ä»¶
    header_info = parse_header_file(header_path)
    if not header_info:
        return None
    
    print(f"å¯¼è”æ•°: {header_info['num_leads']}")
    print(f"é‡‡æ ·ç‡: {header_info['sampling_rate']} Hz")
    print(f"æ—¶é•¿: {header_info['num_samples'] / header_info['sampling_rate']:.1f}ç§’")
    
    # è¯»å–æ•°æ®
    signal_data = read_mat_file(mat_path, header_info['num_leads'], header_info['num_samples'])
    if signal_data is None:
        return None
    
    # è½¬æ¢ä¸ºç‰©ç†å•ä½
    physical_data = convert_to_physical_units(signal_data, header_info['gains'], header_info['baselines'])
    
    # åˆ†ææ‰€æœ‰å¯¼è”
    lead_analyses = {}
    for i, lead_name in enumerate(header_info['leads']):
        ecg_signal = physical_data[:, i]
        
        # ä¿¡å·è´¨é‡åˆ†æ
        quality = analyze_signal_quality(ecg_signal, header_info['sampling_rate'])
        
        # Rå³°æ£€æµ‹
        r_peaks = advanced_r_peak_detection(ecg_signal, header_info['sampling_rate'])
        
        # å…¨é¢HRVåˆ†æ
        hrv_metrics = calculate_comprehensive_hrv_metrics(r_peaks, header_info['sampling_rate'])
        
        lead_analyses[lead_name] = {
            'r_peaks_count': len(r_peaks),
            'quality': quality,
            **hrv_metrics
        }
        
        print(f"å¯¼è” {lead_name}: {len(r_peaks)}ä¸ªRå³°, SNR: {quality.get('snr_db', 0):.1f}dB")
    
    # é€‰æ‹©æœ€ä½³å¯¼è”è¿›è¡Œä¸»è¦åˆ†æ
    best_lead = max(lead_analyses.keys(), 
                   key=lambda x: lead_analyses[x]['quality'].get('snr_db', 0))
    
    print(f"æœ€ä½³åˆ†æå¯¼è”: {best_lead}")
    
    # æ•´åˆç»“æœ
    result = {
        'record_name': record_name,
        'num_leads': header_info['num_leads'],
        'sampling_rate': header_info['sampling_rate'],
        'duration_sec': header_info['num_samples'] / header_info['sampling_rate'],
        'best_lead': best_lead,
        **{k: v for k, v in header_info.items() if k in ['age', 'sex', 'diagnosis']},
        **lead_analyses[best_lead]
    }
    
    # æ·»åŠ å¤šå¯¼è”ç»Ÿè®¡
    all_r_peaks = [lead_analyses[lead]['r_peaks_count'] for lead in header_info['leads']]
    result['mean_r_peaks_across_leads'] = np.mean(all_r_peaks)
    result['r_peaks_consistency'] = 1 - (np.std(all_r_peaks) / np.mean(all_r_peaks)) if np.mean(all_r_peaks) > 0 else 0
    
    print("âœ… å¢å¼ºç‰ˆåˆ†æå®Œæˆ")
    return result

def analyze_directory_enhanced(data_dir, output_file=None):
    """æ‰¹é‡å¢å¼ºç‰ˆåˆ†æ"""
    print(f"ğŸ” å¢å¼ºç‰ˆåˆ†æç›®å½•: {data_dir}")
    
    # è¯»å–RECORDSæ–‡ä»¶
    records_file = os.path.join(data_dir, 'RECORDS')
    if not os.path.exists(records_file):
        print("âŒ æœªæ‰¾åˆ°RECORDSæ–‡ä»¶")
        return None
    
    with open(records_file, 'r') as f:
        record_names = [line.strip() for line in f if line.strip()]
    
    print(f"æ‰¾åˆ° {len(record_names)} ä¸ªè®°å½•")
    
    # åˆ†ææ‰€æœ‰è®°å½•
    results = []
    successful = 0
    
    for i, record_name in enumerate(record_names, 1):
        print(f"\nè¿›åº¦: {i}/{len(record_names)}")
        result = analyze_single_record_enhanced(record_name, data_dir)
        
        if result:
            results.append(result)
            successful += 1
    
    if results:
        df = pd.DataFrame(results)
        
        if output_file is None:
            output_file = os.path.join(data_dir, 'enhanced_ecg_analysis_results.csv')
        
        df.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"\nğŸ“Š å¢å¼ºç‰ˆåˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        print(f"âœ… æˆåŠŸåˆ†æ: {successful}/{len(record_names)} ä¸ªè®°å½•")
        
        # æ˜¾ç¤ºç»Ÿè®¡æ‘˜è¦
        print(f"\nğŸ“ˆ å¢å¼ºç‰ˆåˆ†ææ‘˜è¦:")
        
        # åŸºç¡€æŒ‡æ ‡
        numeric_cols = ['mean_hr', 'std_rr', 'rmssd', 'lf_power', 'hf_power', 'lf_hf_ratio', 'sd1', 'sd2']
        for col in numeric_cols:
            if col in df.columns:
                values = pd.to_numeric(df[col], errors='coerce').dropna()
                if len(values) > 0:
                    print(f"{col}: {values.mean():.2f} Â± {values.std():.2f}")
        
        # ä¿¡å·è´¨é‡
        if 'quality' in df.columns:
            try:
                import ast
                snr_values = []
                for quality_str in df['quality']:
                    try:
                        quality_dict = ast.literal_eval(str(quality_str))
                        snr = quality_dict.get('snr_db', np.nan)
                        if not np.isnan(snr):
                            snr_values.append(snr)
                    except:
                        continue
                
                if snr_values:
                    print(f"å¹³å‡SNR: {np.mean(snr_values):.1f} Â± {np.std(snr_values):.1f} dB")
            except Exception as e:
                print(f"SNRç»Ÿè®¡é”™è¯¯: {e}")
        
        return df
    else:
        print("âŒ æ²¡æœ‰æˆåŠŸåˆ†æçš„è®°å½•")
        return None

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="å¢å¼ºç‰ˆECGæ•°æ®åˆ†æå™¨")
    parser.add_argument("data_dir", help="ECGæ•°æ®ç›®å½•è·¯å¾„")
    parser.add_argument("--output", "-o", help="è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    if not os.path.exists(args.data_dir):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {args.data_dir}")
        exit(1)
    
    # æ£€æŸ¥scipyæ˜¯å¦å¯ç”¨
    try:
        from scipy import signal
        from scipy.stats import skew, kurtosis
        print("ğŸ¯ ä½¿ç”¨å®Œæ•´çš„ä¿¡å·å¤„ç†å’Œç»Ÿè®¡åŠŸèƒ½")
    except ImportError:
        print("âš ï¸  éƒ¨åˆ†åŠŸèƒ½å¯èƒ½å—é™ï¼Œå»ºè®®å®‰è£…scipy")
    
    analyze_directory_enhanced(args.data_dir, args.output)