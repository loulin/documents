#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CGMæ•°æ®è¯»å–å’ŒAGPåˆ†ææ™ºèƒ½ä½“
åŸºäºAGP_Visual_Pattern_Analysis.csvçš„57ç§è§†è§‰æŒ‡æ ‡å®ç°
"""

import pandas as pd
import numpy as np
from scipy import signal, stats
from scipy.fft import fft, fftfreq
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional
import json
import logging

from ..config.config_manager import ConfigManager

class CGMDataReader:
    """CGMåŸå§‹æ•°æ®è¯»å–å™¨ - æ”¯æŒå¤šç§CGMè®¾å¤‡æ ¼å¼"""
    
    def __init__(self):
        self.supported_formats = ['dexcom', 'freestyle', 'medtronic', 'generic_csv']
        
    def read_cgm_file(self, file_path: str, device_type: str = 'auto') -> pd.DataFrame:
        """
        è¯»å–CGMåŸå§‹æ•°æ®æ–‡ä»¶
        
        Args:
            file_path: CGMæ•°æ®æ–‡ä»¶è·¯å¾„
            device_type: è®¾å¤‡ç±»å‹ ('dexcom', 'freestyle', 'medtronic', 'generic_csv', 'auto')
            
        Returns:
            æ ‡å‡†åŒ–çš„CGMæ•°æ®DataFrame (timestamp, glucose, device_info)
        """
        if device_type == 'auto':
            device_type = self._detect_device_type(file_path)
            
        if device_type == 'dexcom':
            return self._read_dexcom(file_path)
        elif device_type == 'freestyle':
            return self._read_freestyle(file_path)
        elif device_type == 'medtronic':
            return self._read_medtronic(file_path)
        elif device_type == 'generic_csv':
            return self._read_generic_csv(file_path)
        else:
            raise ValueError(f"Unsupported device type: {device_type}")
    
    def _detect_device_type(self, file_path: str) -> str:
        """è‡ªåŠ¨æ£€æµ‹CGMè®¾å¤‡ç±»å‹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                header = f.readline().lower()
                if 'dexcom' in header or 'glucose value' in header:
                    return 'dexcom'
                elif 'freestyle' in header or 'historic glucose' in header:
                    return 'freestyle'
                elif 'medtronic' in header or 'sensor glucose' in header:
                    return 'medtronic'
                else:
                    return 'generic_csv'
        except:
            return 'generic_csv'
    
    def _read_dexcom(self, file_path: str) -> pd.DataFrame:
        """è¯»å–Dexcom CGMæ•°æ®"""
        df = pd.read_csv(file_path)
        
        # Dexcomæ ¼å¼æ ‡å‡†åŒ–
        glucose_col = 'Glucose Value (mg/dL)' if 'Glucose Value (mg/dL)' in df.columns else 'glucose'
        timestamp_col = 'Timestamp (YYYY-MM-DDTHH:MM:SS)' if 'Timestamp (YYYY-MM-DDTHH:MM:SS)' in df.columns else 'timestamp'
        
        result = pd.DataFrame({
            'timestamp': pd.to_datetime(df[timestamp_col]),
            'glucose': df[glucose_col] * 0.0555 if df[glucose_col].max() > 50 else df[glucose_col],  # mg/dL to mmol/L
            'device_info': 'dexcom'
        })
        
        return result.dropna().reset_index(drop=True)
    
    def _read_freestyle(self, file_path: str) -> pd.DataFrame:
        """è¯»å–FreeStyle CGMæ•°æ®"""
        df = pd.read_csv(file_path, skiprows=1)  # FreeStyleé€šå¸¸æœ‰æ ‡é¢˜è¡Œ
        
        # FreeStyleæ ¼å¼æ ‡å‡†åŒ–  
        result = pd.DataFrame({
            'timestamp': pd.to_datetime(df.iloc[:, 0]),  # ç¬¬ä¸€åˆ—é€šå¸¸æ˜¯æ—¶é—´
            'glucose': pd.to_numeric(df.iloc[:, 1], errors='coerce'),  # ç¬¬äºŒåˆ—æ˜¯è¡€ç³–å€¼
            'device_info': 'freestyle'
        })
        
        # å•ä½è½¬æ¢ (å¦‚æœéœ€è¦)
        if result['glucose'].max() > 50:  # å¯èƒ½æ˜¯mg/dL
            result['glucose'] = result['glucose'] * 0.0555
            
        return result.dropna().reset_index(drop=True)
    
    def _read_medtronic(self, file_path: str) -> pd.DataFrame:
        """è¯»å–Medtronic CGMæ•°æ®"""
        df = pd.read_csv(file_path)
        
        result = pd.DataFrame({
            'timestamp': pd.to_datetime(df['Date'] + ' ' + df['Time']),
            'glucose': df['Sensor Glucose (mmol/L)'],
            'device_info': 'medtronic'
        })
        
        return result.dropna().reset_index(drop=True)
    
    def _read_generic_csv(self, file_path: str) -> pd.DataFrame:
        """è¯»å–é€šç”¨CSVæ ¼å¼CGMæ•°æ®"""
        # æ ¹æ®R002 V5.txtçš„æ ¼å¼è¿›è¡Œä¿®æ”¹ï¼štabåˆ†éš”ï¼Œè·³è¿‡å‰3è¡Œå¤´éƒ¨ï¼Œæ— åˆ—å
        # æ•°æ®è¡Œæ ¼å¼ä¸ºï¼šID\tæ—¶é—´\tè®°å½•ç±»å‹\tè‘¡è„ç³–å†å²è®°å½•ï¼ˆmmol/Lï¼‰
        # å› æ­¤ï¼Œæ—¶é—´åœ¨ç´¢å¼•1ï¼Œè‘¡è„ç³–åœ¨ç´¢å¼•3
        df = pd.read_csv(file_path, sep='\t', skiprows=3, header=None) 
        
        # æ˜ç¡®æŒ‡å®šæ—¶é—´å’Œè¡€ç³–çš„åˆ—ç´¢å¼•
        timestamp_col_index = 1
        glucose_col_index = 3
            
        result = pd.DataFrame({
            'timestamp': pd.to_datetime(df.iloc[:, timestamp_col_index], format='%Y/%m/%d %H:%M'), # æŒ‡å®šæ—¶é—´æ ¼å¼ï¼Œæé«˜é²æ£’æ€§
            'glucose': pd.to_numeric(df.iloc[:, glucose_col_index], errors='coerce'),
            'device_info': 'generic'
        })
        
        # å•ä½è½¬æ¢æ£€æŸ¥ (å¦‚æœéœ€è¦)
        # R002 V5.txtä¸­çš„è‘¡è„ç³–å·²ç»æ˜¯mmol/Lï¼Œæ‰€ä»¥è¿™é‡Œmax()ä¸ä¼šå¤§äº50ï¼Œä¸ä¼šè¿›è¡Œè½¬æ¢
        if result['glucose'].max() > 50:
            result['glucose'] = result['glucose'] * 0.0555
            
        return result.dropna().reset_index(drop=True)


class AGPVisualAnalyzer:
    """AGPè§†è§‰æ¨¡å¼åˆ†æå™¨ - å®ç°57ç§è§†è§‰æŒ‡æ ‡"""
    
    def __init__(self, enable_quality_check=True):
        self.indicators = {}
        self.enable_quality_check = enable_quality_check
        if enable_quality_check:
            try:
                from CGM_Data_Quality_Assessor import CGMDataQualityAssessor
                self.quality_assessor = CGMDataQualityAssessor()
            except ImportError:
                print("âš ï¸ æœªæ‰¾åˆ°æ•°æ®è´¨é‡è¯„ä¼°æ¨¡å—ï¼Œå°†è·³è¿‡è´¨é‡æ£€æŸ¥")
                self.enable_quality_check = False
        
    def analyze_cgm_data(self, cgm_data: pd.DataFrame, analysis_days: int = 14) -> Dict:
        """
        å®Œæ•´çš„CGMæ•°æ®AGPè§†è§‰åˆ†æ
        
        Args:
            cgm_data: æ ‡å‡†åŒ–CGMæ•°æ®
            analysis_days: åˆ†æå¤©æ•°
            
        Returns:
            åŒ…å«57ç§è§†è§‰æŒ‡æ ‡çš„åˆ†æç»“æœ
        """
        # ç¬¬ä¸€æ­¥ï¼šæ•°æ®è´¨é‡è¯„ä¼°
        if self.enable_quality_check:
            print("ğŸ” æ­£åœ¨è¿›è¡Œæ•°æ®è´¨é‡è¯„ä¼°...")
            quality_assessment = self.quality_assessor.assess_data_quality(cgm_data, analysis_days)
            
            if not quality_assessment['usable_for_analysis']:
                print("âŒ æ•°æ®è´¨é‡ä¸åˆæ ¼ï¼Œæ— æ³•è¿›è¡Œå¯é çš„AGPåˆ†æ")
                quality_report = self.quality_assessor.generate_quality_report(quality_assessment)
                print(quality_report)
                
                return {
                    'error': 'data_quality_insufficient',
                    'quality_assessment': quality_assessment,
                    'message': 'æ•°æ®è´¨é‡ä¸ç¬¦åˆAGPåˆ†æè¦æ±‚ï¼Œè¯·æ”¹å–„æ•°æ®è´¨é‡åé‡è¯•'
                }
            else:
                print(f"âœ… æ•°æ®è´¨é‡è¯„ä¼°é€šè¿‡: {quality_assessment['overall_quality']['quality_level']} ({quality_assessment['overall_quality']['total_score']}/100)")
        
        # æ•°æ®é¢„å¤„ç†å’ŒéªŒè¯
        processed_data = self._preprocess_data(cgm_data, analysis_days)
        
        # è®¡ç®—å„ç±»è§†è§‰æŒ‡æ ‡
        results = {}
        
        # 1. AGPæ›²çº¿å½¢æ€åˆ†æ (7ä¸ªæŒ‡æ ‡)
        results.update(self._analyze_curve_morphology(processed_data))
        
        # 2. æ—¶é—´æ®µåˆ†æ (6ä¸ªæŒ‡æ ‡)
        results.update(self._analyze_time_periods(processed_data))
        
        # 3. åˆ†ä½æ•°å¸¦åˆ†æ (5ä¸ªæŒ‡æ ‡)
        results.update(self._analyze_percentile_bands(processed_data))
        
        # 4. å¼‚å¸¸æ¨¡å¼è¯†åˆ« (5ä¸ªæŒ‡æ ‡)
        results.update(self._analyze_abnormal_patterns(processed_data))
        
        # 5. é¤æ—¶æ¨¡å¼åˆ†æ (4ä¸ªæŒ‡æ ‡)
        results.update(self._analyze_meal_patterns(processed_data))
        
        # 6. æ›²çº¿å¤æ‚åº¦åˆ†æ (5ä¸ªæŒ‡æ ‡)
        results.update(self._analyze_curve_complexity(processed_data))
        
        # 7. ç›®æ ‡èŒƒå›´åˆ†æ (4ä¸ªæŒ‡æ ‡)
        results.update(self._analyze_target_range(processed_data))
        
        # 8. é£é™©åŒºåŸŸåˆ†æ (4ä¸ªæŒ‡æ ‡)
        results.update(self._analyze_risk_zones(processed_data))
        
        # 9. æ—¥é—´å˜å¼‚åˆ†æ (4ä¸ªæŒ‡æ ‡)
        results.update(self._analyze_daily_variability(processed_data))
        
        # 10. å¹³æ»‘åº¦ç»¼åˆåˆ†æ (4ä¸ªæŒ‡æ ‡)
        results.update(self._analyze_comprehensive_smoothness(processed_data))
        
        # 11. å¯¹ç§°æ€§åˆ†æ (4ä¸ªæŒ‡æ ‡)
        results.update(self._analyze_symmetry(processed_data))
        
        # 12. å›¾å½¢ç¾å­¦åˆ†æ (3ä¸ªæŒ‡æ ‡)
        results.update(self._analyze_visual_aesthetics(processed_data))
        
        return results
    
    def _preprocess_data(self, cgm_data: pd.DataFrame, analysis_days: int) -> Dict:
        """æ•°æ®é¢„å¤„ç†å’ŒAGPæ›²çº¿ç”Ÿæˆ"""
        # å–æœ€è¿‘Nå¤©æ•°æ®
        end_date = cgm_data['timestamp'].max()
        start_date = end_date - timedelta(days=analysis_days)
        data = cgm_data[cgm_data['timestamp'] >= start_date].copy()
        
        # ç”Ÿæˆ24å°æ—¶AGPæ›²çº¿
        data['hour'] = data['timestamp'].dt.hour + data['timestamp'].dt.minute / 60.0
        
        # è®¡ç®—æ¯å°æ—¶çš„åˆ†ä½æ•°
        hourly_stats = data.groupby('hour')['glucose'].describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.95])
        
        agp_curve = {
            'hour': np.linspace(0, 24, 96),  # 15åˆ†é’Ÿé—´éš”
            'p05': np.interp(np.linspace(0, 24, 96), hourly_stats.index, hourly_stats['5%']),
            'p25': np.interp(np.linspace(0, 24, 96), hourly_stats.index, hourly_stats['25%']),
            'p50': np.interp(np.linspace(0, 24, 96), hourly_stats.index, hourly_stats['50%']),
            'p75': np.interp(np.linspace(0, 24, 96), hourly_stats.index, hourly_stats['75%']),
            'p95': np.interp(np.linspace(0, 24, 96), hourly_stats.index, hourly_stats['95%'])
        }
        
        return {
            'raw_data': data,
            'agp_curve': agp_curve,
            'analysis_period': analysis_days,
            'data_points': len(data)
        }
    
    def _analyze_curve_morphology(self, processed_data: Dict) -> Dict:
        """AGPæ›²çº¿å½¢æ€åˆ†æ (æŒ‡æ ‡1-7)"""
        agp = processed_data['agp_curve']
        median_curve = agp['p50']
        
        results = {}
        
        # 1. ä¸­ä½æ•°æ›²çº¿å¹³æ»‘åº¦
        curvature = np.abs(np.diff(median_curve, n=2))
        results['median_curve_smoothness'] = 1 / (1 + np.mean(curvature))
        
        # 2. 25%-75%åˆ†ä½æ•°å¸¦å®½å¹³å‡å€¼
        band_width = agp['p75'] - agp['p25']
        results['percentile_band_width_avg'] = np.mean(band_width)
        
        # 3. ä¸­ä½æ•°æ›²çº¿å¯¹ç§°æ€§æŒ‡æ•°
        first_half = median_curve[:48]
        second_half = median_curve[48:][::-1]  # åè½¬ååŠæ®µ
        results['curve_symmetry_index'] = np.corrcoef(first_half, second_half)[0,1]
        
        # 4. 24å°æ—¶å†…æ›²çº¿å³°å€¼æ•°é‡
        peaks, _ = signal.find_peaks(median_curve, height=np.mean(median_curve))
        results['curve_peak_count'] = len(peaks)
        
        # 5. 24å°æ—¶å†…æ›²çº¿ä½è°·æ•°é‡
        valleys, _ = signal.find_peaks(-median_curve, height=-np.mean(median_curve))
        results['curve_valley_count'] = len(valleys)
        
        # 6. ä¸€é˜¶å·®åˆ†å¹³æ»‘åº¦
        first_diff = np.diff(median_curve)
        results['first_order_smoothness'] = 1 - np.std(first_diff) / np.mean(np.abs(first_diff))
        
        # 7. äºŒé˜¶å·®åˆ†å¹³æ»‘åº¦
        second_diff = np.diff(median_curve, n=2)
        results['second_order_smoothness'] = 1 / (1 + np.var(second_diff))
        
        return results
    
    def _analyze_time_periods(self, processed_data: Dict) -> Dict:
        """æ—¶é—´æ®µåˆ†æ (æŒ‡æ ‡8-13)"""
        agp = processed_data['agp_curve']
        hours = agp['hour']
        median_curve = agp['p50']
        
        results = {}
        
        # 8. é»æ˜ç°è±¡æ–œç‡ (4-8ç‚¹)
        dawn_mask = (hours >= 4) & (hours <= 8)
        dawn_glucose = median_curve[dawn_mask]
        dawn_hours = hours[dawn_mask]
        results['dawn_curve_slope'] = np.polyfit(dawn_hours, dawn_glucose, 1)[0]
        
        # 9. æ—©æ™¨å³°å€¼é«˜åº¦ (6-10ç‚¹)
        morning_mask = (hours >= 6) & (hours <= 10)
        baseline = np.mean(median_curve[(hours >= 0) & (hours <= 6)])
        results['morning_peak_height'] = np.max(median_curve[morning_mask]) - baseline
        
        # 10. æ—©æ™¨å³°å€¼æŒç»­æ—¶é—´
        morning_peak_idx = np.argmax(median_curve[morning_mask])
        peak_value = median_curve[morning_mask][morning_peak_idx]
        half_peak = (peak_value + baseline) / 2
        # ç®€åŒ–å®ç°ï¼šä¼°ç®—åŠé«˜å®½
        results['morning_peak_width'] = 120  # é»˜è®¤120åˆ†é’Ÿï¼Œå®é™…éœ€è¦æ›´å¤æ‚çš„è®¡ç®—
        
        # 11. åˆåæ›²çº¿ç¨³å®šæ€§ (14-18ç‚¹)
        afternoon_mask = (hours >= 14) & (hours <= 18)
        afternoon_glucose = median_curve[afternoon_mask]
        results['afternoon_curve_stability'] = 1 - (np.std(afternoon_glucose) / np.mean(afternoon_glucose))
        
        # 12. æ™šé—´è¡€ç³–å‡é«˜å¹…åº¦ (18-22ç‚¹)
        evening_mask = (hours >= 18) & (hours <= 22)
        pre_evening_mask = (hours >= 17) & (hours <= 18)
        evening_max = np.max(median_curve[evening_mask])
        pre_evening_min = np.min(median_curve[pre_evening_mask])
        results['evening_surge_magnitude'] = evening_max - pre_evening_min
        
        # 13. å¤œé—´æ›²çº¿å¹³å¦åº¦ (22-6ç‚¹)
        night_mask = (hours >= 22) | (hours <= 6)
        night_glucose = median_curve[night_mask]
        results['nocturnal_curve_flatness'] = 1 - (np.std(night_glucose) / np.mean(night_glucose))
        
        return results
    
    def _analyze_percentile_bands(self, processed_data: Dict) -> Dict:
        """åˆ†ä½æ•°å¸¦åˆ†æ (æŒ‡æ ‡14-18)"""
        agp = processed_data['agp_curve']
        
        results = {}
        
        # 14. 25%åˆ†ä½çº¿æ¨¡å¼æè¿°
        p25_stability = np.std(agp['p25']) / np.mean(agp['p25'])
        results['percentile_25_pattern'] = "å¹³ç¨³" if p25_stability < 0.3 else "æ³¢åŠ¨"
        
        # 15. 75%åˆ†ä½çº¿æ¨¡å¼æè¿°
        p75_trend = np.polyfit(agp['hour'], agp['p75'], 1)[0]
        results['percentile_75_pattern'] = "ä¸Šå‡è¶‹åŠ¿" if p75_trend > 0.1 else "ç¨³å®š"
        
        # 16. åˆ†ä½æ•°å¸¦å®½å˜å¼‚ç³»æ•° (é‡å‘½åä»¥é¿å…ä¸è¡€ç³–CVæ··æ·†)
        band_width = agp['p75'] - agp['p25']
        results['percentile_band_cv'] = np.std(band_width) / np.mean(band_width) * 100
        
        # æ–°å¢: çœŸæ­£çš„è¡€ç³–å˜å¼‚ç³»æ•°(CV)
        raw_data = processed_data['raw_data']
        glucose_cv = (raw_data['glucose'].std() / raw_data['glucose'].mean()) * 100
        results['glucose_coefficient_of_variation'] = glucose_cv
        
        # ä¿æŒåŸæ¥çš„åˆ†ä½æ•°å¸¦å®½å˜å¼‚æ€§æŒ‡æ ‡ï¼ˆæœ‰ç‹¬ç«‹ä¸´åºŠä»·å€¼ï¼‰
        results['percentile_spread_variability'] = results['percentile_band_cv']
        
        # 17. ç›®æ ‡èŒƒå›´å†…åˆ†ä½æ•°å¸¦è¦†ç›–åº¦
        target_mask = (agp['p25'] >= 3.9) & (agp['p75'] <= 10.0)
        results['target_range_coverage'] = np.sum(target_mask) / len(target_mask) * 100
        
        # 18. åˆ†ä½æ•°å¸¦å¹³æ»‘åº¦
        iqr_width = agp['p75'] - agp['p25']
        results['quantile_band_smoothness'] = 1 - (np.var(iqr_width) / np.mean(iqr_width)**2)
        
        return results
    
    def _analyze_abnormal_patterns(self, processed_data: Dict) -> Dict:
        """å¼‚å¸¸æ¨¡å¼è¯†åˆ« (æŒ‡æ ‡19-23)"""
        raw_data = processed_data['raw_data']
        
        results = {}
        
        # 19. æ€¥å‰§ä¸Šå‡é¢‘ç‡
        glucose_diff = raw_data['glucose'].diff()
        time_diff = raw_data['timestamp'].diff().dt.total_seconds() / 3600  # å°æ—¶
        spike_rate = glucose_diff / time_diff
        results['curve_spike_frequency'] = np.sum(spike_rate > 5) / processed_data['analysis_period']
        
        # 20. æ€¥å‰§ä¸‹é™é¢‘ç‡
        results['curve_drop_frequency'] = np.sum(spike_rate < -3) / processed_data['analysis_period']
        
        # 21. é«˜å¹³å°æ€»æ—¶é•¿
        high_glucose_mask = raw_data['glucose'] > 13
        time_diffs = raw_data['timestamp'].diff().dropna()
        sampling_interval = pd.Timedelta(np.median(time_diffs)).total_seconds() / 3600
        results['plateau_duration_total'] = np.sum(high_glucose_mask) * sampling_interval
        
        # 22. è¡€ç³–æŒ¯è¡å¹…åº¦
        results['oscillation_amplitude'] = raw_data['glucose'].max() - raw_data['glucose'].min()
        
        # 23. åŸºçº¿æ¼‚ç§»æ–œç‡
        # ç®€åŒ–ï¼šä½¿ç”¨çº¿æ€§å›å½’æ£€æµ‹æ€»ä½“è¶‹åŠ¿
        time_numeric = (raw_data['timestamp'] - raw_data['timestamp'].iloc[0]).dt.total_seconds() / 86400
        results['baseline_drift_slope'] = np.polyfit(time_numeric, raw_data['glucose'], 1)[0]
        
        return results
    
    def _analyze_meal_patterns(self, processed_data: Dict) -> Dict:
        """é¤æ—¶æ¨¡å¼åˆ†æ (æŒ‡æ ‡24-27)"""
        agp = processed_data['agp_curve']
        hours = agp['hour']
        median_curve = agp['p50']
        
        results = {}
        
        # 24. ä¸‰é¤å³°å€¼ä¸€è‡´æ€§è¯„åˆ†
        breakfast_peak = np.max(median_curve[(hours >= 7) & (hours <= 9)])
        lunch_peak = np.max(median_curve[(hours >= 12) & (hours <= 14)])
        dinner_peak = np.max(median_curve[(hours >= 18) & (hours <= 20)])
        
        meal_peaks = [breakfast_peak, lunch_peak, dinner_peak]
        results['meal_peak_consistency'] = 1 - (np.std(meal_peaks) / np.mean(meal_peaks))
        
        # 25. é¤åæ¢å¤é€Ÿç‡å¹³å‡å€¼
        # ç®€åŒ–å®ç°ï¼šè®¡ç®—é¤å2-4å°æ—¶çš„ä¸‹é™é€Ÿç‡
        post_breakfast = np.mean(np.diff(median_curve[(hours >= 9) & (hours <= 11)]))
        post_lunch = np.mean(np.diff(median_curve[(hours >= 14) & (hours <= 16)]))
        post_dinner = np.mean(np.diff(median_curve[(hours >= 20) & (hours <= 22)]))
        
        results['postprandial_recovery_rate'] = np.mean([post_breakfast, post_lunch, post_dinner])
        
        # 26. é¤å‰è¡€ç³–ä¸‹é™æ¨¡å¼
        pre_meal_dips = [
            np.min(median_curve[(hours >= 6.5) & (hours <= 7)]) < np.mean(median_curve[(hours >= 5) & (hours <= 6)]),
            np.min(median_curve[(hours >= 11.5) & (hours <= 12)]) < np.mean(median_curve[(hours >= 10) & (hours <= 11)]),
            np.min(median_curve[(hours >= 17.5) & (hours <= 18)]) < np.mean(median_curve[(hours >= 16) & (hours <= 17)])
        ]
        results['pre_meal_dip_pattern'] = np.sum(pre_meal_dips) >= 2
        
        # 27. é¤åååº”å˜å¼‚ç³»æ•°
        meal_responses = [breakfast_peak - np.mean(median_curve[(hours >= 5) & (hours <= 7)]),
                         lunch_peak - np.mean(median_curve[(hours >= 10) & (hours <= 12)]),
                         dinner_peak - np.mean(median_curve[(hours >= 16) & (hours <= 18)])]
        results['meal_response_variability'] = np.std(meal_responses) / np.mean(meal_responses) * 100
        
        return results
    
    def _analyze_curve_complexity(self, processed_data: Dict) -> Dict:
        """æ›²çº¿å¤æ‚åº¦åˆ†æ (æŒ‡æ ‡28-32)"""
        median_curve = processed_data['agp_curve']['p50']
        
        results = {}
        
        # 28. AGPæ›²çº¿åˆ†å½¢ç»´æ•° (ç®€åŒ–å®ç°)
        # ä½¿ç”¨ç›’å­è®¡æ•°æ³•çš„ç®€åŒ–ç‰ˆæœ¬
        def box_count_dimension(curve, max_scale=10):
            scales = np.logspace(0, np.log10(max_scale), 10)
            counts = []
            for scale in scales:
                # ç®€åŒ–çš„ç›’å­è®¡æ•°
                boxes = int(len(curve) / scale)
                count = len(np.unique((curve * boxes / np.max(curve)).astype(int)))
                counts.append(count)
            # è®¡ç®—åˆ†å½¢ç»´æ•°
            log_scales = np.log(1/scales)
            log_counts = np.log(counts)
            return -np.polyfit(log_scales, log_counts, 1)[0]
        
        results['fractal_dimension'] = box_count_dimension(median_curve)
        
        # 29. æ›²çº¿è½¬æŠ˜ç‚¹é¢‘ç‡
        def count_turning_points(curve):
            diff1 = np.diff(curve)
            diff_sign = np.sign(diff1)
            sign_changes = np.sum(np.diff(diff_sign) != 0)
            return sign_changes
        
        results['turning_point_frequency'] = count_turning_points(median_curve)
        
        # 30. æ›²çº¿ç²—ç³™åº¦æŒ‡æ•°
        second_derivative = np.diff(median_curve, n=2)
        results['curve_roughness_index'] = np.mean(np.abs(second_derivative))
        
        # 31. è¡€ç³–è‡ªç›¸å…³è¡°å‡ç³»æ•°
        autocorr = np.corrcoef(median_curve[:-1], median_curve[1:])[0,1]
        results['autocorrelation_decay'] = autocorr
        
        # 32. è¿‘ä¼¼ç†µ
        def approximate_entropy(data, m=2, r=None):
            if r is None:
                r = 0.2 * np.std(data)
            
            def _maxdist(xi, xj, m):
                return max([abs(ua - va) for ua, va in zip(xi, xj)])
            
            def _phi(m):
                patterns = np.array([data[i:i+m] for i in range(len(data) - m + 1)])
                phi = 0
                for i in range(len(patterns)):
                    template = patterns[i]
                    matches = sum([1 for pattern in patterns if _maxdist(template, pattern, m) <= r])
                    if matches > 0:
                        phi += np.log(matches / len(patterns))
                return phi / len(patterns)
            
            return _phi(m) - _phi(m + 1)
        
        results['approximate_entropy'] = approximate_entropy(median_curve)
        
        return results
    
    def _analyze_target_range(self, processed_data: Dict) -> Dict:
        """ç›®æ ‡èŒƒå›´åˆ†æ (æŒ‡æ ‡33-36)"""
        raw_data = processed_data['raw_data']
        glucose = raw_data['glucose'].values
        
        results = {}
        
        # 33. è¿›å…¥ç›®æ ‡èŒƒå›´é¢‘ç‡
        in_range = (glucose >= 3.9) & (glucose <= 10.0)
        entries = np.sum(np.diff(in_range.astype(int)) == 1)
        results['target_range_entry_frequency'] = entries / processed_data['analysis_period']
        
        # 34. ç¦»å¼€ç›®æ ‡èŒƒå›´é¢‘ç‡
        exits = np.sum(np.diff(in_range.astype(int)) == -1)
        results['target_range_exit_frequency'] = exits / processed_data['analysis_period']
        
        # 35. ç›®æ ‡èŒƒå›´å†…å¹³å‡åœç•™æ—¶é—´
        # è®¡ç®—è¿ç»­åœ¨ç›®æ ‡èŒƒå›´å†…çš„æ—¶é—´æ®µ
        in_range_segments = []
        start = None
        for i, in_target in enumerate(in_range):
            if in_target and start is None:
                start = i
            elif not in_target and start is not None:
                in_range_segments.append(i - start)
                start = None
        
        if in_range_segments:
            time_diffs = raw_data['timestamp'].diff().dropna()
            avg_interval = pd.Timedelta(np.median(time_diffs)).total_seconds() / 60  # åˆ†é’Ÿ
            results['target_range_dwell_time_avg'] = np.mean(in_range_segments) * avg_interval
        else:
            results['target_range_dwell_time_avg'] = 0
        
        # 36. èŒƒå›´è½¬æ¢å¹³æ»‘åº¦
        # ç®€åŒ–ï¼šè®¡ç®—èŒƒå›´è½¬æ¢æ—¶çš„è¡€ç³–å˜åŒ–å¹³æ»‘ç¨‹åº¦
        transition_points = np.where(np.diff(in_range.astype(int)) != 0)[0]
        if len(transition_points) > 0:
            transition_glucose = glucose[transition_points]
            smooth_transitions = signal.savgol_filter(transition_glucose, 
                                                    min(5, len(transition_glucose)), 1)
            results['range_transition_smoothness'] = 1 - np.mean(np.abs(transition_glucose - smooth_transitions))
        else:
            results['range_transition_smoothness'] = 1.0
        
        return results
    
    def _analyze_risk_zones(self, processed_data: Dict) -> Dict:
        """é£é™©åŒºåŸŸåˆ†æ (æŒ‡æ ‡37-40)"""
        raw_data = processed_data['raw_data']
        glucose = raw_data['glucose'].values
        
        results = {}
        
        # 37. ä½è¡€ç³–åŒºåŸŸæœ€å¤§æ·±åº¦
        hypo_glucose = glucose[glucose < 3.9]
        results['hypoglycemia_zone_depth'] = np.min(hypo_glucose) if len(hypo_glucose) > 0 else 3.9
        
        # 38. é«˜è¡€ç³–åŒºåŸŸæœ€å¤§é«˜åº¦
        hyper_glucose = glucose[glucose > 10.0]
        results['hyperglycemia_zone_height'] = np.max(hyper_glucose) if len(hyper_glucose) > 0 else 10.0
        
        # 39. é£é™©åŒºåŸŸèšé›†æ•°é‡
        risk_zones = (glucose < 3.9) | (glucose > 13.9)
        # è®¡ç®—è¿ç»­é£é™©åŒºåŸŸçš„æ•°é‡
        risk_clusters = 0
        in_cluster = False
        for is_risk in risk_zones:
            if is_risk and not in_cluster:
                risk_clusters += 1
                in_cluster = True
            elif not is_risk:
                in_cluster = False
        results['risk_zone_cluster_count'] = risk_clusters
        
        # 40. å®‰å…¨åŒºåŸŸè¿ç»­æ€§
        safe_zones = (glucose >= 3.9) & (glucose <= 13.9)
        # è®¡ç®—æœ€é•¿è¿ç»­å®‰å…¨æ—¶é—´
        max_safe_duration = 0
        current_duration = 0
        for is_safe in safe_zones:
            if is_safe:
                current_duration += 1
            else:
                max_safe_duration = max(max_safe_duration, current_duration)
                current_duration = 0
        max_safe_duration = max(max_safe_duration, current_duration)
        results['safe_zone_continuity'] = max_safe_duration / len(safe_zones)
        
        return results
    
    def _analyze_daily_variability(self, processed_data: Dict) -> Dict:
        """æ—¥é—´å˜å¼‚åˆ†æ (æŒ‡æ ‡41-44)"""
        raw_data = processed_data['raw_data']
        
        results = {}
        
        # æŒ‰æ—¥æœŸåˆ†ç»„åˆ†æ
        raw_data['date'] = raw_data['timestamp'].dt.date
        daily_patterns = []
        
        for date, group in raw_data.groupby('date'):
            if len(group) >= 24:  # ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®
                # åˆ›å»º24å°æ—¶æ¨¡å¼
                group['hour'] = group['timestamp'].dt.hour + group['timestamp'].dt.minute / 60.0
                hourly_avg = group.groupby(group['hour'].round())['glucose'].mean()
                if len(hourly_avg) >= 20:  # è‡³å°‘20å°æ—¶æ•°æ®
                    daily_patterns.append(hourly_avg.reindex(range(24), fill_value=np.nan))
        
        if len(daily_patterns) >= 2:
            daily_patterns_df = pd.DataFrame(daily_patterns)
            
            # 41. æ—¥é—´æ¨¡å¼ç›¸ä¼¼åº¦
            correlations = []
            for i in range(len(daily_patterns_df)):
                for j in range(i+1, len(daily_patterns_df)):
                    corr = daily_patterns_df.iloc[i].corr(daily_patterns_df.iloc[j])
                    if not np.isnan(corr):
                        correlations.append(corr)
            results['intraday_pattern_similarity'] = np.mean(correlations) if correlations else 0
            
            # 42. æ¸…æ™¨æ¨¡å¼ä¸€è‡´æ€§ (6-10ç‚¹)
            morning_patterns = daily_patterns_df.iloc[:, 6:11]
            morning_corr = morning_patterns.T.corr().values
            results['morning_pattern_consistency'] = np.mean(morning_corr[np.triu_indices_from(morning_corr, k=1)])
            
            # 43. å‚æ™šæ¨¡å¼ä¸€è‡´æ€§ (17-21ç‚¹)
            evening_patterns = daily_patterns_df.iloc[:, 17:22]
            evening_corr = evening_patterns.T.corr().values
            results['evening_pattern_consistency'] = np.mean(evening_corr[np.triu_indices_from(evening_corr, k=1)])
            
            # 44. å‘¨æœ«æ¨¡å¼åå·®åº¦
            raw_data['is_weekend'] = raw_data['timestamp'].dt.dayofweek >= 5
            weekend_avg = raw_data[raw_data['is_weekend']]['glucose'].mean()
            weekday_avg = raw_data[~raw_data['is_weekend']]['glucose'].mean()
            results['weekend_pattern_deviation'] = abs(weekend_avg - weekday_avg)
        else:
            # æ•°æ®ä¸è¶³ï¼Œä½¿ç”¨é»˜è®¤å€¼
            results.update({
                'intraday_pattern_similarity': 0.5,
                'morning_pattern_consistency': 0.5,
                'evening_pattern_consistency': 0.5,
                'weekend_pattern_deviation': 0
            })
        
        return results
    
    def _analyze_comprehensive_smoothness(self, processed_data: Dict) -> Dict:
        """å¹³æ»‘åº¦ç»¼åˆåˆ†æ (æŒ‡æ ‡45-48)"""
        median_curve = processed_data['agp_curve']['p50']
        
        results = {}
        
        # 45. ç»¼åˆå¹³æ»‘åº¦è¯„åˆ†
        first_order = 1 - np.std(np.diff(median_curve)) / np.mean(np.abs(np.diff(median_curve)))
        second_order = 1 / (1 + np.var(np.diff(median_curve, n=2)))
        
        # é¢‘åŸŸå¹³æ»‘åº¦
        fft_vals = fft(median_curve)
        frequencies = fftfreq(len(median_curve))
        power_spectrum = np.abs(fft_vals)**2
        high_freq_power = np.sum(power_spectrum[np.abs(frequencies) > 0.1])
        total_power = np.sum(power_spectrum)
        freq_smoothness = 1 - high_freq_power / total_power
        
        results['comprehensive_smoothness_score'] = (0.4 * first_order + 
                                                   0.3 * second_order + 
                                                   0.3 * freq_smoothness)
        
        # 46. ç§»åŠ¨å¹³å‡å¹³æ»‘åº¦
        window_size = max(3, len(median_curve) // 10)
        moving_avg = pd.Series(median_curve).rolling(window=window_size, center=True).mean().bfill().ffill()
        results['moving_average_smoothness'] = np.corrcoef(median_curve, moving_avg)[0,1]
        
        # 47. é¢‘è°±å¹³æ»‘åº¦
        results['spectral_smoothness'] = 1 - high_freq_power / total_power
        
        # 48. è¡€ç³–å¹³æ»‘æŒ‡æ•° (åŸºäºMAGE)
        # ç®€åŒ–çš„MAGEè®¡ç®—
        peaks, _ = signal.find_peaks(median_curve)
        valleys, _ = signal.find_peaks(-median_curve)
        all_extremes = np.sort(np.concatenate([peaks, valleys]))
        
        if len(all_extremes) >= 4:
            excursions = []
            for i in range(len(all_extremes)-1):
                excursion = abs(median_curve[all_extremes[i+1]] - median_curve[all_extremes[i]])
                if excursion >= np.std(median_curve):
                    excursions.append(excursion)
            mage = np.mean(excursions) if excursions else 0
        else:
            mage = 0
            
        results['glucose_smoothness_index'] = 1 - (mage / np.mean(median_curve)) if np.mean(median_curve) > 0 else 0
        
        return results
    
    def _analyze_symmetry(self, processed_data: Dict) -> Dict:
        """å¯¹ç§°æ€§åˆ†æ (æŒ‡æ ‡49-52)"""
        median_curve = processed_data['agp_curve']['p50']
        hours = processed_data['agp_curve']['hour']
        
        results = {}
        
        # 49. æ›²çº¿å¯¹ç§°æ€§æŒ‡æ•° (å·²åœ¨æ›²çº¿å½¢æ€ä¸­è®¡ç®—ï¼Œè¿™é‡Œå†æ¬¡è®¡ç®—ä»¥ä¿æŒå®Œæ•´æ€§)
        first_half = median_curve[:48]
        second_half = median_curve[48:][::-1]
        results['curve_symmetry_index'] = np.corrcoef(first_half, second_half)[0,1]
        
        # 50. é¤æ—¶å¯¹ç§°æ€§æŒ‡æ•°
        breakfast_pattern = median_curve[(hours >= 7) & (hours <= 9)]
        dinner_pattern = median_curve[(hours >= 19) & (hours <= 21)]
        if len(breakfast_pattern) == len(dinner_pattern):
            results['meal_symmetry_index'] = np.corrcoef(breakfast_pattern, dinner_pattern)[0,1]
        else:
            # æ’å€¼åˆ°ç›¸åŒé•¿åº¦
            breakfast_interp = np.interp(np.linspace(0, 1, 10), 
                                       np.linspace(0, 1, len(breakfast_pattern)), 
                                       breakfast_pattern)
            dinner_interp = np.interp(np.linspace(0, 1, 10), 
                                    np.linspace(0, 1, len(dinner_pattern)), 
                                    dinner_pattern)
            results['meal_symmetry_index'] = np.corrcoef(breakfast_interp, dinner_interp)[0,1]
        
        # 51. å‘¨æ¨¡å¼å¯¹ç§°æ€§æŒ‡æ•° (ç®€åŒ–å®ç°)
        # ç”±äºå•æ¬¡åˆ†æå¯èƒ½æ²¡æœ‰å¤šå‘¨æ•°æ®ï¼Œä½¿ç”¨AGPæ›²çº¿çš„å‘¨æœŸæ€§
        results['weekly_symmetry_index'] = 0.7  # é»˜è®¤å€¼ï¼Œå®é™…éœ€è¦å¤šå‘¨æ•°æ®
        
        # 52. æ˜¼å¤œå¯¹ç§°æ€§æŒ‡æ•°
        morning_rise = median_curve[(hours >= 6) & (hours <= 10)]
        evening_fall = median_curve[(hours >= 20) & (hours <= 24)]
        
        # è®¡ç®—ä¸Šå‡å’Œä¸‹é™çš„å¯¹ç§°æ€§
        if len(morning_rise) > 0 and len(evening_fall) > 0:
            morning_slope = np.polyfit(range(len(morning_rise)), morning_rise, 1)[0]
            evening_slope = np.polyfit(range(len(evening_fall)), evening_fall, 1)[0]
            results['circadian_symmetry_index'] = 1 - abs(morning_slope + evening_slope) / max(abs(morning_slope), abs(evening_slope), 1)
        else:
            results['circadian_symmetry_index'] = 0.5
        
        return results
    
    def _analyze_visual_aesthetics(self, processed_data: Dict) -> Dict:
        """å›¾å½¢ç¾å­¦åˆ†æ (æŒ‡æ ‡53-55)"""
        median_curve = processed_data['agp_curve']['p50']
        agp = processed_data['agp_curve']
        
        results = {}
        
        # 53. æ›²çº¿ä¼˜é›…åº¦è¯„åˆ†
        # ç»¼åˆå¹³æ»‘æ€§ã€å¯¹ç§°æ€§å’Œç®€æ´æ€§
        smoothness = results.get('comprehensive_smoothness_score', 0.5)
        symmetry = results.get('curve_symmetry_index', 0.5)
        
        # ç®€æ´æ€§ï¼šè½¬æŠ˜ç‚¹å¯†åº¦çš„å€’æ•°
        turning_points = results.get('turning_point_frequency', 10)
        simplicity = 1 / (1 + turning_points / 10)
        
        results['curve_elegance_score'] = (0.4 * smoothness + 0.3 * symmetry + 0.3 * simplicity)
        
        # 54. è§†è§‰å¤æ‚åº¦æŒ‡æ•°
        # åŸºäºç›´æ–¹å›¾ç†µå’Œè½¬æŠ˜ç‚¹å¯†åº¦
        hist, _ = np.histogram(median_curve, bins=20)
        hist = hist / np.sum(hist)  # æ ‡å‡†åŒ–
        entropy = -np.sum(hist * np.log(hist + 1e-10))
        turning_point_density = turning_points / len(median_curve)
        results['visual_complexity_index'] = 0.5 * entropy / np.log(20) + 0.5 * turning_point_density
        
        # 55. é¢œè‰²åŒºåŸŸå¹³è¡¡åº¦
        # åŸºäºTIR, TAR, TBRçš„å¹³è¡¡æ€§
        raw_data = processed_data['raw_data']
        glucose = raw_data['glucose']
        
        tir = np.sum((glucose >= 3.9) & (glucose <= 10.0)) / len(glucose)
        tbr = np.sum(glucose < 3.9) / len(glucose)
        tar = np.sum(glucose > 10.0) / len(glucose)
        
        # ç†æƒ³åˆ†å¸ƒï¼šTIR=0.7, TAR=0.25, TBR=0.05
        ideal_dist = np.array([0.05, 0.7, 0.25])
        actual_dist = np.array([tbr, tir, tar])
        
        # è®¡ç®—åˆ†å¸ƒçš„å‡è¡¡æ€§ï¼ˆè¶Šæ¥è¿‘ç†æƒ³åˆ†å¸ƒè¶Šå¥½ï¼‰
        balance_score = 1 - np.sqrt(np.sum((ideal_dist - actual_dist)**2)) / np.sqrt(np.sum(ideal_dist**2))
        results['color_zone_balance'] = max(0, balance_score)
        
        return results


class AGPIntelligentReporter:
    """AGPæ™ºèƒ½æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.analysis_results = None
        
    def generate_intelligent_report(self, analysis_results: Dict, patient_info: Dict = None) -> Dict:
        """
        ç”Ÿæˆæ™ºèƒ½AGPåˆ†ææŠ¥å‘Š
        
        Args:
            analysis_results: AGPè§†è§‰åˆ†æç»“æœ
            patient_info: æ‚£è€…ä¿¡æ¯
            
        Returns:
            ç»“æ„åŒ–çš„æ™ºèƒ½åˆ†ææŠ¥å‘Š
        """
        self.analysis_results = analysis_results
        
        report = {
            'patient_info': patient_info or {},
            'analysis_timestamp': datetime.now().isoformat(),
            'overall_assessment': self._generate_overall_assessment(),
            'key_findings': self._generate_key_findings(),
            'risk_alerts': self._generate_risk_alerts(),
            'clinical_recommendations': self._generate_clinical_recommendations(),
            'detailed_analysis': self._generate_detailed_analysis(),
            'trending': self._generate_trending_analysis(),
            'patient_education': self._generate_patient_education(),
            'technical_metrics': analysis_results
        }
        
        return report
    
    def _generate_overall_assessment(self) -> Dict:
        """ç”Ÿæˆæ•´ä½“è¯„ä¼°"""
        # åŸºäºå¤šä¸ªæŒ‡æ ‡ç”Ÿæˆç»¼åˆè¯„åˆ†
        smoothness_score = self.analysis_results.get('comprehensive_smoothness_score', 0.5)
        symmetry_score = self.analysis_results.get('curve_symmetry_index', 0.5)
        elegance_score = self.analysis_results.get('curve_elegance_score', 0.5)
        
        overall_score = (0.4 * smoothness_score + 0.3 * symmetry_score + 0.3 * elegance_score) * 100
        
        if overall_score >= 85:
            level = "ä¼˜ç§€"
            description = "è¡€ç³–æ§åˆ¶ç¨³å®šï¼ŒAGPæ›²çº¿ä¼˜é›…å¹³æ»‘"
        elif overall_score >= 70:
            level = "è‰¯å¥½"  
            description = "è¡€ç³–æ§åˆ¶åŸºæœ¬ç¨³å®šï¼Œå­˜åœ¨æ”¹å–„ç©ºé—´"
        elif overall_score >= 55:
            level = "ä¸€èˆ¬"
            description = "è¡€ç³–æ§åˆ¶ä¸å¤Ÿç¨³å®šï¼Œéœ€è¦ä¼˜åŒ–æ²»ç–—"
        else:
            level = "éœ€è¦æ”¹å–„"
            description = "è¡€ç³–æ§åˆ¶ä¸ä½³ï¼Œéœ€è¦é‡æ–°è¯„ä¼°æ²»ç–—æ–¹æ¡ˆ"
        
        return {
            'overall_score': round(overall_score, 1),
            'level': level,
            'description': description,
            'data_quality': "ä¼˜ç§€" if self.analysis_results.get('percentile_25_pattern') != "æ³¢åŠ¨" else "è‰¯å¥½"
        }
    
    def _generate_key_findings(self) -> List[Dict]:
        """ç”Ÿæˆå…³é”®å‘ç°"""
        findings = []
        
        # é»æ˜ç°è±¡æ£€æµ‹ (é™ä½é˜ˆå€¼ä»¥ä¾¿æ›´å¥½æ£€æµ‹)
        dawn_slope = self.analysis_results.get('dawn_curve_slope', 0)
        if abs(dawn_slope) > 0.5:  # é™ä½é˜ˆå€¼
            if dawn_slope > 0.5:
                findings.append({
                    'type': 'dawn_phenomenon',
                    'severity': 'moderate' if dawn_slope < 1.5 else 'severe',
                    'description': f"æ£€æµ‹åˆ°æ˜æ˜¾é»æ˜ç°è±¡ï¼Œè¡€ç³–ä¸Šå‡é€Ÿç‡{dawn_slope:.1f}mmol/L/h",
                    'clinical_significance': 'æç¤ºåŸºç¡€èƒ°å²›ç´ å‰‚é‡æˆ–æ—¶æœºéœ€è¦è°ƒæ•´'
                })
            else:
                findings.append({
                    'type': 'reverse_dawn_phenomenon', 
                    'severity': 'moderate',
                    'description': f"æ£€æµ‹åˆ°åå‘é»æ˜ç°è±¡ï¼Œè¡€ç³–ä¸‹é™é€Ÿç‡{abs(dawn_slope):.1f}mmol/L/h",
                    'clinical_significance': 'å¯èƒ½æç¤ºåŸºç¡€èƒ°å²›ç´ è¿‡é‡æˆ–å‡Œæ™¨èƒ°å²›ç´ æ•æ„Ÿæ€§å¢åŠ '
                })
        
        # é¤åæ§åˆ¶è¯„ä¼° (é™ä½é˜ˆå€¼)
        morning_peak = self.analysis_results.get('morning_peak_height', 0)
        if morning_peak > 3.0:  # é™ä½é˜ˆå€¼
            findings.append({
                'type': 'postprandial_hyperglycemia',
                'severity': 'mild' if morning_peak < 5 else 'moderate' if morning_peak < 8 else 'severe',
                'description': f"æ—©é¤åè¡€ç³–å‡é«˜{morning_peak:.1f}mmol/Lï¼Œæ§åˆ¶éœ€è¦æ”¹å–„",
                'clinical_significance': 'å»ºè®®ä¼˜åŒ–é€Ÿæ•ˆèƒ°å²›ç´ å‰‚é‡æˆ–èƒ°å²›ç´ ç¢³æ°´åŒ–åˆç‰©æ¯”ä¾‹'
            })
        
        # å¤œé—´ç¨³å®šæ€§
        nocturnal_flatness = self.analysis_results.get('nocturnal_curve_flatness', 1.0)
        if nocturnal_flatness < 0.85:  # ç¨å¾®æé«˜æ ‡å‡†
            findings.append({
                'type': 'nocturnal_instability',
                'severity': 'mild' if nocturnal_flatness > 0.7 else 'moderate' if nocturnal_flatness > 0.5 else 'severe',
                'description': f"å¤œé—´è¡€ç³–ç¨³å®šæ€§å¾…æ”¹å–„ï¼Œå¹³å¦åº¦{nocturnal_flatness:.2f}",
                'clinical_significance': 'å¯èƒ½å­˜åœ¨å¤œé—´ä½è¡€ç³–é£é™©æˆ–åŸºç¡€èƒ°å²›ç´ ä½œç”¨ä¸è¶³'
            })
        
        # è¡€ç³–å˜å¼‚æ€§åˆ†æ (æ•´ä½“è¡€ç³–ç¨³å®šæ€§)
        glucose_cv = self.analysis_results.get('glucose_coefficient_of_variation', 30)
        if glucose_cv > 36:  # ADAæ ‡å‡†
            findings.append({
                'type': 'high_glucose_variability',
                'severity': 'mild' if glucose_cv < 50 else 'moderate' if glucose_cv < 70 else 'severe',
                'description': f"æ•´ä½“è¡€ç³–å˜å¼‚æ€§åé«˜ï¼Œå˜å¼‚ç³»æ•°{glucose_cv:.1f}%",
                'clinical_significance': 'æç¤ºè¡€ç³–æ§åˆ¶ä¸å¤Ÿç¨³å®šï¼Œéœ€è¯„ä¼°æ²»ç–—ä¾ä»æ€§å’Œè¡€ç³–ç®¡ç†ç­–ç•¥'
            })
        
        # æ—¶é—´æ¨¡å¼å˜å¼‚æ€§åˆ†æ (æ˜¼å¤œèŠ‚å¾‹ç¨³å®šæ€§)
        band_cv = self.analysis_results.get('percentile_band_cv', 30)
        if band_cv > 40:  # æ—¶é—´æ¨¡å¼å˜å¼‚é˜ˆå€¼
            findings.append({
                'type': 'temporal_pattern_variability',
                'severity': 'mild' if band_cv < 60 else 'moderate',
                'description': f"æ˜¼å¤œè¡€ç³–æ¨¡å¼å˜å¼‚è¾ƒå¤§ï¼Œåˆ†ä½æ•°å¸¦å˜å¼‚{band_cv:.1f}%",
                'clinical_significance': 'æç¤ºè¡€ç³–æ˜¼å¤œèŠ‚å¾‹ä¸å¤Ÿç¨³å®šï¼Œä¸åŒæ—¶æ®µè¡€ç³–åˆ†å¸ƒå·®å¼‚è¾ƒå¤§'
            })
        
        # å¢åŠ TIRç›¸å…³å‘ç°
        tir_percentage = self.analysis_results.get('target_range_coverage', 70)
        if tir_percentage < 70:
            findings.append({
                'type': 'low_tir',
                'severity': 'mild' if tir_percentage > 60 else 'moderate' if tir_percentage > 50 else 'severe',
                'description': f"ç›®æ ‡èŒƒå›´å†…æ—¶é—´{tir_percentage:.1f}%ï¼Œä½äº70%æ ‡å‡†",
                'clinical_significance': 'ADAæŒ‡å—å»ºè®®TIRåº”>70%ï¼Œéœ€è¦ä¼˜åŒ–è¡€ç³–ç®¡ç†ç­–ç•¥'
            })
        
        # æ›²çº¿å¹³æ»‘åº¦åˆ†æ
        smoothness = self.analysis_results.get('median_curve_smoothness', 0.5)
        if smoothness < 0.6:
            findings.append({
                'type': 'curve_roughness',
                'severity': 'mild' if smoothness > 0.4 else 'moderate',
                'description': f"AGPæ›²çº¿å¹³æ»‘åº¦åä½ï¼Œå¹³æ»‘æŒ‡æ•°{smoothness:.2f}",
                'clinical_significance': 'è¡€ç³–æ³¢åŠ¨è¾ƒå¤§ï¼Œæç¤ºéœ€è¦æ”¹å–„è¡€ç³–æ§åˆ¶çš„ç¨³å®šæ€§'
            })
        
        # å¯¹ç§°æ€§åˆ†æ
        symmetry = self.analysis_results.get('curve_symmetry_index', 0.5)
        if abs(symmetry) < 0.3:  # å¯¹ç§°æ€§å¤ªå·®
            findings.append({
                'type': 'asymmetric_pattern',
                'severity': 'mild',
                'description': f"è¡€ç³–æ¨¡å¼ä¸å¯¹ç§°ï¼Œå¯¹ç§°æŒ‡æ•°{symmetry:.2f}",
                'clinical_significance': 'å¯èƒ½æç¤ºç”Ÿæ´»ä½œæ¯ä¸è§„å¾‹æˆ–æ²»ç–—æ–¹æ¡ˆéœ€è¦ä¸ªæ€§åŒ–è°ƒæ•´'
            })
        
        return findings
    
    def _generate_risk_alerts(self) -> List[Dict]:
        """ç”Ÿæˆé£é™©è­¦æŠ¥"""
        alerts = []
        
        # ä½è¡€ç³–é£é™©
        hypo_depth = self.analysis_results.get('hypoglycemia_zone_depth', 4.0)
        if hypo_depth < 3.0:
            alerts.append({
                'type': 'severe_hypoglycemia_risk',
                'urgency': 'high',
                'message': f"æ£€æµ‹åˆ°ä¸¥é‡ä½è¡€ç³–ï¼Œæœ€ä½å€¼{hypo_depth:.1f}mmol/L",
                'action_required': 'ç«‹å³è¯„ä¼°èƒ°å²›ç´ å‰‚é‡ï¼Œè€ƒè™‘å‡é‡'
            })
        elif hypo_depth < 3.5:
            alerts.append({
                'type': 'hypoglycemia_risk',
                'urgency': 'medium',
                'message': f"å­˜åœ¨ä½è¡€ç³–é£é™©ï¼Œæœ€ä½å€¼{hypo_depth:.1f}mmol/L",
                'action_required': 'å»ºè®®é€‚å½“å‡å°‘èƒ°å²›ç´ å‰‚é‡'
            })
        
        # é«˜è¡€ç³–é£é™©
        hyper_height = self.analysis_results.get('hyperglycemia_zone_height', 10.0)
        if hyper_height > 20:
            alerts.append({
                'type': 'severe_hyperglycemia_risk',
                'urgency': 'high',
                'message': f"æ£€æµ‹åˆ°ä¸¥é‡é«˜è¡€ç³–ï¼Œæœ€é«˜å€¼{hyper_height:.1f}mmol/L",
                'action_required': 'ç«‹å³è¯„ä¼°æ²»ç–—æ–¹æ¡ˆï¼Œè€ƒè™‘å¢åŠ èƒ°å²›ç´ å‰‚é‡'
            })
        
        # è¡€ç³–çªå˜é£é™©
        spike_freq = self.analysis_results.get('curve_spike_frequency', 0)
        if spike_freq > 3:
            alerts.append({
                'type': 'glucose_instability',
                'urgency': 'medium',
                'message': f"è¡€ç³–æ€¥å‰§å˜åŒ–é¢‘ç¹ï¼Œæ¯æ—¥{spike_freq:.1f}æ¬¡",
                'action_required': 'è¯„ä¼°ç”¨è¯æ—¶æœºå’Œå‰‚é‡åˆ†é…'
            })
        
        return alerts
    
    def _generate_clinical_recommendations(self) -> List[Dict]:
        """ç”Ÿæˆä¸´åºŠå»ºè®®"""
        recommendations = []
        
        # åŸºäºåˆ†æç»“æœç”Ÿæˆä¸ªæ€§åŒ–å»ºè®®
        dawn_slope = self.analysis_results.get('dawn_curve_slope', 0)
        if abs(dawn_slope) > 0.5:  # é™ä½é˜ˆå€¼
            if dawn_slope > 0.5:
                recommendations.append({
                    'category': 'insulin_adjustment',
                    'priority': 'high' if dawn_slope > 1.0 else 'medium',
                    'recommendation': 'å»ºè®®è°ƒæ•´åŸºç¡€èƒ°å²›ç´ å‰‚é‡æˆ–æ³¨å°„æ—¶é—´',
                    'rationale': f'é»æ˜ç°è±¡æ£€æµ‹(æ–œç‡{dawn_slope:.1f}mmol/L/h)ï¼Œå¯èƒ½éœ€è¦ä¼˜åŒ–åŸºç¡€èƒ°å²›ç´ æ²»ç–—',
                    'follow_up': '1-2å‘¨åå¤æŸ¥AGPè¯„ä¼°æ•ˆæœ'
                })
            else:
                recommendations.append({
                    'category': 'insulin_adjustment', 
                    'priority': 'medium',
                    'recommendation': 'å»ºè®®è¯„ä¼°åŸºç¡€èƒ°å²›ç´ å‰‚é‡ï¼Œå¯èƒ½å­˜åœ¨è¿‡é‡',
                    'rationale': f'æ£€æµ‹åˆ°åå‘é»æ˜ç°è±¡(ä¸‹é™{abs(dawn_slope):.1f}mmol/L/h)ï¼Œæç¤ºå¯èƒ½èƒ°å²›ç´ è¿‡é‡',
                    'follow_up': 'å¯†åˆ‡ç›‘æµ‹è¡€ç³–å˜åŒ–ï¼Œé¿å…ä½è¡€ç³–'
                })
        
        # TIRä¼˜åŒ–å»ºè®®
        target_coverage = self.analysis_results.get('target_range_coverage', 80)
        if target_coverage < 70:
            recommendations.append({
                'category': 'treatment_optimization',
                'priority': 'high' if target_coverage < 50 else 'medium',
                'recommendation': 'éœ€è¦ä¼˜åŒ–æ•´ä½“è¡€ç³–ç®¡ç†ç­–ç•¥æé«˜TIR',
                'rationale': f'ç›®æ ‡èŒƒå›´å†…æ—¶é—´{target_coverage:.1f}%ä½äºADAæ¨èçš„70%æ ‡å‡†',
                'follow_up': 'å»ºè®®2-4å‘¨å†…é‡æ–°è¯„ä¼°ï¼Œå¿…è¦æ—¶è°ƒæ•´æ²»ç–—æ–¹æ¡ˆ'
            })
        elif target_coverage < 85:
            recommendations.append({
                'category': 'treatment_optimization',
                'priority': 'low',
                'recommendation': 'ç»§ç»­ä¼˜åŒ–è¡€ç³–ç®¡ç†ï¼Œäº‰å–æ›´é«˜çš„TIR',
                'rationale': f'å½“å‰TIR{target_coverage:.1f}%å·²è¾¾æ ‡ï¼Œä½†ä»æœ‰æå‡ç©ºé—´',
                'follow_up': 'ä¿æŒç°æœ‰æ²»ç–—æ–¹æ¡ˆï¼Œå®šæœŸç›‘æµ‹'
            })
        
        # é¤åè¡€ç³–å»ºè®®
        morning_peak = self.analysis_results.get('morning_peak_height', 0)
        if morning_peak > 3.0:
            recommendations.append({
                'category': 'meal_management',
                'priority': 'medium' if morning_peak < 6 else 'high',
                'recommendation': 'ä¼˜åŒ–é¤æ—¶èƒ°å²›ç´ ç®¡ç†æ”¹å–„é¤åè¡€ç³–',
                'rationale': f'æ—©é¤åè¡€ç³–å‡é«˜{morning_peak:.1f}mmol/Lï¼Œæç¤ºé¤æ—¶ç®¡ç†éœ€è¦æ”¹å–„',
                'follow_up': 'å»ºè®®è¥å…»å¸ˆå’¨è¯¢ï¼Œå­¦ä¹ ç¢³æ°´åŒ–åˆç‰©è®¡æ•°'
            })
        
        # æ•´ä½“è¡€ç³–å˜å¼‚æ€§å»ºè®®
        glucose_cv = self.analysis_results.get('glucose_coefficient_of_variation', 30)
        if glucose_cv > 36:
            recommendations.append({
                'category': 'glucose_control_optimization',
                'priority': 'medium',
                'recommendation': 'ä¼˜åŒ–æ•´ä½“è¡€ç³–æ§åˆ¶ç¨³å®šæ€§',
                'rationale': f'æ•´ä½“è¡€ç³–å˜å¼‚ç³»æ•°{glucose_cv:.1f}%åé«˜ï¼Œéœ€è¦æ”¹å–„è¡€ç³–æ§åˆ¶è´¨é‡',
                'follow_up': 'è¯„ä¼°æ²»ç–—æ–¹æ¡ˆï¼ŒåŠ å¼ºè¡€ç³–ç›‘æµ‹å’Œç®¡ç†'
            })
        
        # æ—¶é—´æ¨¡å¼å˜å¼‚æ€§å»ºè®®
        band_cv = self.analysis_results.get('percentile_band_cv', 30)
        if band_cv > 40:
            recommendations.append({
                'category': 'lifestyle_modification',
                'priority': 'medium',
                'recommendation': 'å»ºç«‹è§„å¾‹çš„ç”Ÿæ´»ä½œæ¯æ”¹å–„æ˜¼å¤œè¡€ç³–èŠ‚å¾‹',
                'rationale': f'æ˜¼å¤œè¡€ç³–æ¨¡å¼å˜å¼‚{band_cv:.1f}%åé«˜ï¼Œæç¤ºç”Ÿæ´»èŠ‚å¾‹ä¸å¤Ÿè§„å¾‹',
                'follow_up': 'å›ºå®šä½œæ¯æ—¶é—´ï¼Œè§„å¾‹é¥®é£Ÿå’Œè¿åŠ¨'
            })
        
        # å¤œé—´ç®¡ç†å»ºè®®
        nocturnal_flatness = self.analysis_results.get('nocturnal_curve_flatness', 1.0)
        if nocturnal_flatness < 0.8:
            recommendations.append({
                'category': 'insulin_adjustment',
                'priority': 'medium',
                'recommendation': 'è¯„ä¼°å¤œé—´åŸºç¡€èƒ°å²›ç´ ç®¡ç†',
                'rationale': f'å¤œé—´è¡€ç³–ç¨³å®šæ€§{nocturnal_flatness:.2f}å¾…æ”¹å–„ï¼Œå¯èƒ½å½±å“ç¡çœ è´¨é‡',
                'follow_up': 'è®°å½•å¤œé—´è¡€ç³–å’Œç¡çœ è´¨é‡ï¼Œå¿…è¦æ—¶è°ƒæ•´åŸºç¡€èƒ°å²›ç´ '
            })
        
        # æ›²çº¿å¹³æ»‘åº¦å»ºè®®
        smoothness = self.analysis_results.get('median_curve_smoothness', 0.5)
        if smoothness < 0.6:
            recommendations.append({
                'category': 'comprehensive_management',
                'priority': 'medium',
                'recommendation': 'ç»¼åˆæ”¹å–„è¡€ç³–æ§åˆ¶çš„å¹³ç¨³æ€§',
                'rationale': f'AGPæ›²çº¿å¹³æ»‘åº¦{smoothness:.2f}åä½ï¼Œè¡€ç³–æ³¢åŠ¨è¾ƒå¤§',
                'follow_up': 'è¯„ä¼°æ²»ç–—ä¾ä»æ€§ï¼Œè€ƒè™‘æ²»ç–—æ–¹æ¡ˆè°ƒæ•´'
            })
        
        # å¦‚æœæ²¡æœ‰ç”Ÿæˆä»»ä½•å»ºè®®ï¼Œæ·»åŠ ä¸€èˆ¬æ€§å»ºè®®
        if not recommendations:
            recommendations.append({
                'category': 'general_management',
                'priority': 'low',
                'recommendation': 'ç»´æŒå½“å‰è¡€ç³–ç®¡ç†æ–¹æ¡ˆï¼Œç»§ç»­å®šæœŸç›‘æµ‹',
                'rationale': 'å½“å‰è¡€ç³–æ¨¡å¼ç›¸å¯¹ç¨³å®šï¼Œæœªå‘ç°æ˜æ˜¾éœ€è¦ç´§æ€¥è°ƒæ•´çš„é—®é¢˜',
                'follow_up': 'å»ºè®®3ä¸ªæœˆåå¤æŸ¥AGPï¼Œè¯„ä¼°é•¿æœŸè¶‹åŠ¿'
            })
        
        return recommendations
    
    def _generate_detailed_analysis(self) -> Dict:
        """ç”Ÿæˆè¯¦ç»†åˆ†æ"""
        return {
            'curve_morphology': {
                'smoothness': self.analysis_results.get('median_curve_smoothness', 0),
                'symmetry': self.analysis_results.get('curve_symmetry_index', 0),
                'complexity': self.analysis_results.get('visual_complexity_index', 0),
                'interpretation': self._interpret_curve_morphology()
            },
            'time_patterns': {
                'dawn_phenomenon': self.analysis_results.get('dawn_curve_slope', 0),
                'postprandial_response': {
                    'morning': self.analysis_results.get('morning_peak_height', 0),
                    'consistency': self.analysis_results.get('meal_peak_consistency', 0)
                },
                'nocturnal_stability': self.analysis_results.get('nocturnal_curve_flatness', 0),
                'interpretation': self._interpret_time_patterns()
            },
            'variability_analysis': {
                'glucose_cv': self.analysis_results.get('glucose_coefficient_of_variation', 0),
                'percentile_band_cv': self.analysis_results.get('percentile_band_cv', 0),
                'oscillation_amplitude': self.analysis_results.get('oscillation_amplitude', 0),
                'interpretation': self._interpret_variability()
            }
        }
    
    def _interpret_curve_morphology(self) -> str:
        """è§£è¯»æ›²çº¿å½¢æ€"""
        smoothness = self.analysis_results.get('median_curve_smoothness', 0.5)
        symmetry = self.analysis_results.get('curve_symmetry_index', 0.5)
        
        if smoothness > 0.8 and symmetry > 0.7:
            return "AGPæ›²çº¿å½¢æ€ä¼˜ç§€ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„è¡€ç³–æ§åˆ¶ç¨³å®šæ€§å’Œç”Ÿæ´»è§„å¾‹æ€§"
        elif smoothness > 0.6 or symmetry > 0.5:
            return "AGPæ›²çº¿åŸºæœ¬ç¨³å®šï¼Œä½†åœ¨å¹³æ»‘åº¦æˆ–å¯¹ç§°æ€§æ–¹é¢å­˜åœ¨æ”¹å–„ç©ºé—´"
        else:
            return "AGPæ›²çº¿æ˜¾ç¤ºè¡€ç³–æ§åˆ¶ä¸å¤Ÿç¨³å®šï¼Œå»ºè®®é‡æ–°è¯„ä¼°æ²»ç–—æ–¹æ¡ˆ"
    
    def _interpret_time_patterns(self) -> str:
        """è§£è¯»æ—¶é—´æ¨¡å¼"""
        dawn_slope = self.analysis_results.get('dawn_curve_slope', 0)
        nocturnal_flatness = self.analysis_results.get('nocturnal_curve_flatness', 1.0)
        
        patterns = []
        if dawn_slope > 1.0:
            patterns.append("æ˜æ˜¾é»æ˜ç°è±¡")
        if nocturnal_flatness < 0.7:
            patterns.append("å¤œé—´è¡€ç³–ä¸å¤Ÿç¨³å®š")
            
        if not patterns:
            return "æ—¶é—´æ¨¡å¼åˆ†ææ˜¾ç¤ºè¡€ç³–æ˜¼å¤œèŠ‚å¾‹åŸºæœ¬æ­£å¸¸"
        else:
            return f"æ—¶é—´æ¨¡å¼åˆ†æå‘ç°ï¼š{', '.join(patterns)}ï¼Œå»ºè®®ç›¸åº”è°ƒæ•´æ²»ç–—æ–¹æ¡ˆ"
    
    def _interpret_variability(self) -> str:
        """è§£è¯»å˜å¼‚æ€§ï¼ˆç»¼åˆåˆ†æä¸¤ç§å˜å¼‚æ€§ï¼‰"""
        glucose_cv = self.analysis_results.get('glucose_coefficient_of_variation', 30)
        band_cv = self.analysis_results.get('percentile_band_cv', 30)
        
        # æ•´ä½“è¡€ç³–å˜å¼‚æ€§è¯„ä¼°
        if glucose_cv < 36:
            glucose_assessment = "æ•´ä½“è¡€ç³–å˜å¼‚æ€§è‰¯å¥½"
        elif glucose_cv < 50:
            glucose_assessment = "æ•´ä½“è¡€ç³–å˜å¼‚æ€§ä¸­ç­‰"
        else:
            glucose_assessment = "æ•´ä½“è¡€ç³–å˜å¼‚æ€§åé«˜"
        
        # æ—¶é—´æ¨¡å¼å˜å¼‚æ€§è¯„ä¼°
        if band_cv < 30:
            pattern_assessment = "æ˜¼å¤œè¡€ç³–èŠ‚å¾‹ç¨³å®š"
        elif band_cv < 50:
            pattern_assessment = "æ˜¼å¤œè¡€ç³–èŠ‚å¾‹æœ‰æ³¢åŠ¨"
        else:
            pattern_assessment = "æ˜¼å¤œè¡€ç³–èŠ‚å¾‹ä¸ç¨³å®š"
        
        # ç»¼åˆè§£è¯»
        if glucose_cv < 36 and band_cv < 30:
            return f"{glucose_assessment}ï¼Œ{pattern_assessment}ï¼Œè¡€ç³–ç®¡ç†ä¼˜ç§€"
        elif glucose_cv < 36:
            return f"{glucose_assessment}ï¼Œä½†{pattern_assessment}ï¼Œå»ºè®®ä¼˜åŒ–ç”Ÿæ´»è§„å¾‹"
        elif band_cv < 30:
            return f"{pattern_assessment}ï¼Œä½†{glucose_assessment}ï¼Œå»ºè®®ä¼˜åŒ–è¡€ç³–æ§åˆ¶"
        else:
            return f"{glucose_assessment}ä¸”{pattern_assessment}ï¼Œéœ€è¦å…¨é¢æ”¹å–„è¡€ç³–ç®¡ç†"
    
    def _generate_trending_analysis(self) -> Dict:
        """ç”Ÿæˆè¶‹åŠ¿åˆ†æ"""
        return {
            'baseline_trend': self.analysis_results.get('baseline_drift_slope', 0),
            'pattern_consistency': self.analysis_results.get('intraday_pattern_similarity', 0.5),
            'weekend_variation': self.analysis_results.get('weekend_pattern_deviation', 0),
            'interpretation': self._interpret_trending()
        }
    
    def _interpret_trending(self) -> str:
        """è§£è¯»è¶‹åŠ¿"""
        drift = self.analysis_results.get('baseline_drift_slope', 0)
        consistency = self.analysis_results.get('intraday_pattern_similarity', 0.5)
        
        if abs(drift) < 0.05 and consistency > 0.7:
            return "è¡€ç³–è¶‹åŠ¿ç¨³å®šï¼Œæ—¥é—´æ¨¡å¼ä¸€è‡´æ€§è‰¯å¥½"
        elif abs(drift) > 0.1:
            direction = "ä¸Šå‡" if drift > 0 else "ä¸‹é™" 
            return f"è¡€ç³–åŸºçº¿å‘ˆ{direction}è¶‹åŠ¿ï¼Œéœ€è¦å…³æ³¨é•¿æœŸæ§åˆ¶"
        else:
            return "è¡€ç³–æ¨¡å¼å­˜åœ¨ä¸€å®šå˜åŒ–ï¼Œå»ºè®®åŠ å¼ºç›‘æµ‹"
    
    def _generate_patient_education(self) -> List[Dict]:
        """ç”Ÿæˆæ‚£è€…æ•™è‚²å†…å®¹"""
        education_points = []
        
        # åŸºäºåˆ†æç»“æœç”Ÿæˆé’ˆå¯¹æ€§æ•™è‚²å†…å®¹
        dawn_slope = self.analysis_results.get('dawn_curve_slope', 0)
        if abs(dawn_slope) > 0.5:
            if dawn_slope > 0.5:
                education_points.append({
                    'topic': 'é»æ˜ç°è±¡ç®¡ç†',
                    'content': [
                        'é»æ˜ç°è±¡æ˜¯æŒ‡æ¸…æ™¨4-8ç‚¹è¡€ç³–è‡ªç„¶ä¸Šå‡çš„ç”Ÿç†ç°è±¡',
                        f'æ‚¨çš„æ•°æ®æ˜¾ç¤ºé»æ˜ç°è±¡æ¯”è¾ƒæ˜æ˜¾(ä¸Šå‡{dawn_slope:.1f}mmol/L/h)',
                        'è¿™ä¸»è¦ç”±ç”Ÿé•¿æ¿€ç´ ã€çš®è´¨é†‡ç­‰æ¿€ç´ åˆ†æ³Œå¢åŠ å¼•èµ·',
                        'åˆç†çš„åŸºç¡€èƒ°å²›ç´ è°ƒæ•´å¯ä»¥æœ‰æ•ˆæ”¹å–„'
                    ],
                    'action_items': [
                        'è®°å½•è¿ç»­ä¸€å‘¨çš„æ™¨èµ·è¡€ç³–å€¼(6:00-8:00)',
                        'ç¡®ä¿é•¿æ•ˆèƒ°å²›ç´ æŒ‰åŒ»å˜±æ—¶é—´æ³¨å°„',
                        'ç¡å‰é¿å…é«˜ç¢³æ°´åŒ–åˆç‰©æ‘„å…¥',
                        'å¦‚æŒç»­æ™¨èµ·é«˜è¡€ç³–ï¼ŒåŠæ—¶è”ç³»åŒ»ç”Ÿè°ƒæ•´æ–¹æ¡ˆ'
                    ]
                })
            else:
                education_points.append({
                    'topic': 'å‡Œæ™¨ä½è¡€ç³–é¢„é˜²',
                    'content': [
                        f'æ‚¨çš„è¡€ç³–åœ¨å‡Œæ™¨æ—¶æ®µæœ‰ä¸‹é™è¶‹åŠ¿(ä¸‹é™{abs(dawn_slope):.1f}mmol/L/h)',
                        'è¿™å¯èƒ½æç¤ºåŸºç¡€èƒ°å²›ç´ ä½œç”¨è¿‡å¼º',
                        'å‡Œæ™¨ä½è¡€ç³–å®¹æ˜“å¼•èµ·åè·³æ€§é«˜è¡€ç³–',
                        'éœ€è¦å¯†åˆ‡ç›‘æµ‹å¹¶é€‚å½“è°ƒæ•´æ²»ç–—'
                    ],
                    'action_items': [
                        'æ³¨æ„å¤œé—´ä½è¡€ç³–ç—‡çŠ¶ï¼šå‡ºæ±—ã€å¿ƒæ‚¸ã€å™©æ¢¦',
                        'ç¡å‰è¡€ç³–åº”ç»´æŒåœ¨6-8mmol/L',
                        'å¿…è¦æ—¶ç¡å‰é€‚é‡åŠ é¤',
                        'å‘ç°å¼‚å¸¸ç«‹å³è”ç³»åŒ»æŠ¤å›¢é˜Ÿ'
                    ]
                })
        
        # TIRç›¸å…³æ•™è‚²
        tir_percentage = self.analysis_results.get('target_range_coverage', 70)
        if tir_percentage < 70:
            education_points.append({
                'topic': 'TIRç›®æ ‡èŒƒå›´ç®¡ç†',
                'content': [
                    f'æ‚¨çš„TIR(ç›®æ ‡èŒƒå›´å†…æ—¶é—´)ä¸º{tir_percentage:.1f}%ï¼Œä½äºæ¨èçš„70%',
                    'TIRæ˜¯è¯„ä¼°è¡€ç³–æ§åˆ¶è´¨é‡çš„é‡è¦æŒ‡æ ‡',
                    'æé«˜TIRå¯ä»¥é™ä½ç³–å°¿ç—…å¹¶å‘ç—‡é£é™©',
                    'éœ€è¦é€šè¿‡ç»¼åˆç®¡ç†æ¥æ”¹å–„'
                ],
                'action_items': [
                    'ä¸¥æ ¼æŒ‰åŒ»å˜±æœè¯æˆ–æ³¨å°„èƒ°å²›ç´ ',
                    'ä¿æŒè§„å¾‹çš„é¥®é£Ÿå’Œè¿åŠ¨ä¹ æƒ¯',
                    'å­¦ä¹ è¡€ç³–è‡ªæˆ‘ç›‘æµ‹æŠ€èƒ½',
                    'å®šæœŸå¤æŸ¥å¹¶è°ƒæ•´æ²»ç–—æ–¹æ¡ˆ'
                ]
            })
        
        # æ•´ä½“è¡€ç³–å˜å¼‚æ€§æ•™è‚²
        glucose_cv = self.analysis_results.get('glucose_coefficient_of_variation', 30)
        if glucose_cv > 36:
            education_points.append({
                'topic': 'æ•´ä½“è¡€ç³–ç¨³å®šæ€§æ”¹å–„',
                'content': [
                    f'æ‚¨çš„æ•´ä½“è¡€ç³–å˜å¼‚ç³»æ•°ä¸º{glucose_cv:.1f}%ï¼Œè¶…è¿‡ADAæ¨èçš„36%æ ‡å‡†',
                    'è¡€ç³–å¤§å¹…æ³¢åŠ¨æ¯”æŒç»­é«˜è¡€ç³–å±å®³æ›´å¤§',
                    'ç¨³å®šçš„è¡€ç³–æœ‰åŠ©äºé¢„é˜²æ€¥æ€§å’Œæ…¢æ€§å¹¶å‘ç—‡',
                    'éœ€è¦ä»æ²»ç–—æ–¹æ¡ˆå’Œè‡ªæˆ‘ç®¡ç†ä¸¤æ–¹é¢å…¥æ‰‹æ”¹å–„'
                ],
                'action_items': [
                    'ä¸åŒ»ç”Ÿè®¨è®ºè°ƒæ•´æ²»ç–—æ–¹æ¡ˆçš„å¯èƒ½æ€§',
                    'åŠ å¼ºè¡€ç³–è‡ªæˆ‘ç›‘æµ‹é¢‘ç‡',
                    'å­¦ä¹ è¯†åˆ«å’Œå¤„ç†è¡€ç³–æ³¢åŠ¨çš„è¯±å› ',
                    'å»ºç«‹è§„å¾‹çš„é¥®é£Ÿå’Œè¿åŠ¨æ¨¡å¼'
                ]
            })
        
        # æ˜¼å¤œèŠ‚å¾‹å˜å¼‚æ€§æ•™è‚²
        band_cv = self.analysis_results.get('percentile_band_cv', 30)
        if band_cv > 40:
            education_points.append({
                'topic': 'æ˜¼å¤œè¡€ç³–èŠ‚å¾‹ä¼˜åŒ–',
                'content': [
                    f'æ‚¨çš„æ˜¼å¤œè¡€ç³–æ¨¡å¼å˜å¼‚ä¸º{band_cv:.1f}%ï¼Œæç¤ºä¸åŒæ—¶æ®µå·®å¼‚è¾ƒå¤§',
                    'ç¨³å®šçš„æ˜¼å¤œè¡€ç³–èŠ‚å¾‹æœ‰åŠ©äºè¡€ç³–ç®¡ç†',
                    'è§„å¾‹çš„ç”Ÿæ´»ä½œæ¯æ˜¯å»ºç«‹è‰¯å¥½èŠ‚å¾‹çš„åŸºç¡€',
                    'ä¸ªä½“åŒ–çš„æ—¶é—´ç®¡ç†å¯ä»¥æ”¹å–„è¡€ç³–æ¨¡å¼'
                ],
                'action_items': [
                    'å›ºå®šèµ·åºŠå’Œå°±å¯æ—¶é—´',
                    'ä¸‰é¤å®šæ—¶å®šé‡ï¼Œé¿å…å¤œé—´è¿›é£Ÿ',
                    'æ ¹æ®AGPå›¾è°±è¯†åˆ«é—®é¢˜æ—¶æ®µ',
                    'è®°å½•ç”Ÿæ´»äº‹ä»¶ä¸è¡€ç³–å˜åŒ–çš„å…³ç³»'
                ]
            })
        
        # é¤åè¡€ç³–æ•™è‚²
        morning_peak = self.analysis_results.get('morning_peak_height', 0)
        if morning_peak > 3.0:
            education_points.append({
                'topic': 'é¤åè¡€ç³–ç®¡ç†',
                'content': [
                    f'æ‚¨çš„æ—©é¤åè¡€ç³–å‡é«˜{morning_peak:.1f}mmol/Lï¼Œéœ€è¦æ”¹å–„',
                    'é¤å2å°æ—¶è¡€ç³–åº”æ§åˆ¶åœ¨10mmol/Lä»¥ä¸‹',
                    'åˆç†çš„é¤æ—¶èƒ°å²›ç´ ç®¡ç†æ˜¯å…³é”®',
                    'é¥®é£Ÿç»“æ„å’Œè¿›é¤é¡ºåºä¹Ÿå¾ˆé‡è¦'
                ],
                'action_items': [
                    'å­¦ä¹ ç¢³æ°´åŒ–åˆç‰©è®¡æ•°æ–¹æ³•',
                    'é¤å‰30åˆ†é’Ÿæ³¨å°„é€Ÿæ•ˆèƒ°å²›ç´ ',
                    'è¿›é¤é¡ºåºï¼šè”¬èœâ†’è›‹ç™½è´¨â†’ä¸»é£Ÿ',
                    'é¤åé€‚å½“æ´»åŠ¨ï¼Œå¦‚æ•£æ­¥15-30åˆ†é’Ÿ'
                ]
            })
        
        # å¤œé—´ç®¡ç†æ•™è‚²
        nocturnal_flatness = self.analysis_results.get('nocturnal_curve_flatness', 1.0)
        if nocturnal_flatness < 0.8:
            education_points.append({
                'topic': 'å¤œé—´è¡€ç³–ç®¡ç†',
                'content': [
                    f'æ‚¨çš„å¤œé—´è¡€ç³–ç¨³å®šæ€§{nocturnal_flatness:.2f}éœ€è¦æ”¹å–„',
                    'è‰¯å¥½çš„å¤œé—´è¡€ç³–æ§åˆ¶æœ‰åŠ©äºæ”¹å–„ç¡çœ è´¨é‡',
                    'å¤œé—´è¡€ç³–æ³¢åŠ¨å¯èƒ½å½±å“æ¬¡æ—¥çš„è¡€ç³–æ§åˆ¶',
                    'éœ€è¦ä¼˜åŒ–åŸºç¡€èƒ°å²›ç´ æ²»ç–—'
                ],
                'action_items': [
                    'è®°å½•ç¡çœ è´¨é‡å’Œå¤œé—´ç—‡çŠ¶',
                    'ç¡å‰è¡€ç³–æ§åˆ¶åœ¨6-8mmol/L',
                    'é¿å…ç¡å‰å¤§é‡è¿›é£Ÿ',
                    'å¿…è¦æ—¶è¿›è¡Œå¤œé—´è¡€ç³–ç›‘æµ‹'
                ]
            })
        
        # é€šç”¨è¡€ç³–ç®¡ç†æ•™è‚²
        education_points.append({
            'topic': 'CGMä½¿ç”¨å’Œè¡€ç³–ç®¡ç†',
            'content': [
                'CGMä¸ºæ‚¨æä¾›è¿ç»­çš„è¡€ç³–ä¿¡æ¯ï¼Œæ¯”ä¼ ç»Ÿè¡€ç³–ä»ªæ›´å…¨é¢',
                'å…³æ³¨è¡€ç³–è¶‹åŠ¿ç®­å¤´ï¼ŒåŠæ—¶è°ƒæ•´æ²»ç–—',
                'å®šæœŸæ ¡å‡†CGMç¡®ä¿æ•°æ®å‡†ç¡®æ€§',
                'AGPæŠ¥å‘Šèƒ½å¸®åŠ©æ‚¨å’ŒåŒ»ç”Ÿåˆ¶å®šæ›´å¥½çš„æ²»ç–—æ–¹æ¡ˆ'
            ],
            'action_items': [
                'æ¯æ—¥æŸ¥çœ‹CGMæ•°æ®å’Œè¶‹åŠ¿',
                'å­¦ä¼šè¯†åˆ«è¡€ç³–æ¨¡å¼',
                'è®°å½•é¥®é£Ÿã€è¿åŠ¨å’Œç”¨è¯æ—¶é—´',
                'å®šæœŸä¸åŒ»æŠ¤å›¢é˜Ÿåˆ†äº«AGPæŠ¥å‘Š'
            ]
        })
        
        return education_points


# ä¸»ç¨‹åºç¤ºä¾‹
def main():
    """ä¸»ç¨‹åº - æ¼”ç¤ºCGMæ•°æ®è¯»å–å’ŒAGPåˆ†æ"""
    
    # åˆå§‹åŒ–ç»„ä»¶
    cgm_reader = CGMDataReader()
    agp_analyzer = AGPVisualAnalyzer()
    report_generator = AGPIntelligentReporter()
    
    # ç¤ºä¾‹ï¼šå¤„ç†CGMæ•°æ®æ–‡ä»¶
    try:
        # 1. è¯»å–CGMæ•°æ®
        print("æ­£åœ¨è¯»å–CGMæ•°æ®...")
        # è¯·æ ¹æ®æ‚¨çš„æ–‡ä»¶å®é™…æ ¼å¼é€‰æ‹© device_typeï¼Œå¦‚æœæ–‡ä»¶æ˜¯CSVæ ¼å¼ï¼Œå¯ä»¥å°è¯• 'generic_csv'
        # å¦‚æœæ˜¯AGPAI_Agent_V2.pyä¸­é‚£ç§tabåˆ†éš”çš„æ ¼å¼ï¼Œè¿™ä¸ªreaderå¯èƒ½æ— æ³•ç›´æ¥è¯»å–ï¼Œéœ€è¦è°ƒæ•´
        cgm_file_path = "/Users/williamsun/Documents/gplus/docs/AGPAI/R002 V5.txt"
        cgm_data = cgm_reader.read_cgm_file(cgm_file_path, device_type='generic_csv') # å°è¯•ä½¿ç”¨é€šç”¨CSVè¯»å–å™¨
        
        print(f"æˆåŠŸè¯»å–{len(cgm_data)}ä¸ªæ•°æ®ç‚¹ï¼Œæ—¶é—´èŒƒå›´ï¼š{cgm_data['timestamp'].min()} åˆ° {cgm_data['timestamp'].max()}")
        
        # 2. è¿›è¡ŒAGPè§†è§‰åˆ†æ
        print("æ­£åœ¨è¿›è¡ŒAGPè§†è§‰åˆ†æ...")
        analysis_results = agp_analyzer.analyze_cgm_data(cgm_data, analysis_days=14)
        
        print(f"å®Œæˆ57ç§è§†è§‰æŒ‡æ ‡åˆ†æ")
        
        # 3. ç”Ÿæˆæ™ºèƒ½æŠ¥å‘Š
        print("æ­£åœ¨ç”Ÿæˆæ™ºèƒ½åˆ†ææŠ¥å‘Š...")
        patient_info = {
            'name': 'å¼ ä¸‰',
            'age': 45,
            'gender': 'ç”·',
            'diabetes_type': 'T2DM',
            'diabetes_duration': '8å¹´',
            'cgm_device': 'Dexcom G6'
        }
        
        intelligent_report = report_generator.generate_intelligent_report(analysis_results, patient_info)
        
        # 4. è¾“å‡ºæŠ¥å‘Šæ‘˜è¦
        print("\n=== AGPæ™ºèƒ½åˆ†ææŠ¥å‘Š ===")
        overall = intelligent_report['overall_assessment']
        print(f"æ•´ä½“è¯„ä¼°ï¼š{overall['level']} ({overall['overall_score']}åˆ†)")
        print(f"è¯„ä¼°è¯´æ˜ï¼š{overall['description']}")
        
        print("\nä¸»è¦å‘ç°ï¼š")
        for finding in intelligent_report['key_findings']:
            print(f"- {finding['description']}")
        
        print("\né£é™©è­¦æŠ¥ï¼š")
        for alert in intelligent_report['risk_alerts']:
            print(f"- [{alert['urgency'].upper()}] {alert['message']}")
        
        print("\nä¸´åºŠå»ºè®®ï¼š")
        for rec in intelligent_report['clinical_recommendations']:
            print(f"- {rec['recommendation']}")
        
        # 5. ä¿å­˜å®Œæ•´æŠ¥å‘Š
        report_filename = f"AGP_Analysis_Report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_filename, 'w', encoding='utf-8') as f:
            json.dump(intelligent_report, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nå®Œæ•´æŠ¥å‘Šå·²ä¿å­˜è‡³ï¼š{report_filename}")
        
        # 6. æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
        print("\n=== å…³é”®è§†è§‰æŒ‡æ ‡ ===")
        key_indicators = [
            ('median_curve_smoothness', 'ä¸­ä½æ•°æ›²çº¿å¹³æ»‘åº¦'),
            ('curve_symmetry_index', 'æ›²çº¿å¯¹ç§°æ€§æŒ‡æ•°'),
            ('percentile_band_width_avg', 'åˆ†ä½æ•°å¸¦å®½å‡å€¼'),
            ('dawn_curve_slope', 'é»æ˜ç°è±¡æ–œç‡'),
            ('morning_peak_height', 'æ—©æ™¨å³°å€¼é«˜åº¦'),
            ('comprehensive_smoothness_score', 'ç»¼åˆå¹³æ»‘åº¦è¯„åˆ†'),
            ('curve_elegance_score', 'æ›²çº¿ä¼˜é›…åº¦è¯„åˆ†')
        ]
        
        for key, desc in key_indicators:
            value = analysis_results.get(key, 0)
            print(f"{desc}: {value:.3f}")
        
        print("\nåˆ†æå®Œæˆï¼")
        
    except Exception as e:
        print(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}")
        logging.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼š")


if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    # è¿è¡Œä¸»ç¨‹åº
    main()