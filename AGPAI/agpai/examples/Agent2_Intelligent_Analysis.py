#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Agent2 æ™ºèƒ½è„†æ€§åˆ†æå™¨ v5.0
æ¢å¤å®Œæ•´çš„ç§‘å­¦åˆ†æåŠŸèƒ½ï¼š
- æ··æ²ŒåŠ¨åŠ›å­¦åˆ†æï¼ˆLyapunovæŒ‡æ•°ã€è¿‘ä¼¼ç†µã€HurstæŒ‡æ•°ï¼‰
- å¤šç»´è¯„åˆ†å†³ç­–ç³»ç»Ÿ
- 24å°æ—¶æ—¶æ®µè„†æ€§åˆ†æ
- æ ‡å‡†è„†æ€§åˆ†å‹ä½“ç³»
- æ²»ç–—ååº”ç‹¬ç«‹è¯„ä¼°
"""

import pandas as pd
import numpy as np
import json
from datetime import datetime, timedelta
from typing import Dict, List, Tuple
from scipy import stats
from scipy.spatial.distance import pdist, squareform
from scipy.signal import find_peaks, savgol_filter
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

def analyze_intelligent_brittleness(filepath: str, patient_id: str) -> dict:
    """æ™ºèƒ½è„†æ€§åˆ†æ - å®Œæ•´çš„ç§‘å­¦åˆ†ææ–¹æ³•"""
    
    # åŠ è½½æ•°æ®
    df = pd.read_excel(filepath)
    if 'å€¼' in df.columns:
        df = df.rename(columns={'å€¼': 'glucose_value', 'æ—¶é—´': 'timestamp'})
    elif 'glucose' in df.columns:
        df = df.rename(columns={'glucose': 'glucose_value'})
    
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df = df.sort_values('timestamp')
    
    glucose_values = df['glucose_value'].dropna().values
    total_days = (df['timestamp'].max() - df['timestamp'].min()).days + 1
    
    print(f"[Agent2 Intelligence] æ•°æ®ç‚¹æ•°: {len(glucose_values)}, ç›‘æµ‹å¤©æ•°: {total_days}")
    
    # 1. æ··æ²ŒåŠ¨åŠ›å­¦æŒ‡æ ‡è®¡ç®—
    print("[Agent2 Intelligence] è®¡ç®—æ··æ²ŒåŠ¨åŠ›å­¦æŒ‡æ ‡...")
    chaos_indicators = calculate_comprehensive_chaos_indicators(glucose_values)
    
    # 2. å¤šç»´è¯„åˆ†è„†æ€§åˆ†å‹
    print("[Agent2 Intelligence] è¿›è¡Œå¤šç»´è„†æ€§åˆ†å‹...")
    brittleness_type, severity, severity_score, decision_scores = classify_brittleness_intelligent(
        chaos_indicators, glucose_values
    )
    
    # 3. 24å°æ—¶æ—¶æ®µè„†æ€§åˆ†æ
    print("[Agent2 Intelligence] åˆ†ææ—¶æ®µè„†æ€§æ¨¡å¼...")
    temporal_analysis = analyze_temporal_brittleness_patterns(df, glucose_values)
    
    # 4. æ™ºèƒ½æ—¶é—´åˆ†æ®µçºµå‘è„†æ€§åˆ†æï¼ˆé‡è¦ï¼ï¼‰
    print("[Agent2 Intelligence] è¿›è¡Œæ™ºèƒ½æ—¶é—´åˆ†æ®µçºµå‘åˆ†æ...")
    longitudinal_analysis = analyze_intelligent_longitudinal_segments(df, glucose_values, total_days)
    
    # 5. åŠ¨æ€æ²»ç–—ååº”è¯„ä¼°
    print("[Agent2 Intelligence] è¯„ä¼°æ²»ç–—ååº”...")
    treatment_response = assess_dynamic_treatment_response(df, glucose_values)
    
    # 5. è„†æ€§ç‰¹å¾è¯¦ç»†åˆ†æ
    clinical_features = generate_clinical_features_intelligent(brittleness_type, chaos_indicators)
    
    # ç”Ÿæˆæ™ºèƒ½åˆ†ææŠ¥å‘Š
    intelligent_report = {
        "æŠ¥å‘Šå¤´ä¿¡æ¯": {
            "æŠ¥å‘Šç±»å‹": "AGPAIæ™ºèƒ½è„†æ€§åˆ†ææŠ¥å‘Š v5.0",
            "æ‚£è€…ID": patient_id,
            "åˆ†ææ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "æŠ¥å‘Šç”Ÿæˆå™¨": "Agent2_Intelligent_Brittleness_Analyzer",
            "ç‰ˆæœ¬å·": "5.0.0",
            "åˆ†ææ–¹æ³•": "æ··æ²ŒåŠ¨åŠ›å­¦ + å¤šç»´è¯„åˆ† + æ—¶æ®µåˆ†æ + æ ‡å‡†åˆ†å‹"
        },
        
        "æ‚£è€…åŸºæœ¬ä¿¡æ¯": {
            "æ‚£è€…ID": patient_id,
            "ç›‘æµ‹å¤©æ•°": total_days,
            "æ•°æ®ç‚¹æ•°": len(glucose_values),
            "æ•°æ®å¯†åº¦": f"{len(glucose_values)/total_days:.1f} è¯»æ•°/å¤©",
            "ç›‘æµ‹æ—¶é—´èŒƒå›´": f"{df['timestamp'].min().strftime('%Y-%m-%d')} è‡³ {df['timestamp'].max().strftime('%Y-%m-%d')}"
        },
        
        "æ··æ²ŒåŠ¨åŠ›å­¦æŒ‡æ ‡": {
            "åˆ†æè¯´æ˜": "åŸºäºéçº¿æ€§åŠ¨åŠ›å­¦ç†è®ºçš„è¡€ç³–ç³»ç»Ÿæ··æ²Œç‰¹å¾åˆ†æ",
            "LyapunovæŒ‡æ•°": {
                "æ•°å€¼": f"{chaos_indicators['lyapunov_exponent']:.6f}",
                "è§£é‡Š": "æ­£å€¼è¡¨ç¤ºæ··æ²Œè¡Œä¸ºï¼Œè´Ÿå€¼è¡¨ç¤ºç¨³å®šæ”¶æ•›",
                "ä¸´åºŠæ„ä¹‰": "è¯„ä¼°è¡€ç³–ç³»ç»Ÿçš„å¯é¢„æµ‹æ€§å’Œç¨³å®šæ€§"
            },
            "è¿‘ä¼¼ç†µ": {
                "æ•°å€¼": f"{chaos_indicators['approximate_entropy']:.4f}",
                "è§£é‡Š": "è¡¡é‡æ—¶é—´åºåˆ—çš„å¤æ‚æ€§å’Œä¸è§„åˆ™ç¨‹åº¦",
                "ä¸´åºŠæ„ä¹‰": "é«˜ç†µå€¼æç¤ºè¡€ç³–æ¨¡å¼å¤æ‚éš¾ä»¥é¢„æµ‹"
            },
            "Shannonç†µ": {
                "æ•°å€¼": f"{chaos_indicators['shannon_entropy']:.4f}",
                "è§£é‡Š": "è¡€ç³–å€¼åˆ†å¸ƒçš„ä¿¡æ¯ç†µ",
                "ä¸´åºŠæ„ä¹‰": "åæ˜ è¡€ç³–åˆ†å¸ƒçš„éšæœºæ€§ç¨‹åº¦"
            },
            "HurstæŒ‡æ•°": {
                "æ•°å€¼": f"{chaos_indicators['hurst_exponent']:.4f}",
                "è§£é‡Š": "0.5ä¸ºéšæœºæ¸¸èµ°ï¼Œ<0.5ä¸ºåæŒç»­æ€§ï¼Œ>0.5ä¸ºæŒç»­æ€§",
                "ä¸´åºŠæ„ä¹‰": "è¯„ä¼°è¡€ç³–ç³»ç»Ÿçš„è®°å¿†ç‰¹æ€§å’Œé•¿ç¨‹ç›¸å…³æ€§"
            },
            "åˆ†å½¢ç»´åº¦": {
                "æ•°å€¼": f"{chaos_indicators['fractal_dimension']:.4f}",
                "è§£é‡Š": "æè¿°è¡€ç³–æ›²çº¿çš„å‡ ä½•å¤æ‚æ€§",
                "ä¸´åºŠæ„ä¹‰": "åæ˜ è¡€ç³–æ³¢åŠ¨çš„ç²¾ç»†ç»“æ„ç‰¹å¾"
            },
            "å…³è”ç»´æ•°": {
                "æ•°å€¼": f"{chaos_indicators['correlation_dimension']:.4f}",
                "è§£é‡Š": "è¡¡é‡å¸å¼•å­çš„ç»´æ•°",
                "ä¸´åºŠæ„ä¹‰": "è¯„ä¼°è¡€ç³–ç³»ç»Ÿçš„åŠ¨åŠ›å­¦å¤æ‚åº¦"
            }
        },
        
        "æ™ºèƒ½è„†æ€§åˆ†å‹è¯„ä¼°": {
            "è„†æ€§åˆ†å‹": brittleness_type,
            "è„†æ€§ä¸¥é‡ç¨‹åº¦": severity,
            "è„†æ€§è¯„åˆ†": f"{severity_score:.1f}/100",
            "é£é™©ç­‰çº§": get_risk_level(severity_score),
            "æ ¸å¿ƒæŒ‡æ ‡": {
                "è¡€ç³–å˜å¼‚ç³»æ•°": f"{chaos_indicators['cv_percent']:.1f}%",
                "å¹³å‡è¡€ç³–": f"{chaos_indicators['mean_glucose']:.2f} mmol/L",
                "è¡€ç³–æ ‡å‡†å·®": f"{chaos_indicators['std_glucose']:.2f} mmol/L",
                "TIRè¾¾æ ‡ç‡": f"{((glucose_values >= 3.9) & (glucose_values <= 10.0)).sum() / len(glucose_values) * 100:.1f}%",
                "è¡€ç³–èŒƒå›´": f"{glucose_values.min():.1f}-{glucose_values.max():.1f} mmol/L"
            },
            "å¤šç»´å†³ç­–ä¾æ®": {
                "æ··æ²Œè¯„åˆ†": decision_scores['chaos_score'],
                "è®°å¿†è¯„åˆ†": decision_scores['memory_score'], 
                "å˜å¼‚è¯„åˆ†": decision_scores['variability_score'],
                "é¢‘åŸŸè¯„åˆ†": decision_scores.get('frequency_score', 0),
                "å†³ç­–é€»è¾‘": "åŸºäºæ··æ²ŒåŠ¨åŠ›å­¦çš„å¤šç»´æŒ‡æ ‡ç»¼åˆè¯„åˆ†ç³»ç»Ÿ"
            },
            "è„†æ€§ç‰¹å¾": clinical_features,
            "ç—…ç†ç”Ÿç†æœºåˆ¶": get_pathophysiology_mechanism_intelligent(brittleness_type, chaos_indicators)
        },
        
        "æ—¶æ®µè„†æ€§é£é™©åˆ†æ": temporal_analysis,
        
        "æ™ºèƒ½æ—¶é—´åˆ†æ®µçºµå‘è„†æ€§åˆ†æ": longitudinal_analysis,
        
        "æ²»ç–—ååº”åŠ¨æ€è¯„ä¼°": treatment_response,
        
        "ç»¼åˆæ™ºèƒ½è§£è¯»": generate_intelligent_interpretation(
            brittleness_type, severity_score, chaos_indicators, treatment_response
        ),
        
        "ä¸ªæ€§åŒ–æ²»ç–—å»ºè®®": generate_intelligent_recommendations(
            brittleness_type, severity_score, chaos_indicators, temporal_analysis, treatment_response
        )
    }
    
    return intelligent_report

def calculate_comprehensive_chaos_indicators(glucose_data: np.ndarray) -> dict:
    """è®¡ç®—å®Œæ•´çš„æ··æ²ŒåŠ¨åŠ›å­¦æŒ‡æ ‡"""
    
    indicators = {}
    
    # åŸºç¡€ç»Ÿè®¡
    mean_glucose = np.mean(glucose_data)
    std_glucose = np.std(glucose_data)
    cv = (std_glucose / mean_glucose) * 100
    
    indicators['mean_glucose'] = mean_glucose
    indicators['std_glucose'] = std_glucose
    indicators['cv_percent'] = cv
    
    # 1. LyapunovæŒ‡æ•°è®¡ç®— (æ”¹è¿›ç‰ˆ)
    indicators['lyapunov_exponent'] = calculate_lyapunov_exponent(glucose_data)
    
    # 2. è¿‘ä¼¼ç†µè®¡ç®—
    indicators['approximate_entropy'] = calculate_approximate_entropy(glucose_data)
    
    # 3. Shannonç†µè®¡ç®—
    indicators['shannon_entropy'] = calculate_shannon_entropy(glucose_data)
    
    # 4. HurstæŒ‡æ•°è®¡ç®— (æ”¹è¿›R/Såˆ†æ)
    indicators['hurst_exponent'] = calculate_hurst_exponent(glucose_data)
    
    # 5. åˆ†å½¢ç»´åº¦è®¡ç®—
    indicators['fractal_dimension'] = calculate_fractal_dimension(glucose_data)
    
    # 6. å…³è”ç»´æ•°è®¡ç®—
    indicators['correlation_dimension'] = calculate_correlation_dimension(glucose_data)
    
    # 7. è‡ªç›¸å…³åˆ†æ
    indicators['autocorrelation'] = calculate_autocorrelation_features(glucose_data)
    
    return indicators

def calculate_lyapunov_exponent(data: np.ndarray, embed_dim: int = 3, lag: int = 1) -> float:
    """æ”¹è¿›çš„LyapunovæŒ‡æ•°è®¡ç®—"""
    try:
        n = len(data)
        if n < 100:
            return -0.001  # æ•°æ®ä¸è¶³æ—¶è¿”å›é»˜è®¤å€¼
        
        # åµŒå…¥ç»´æ•°é‡æ„ç›¸ç©ºé—´
        embedded = embed_time_series(data, embed_dim, lag)
        
        # è®¡ç®—é‚»è¿‘ç‚¹çš„åˆ†ç¦»ç‡
        distances = pdist(embedded)
        if len(distances) == 0 or np.all(distances == 0):
            return -0.001
        
        # æœ€å¤§LyapunovæŒ‡æ•°ä¼°è®¡
        min_dist = np.percentile(distances[distances > 0], 5)  # é¿å…é›¶è·ç¦»
        max_dist = np.percentile(distances, 95)
        
        if max_dist <= min_dist or min_dist <= 0:
            return -0.001
            
        lyapunov = np.log(max_dist / min_dist) / len(data)
        
        # é™åˆ¶èŒƒå›´é¿å…å¼‚å¸¸å€¼
        return np.clip(lyapunov, -2.0, 2.0)
        
    except Exception:
        return -0.001

def embed_time_series(data: np.ndarray, embed_dim: int, lag: int) -> np.ndarray:
    """æ—¶é—´åºåˆ—åµŒå…¥é‡æ„"""
    n = len(data)
    embedded_length = n - (embed_dim - 1) * lag
    
    if embedded_length <= 0:
        return np.array([])
    
    embedded = np.zeros((embedded_length, embed_dim))
    for i in range(embed_dim):
        embedded[:, i] = data[i * lag:i * lag + embedded_length]
    
    return embedded

def calculate_approximate_entropy(data: np.ndarray, m: int = 2, r: float = None) -> float:
    """è®¡ç®—è¿‘ä¼¼ç†µ"""
    try:
        n = len(data)
        if n < 50:
            return 0.1
        
        if r is None:
            r = 0.2 * np.std(data)
        
        def _maxdist(xi, xj):
            return max([abs(ua - va) for ua, va in zip(xi, xj)])
        
        def _phi(m):
            patterns = np.array([data[i:i + m] for i in range(n - m + 1)])
            c = np.zeros(n - m + 1)
            
            for i in range(n - m + 1):
                template = patterns[i]
                for j in range(n - m + 1):
                    if _maxdist(template, patterns[j]) <= r:
                        c[i] += 1
            
            # é¿å…log(0)
            c = np.maximum(c, 1)
            phi = np.sum(np.log(c / float(n - m + 1))) / (n - m + 1)
            return phi
        
        return _phi(m) - _phi(m + 1)
        
    except Exception:
        return 0.1

def calculate_shannon_entropy(data: np.ndarray, bins: int = 50) -> float:
    """è®¡ç®—Shannonç†µ"""
    try:
        hist, _ = np.histogram(data, bins=bins, density=True)
        hist = hist[hist > 0]  # åªä¿ç•™éé›¶å€¼
        
        if len(hist) == 0:
            return 1.0
            
        # å½’ä¸€åŒ–
        hist = hist / np.sum(hist)
        shannon_entropy = -np.sum(hist * np.log(hist))
        
        return shannon_entropy
        
    except Exception:
        return 1.0

def calculate_hurst_exponent(data: np.ndarray) -> float:
    """æ”¹è¿›çš„HurstæŒ‡æ•°è®¡ç®—ï¼ˆR/Såˆ†æï¼‰"""
    try:
        n = len(data)
        if n < 20:
            return 0.5
        
        # ç§»é™¤è¶‹åŠ¿
        y = np.cumsum(data - np.mean(data))
        
        # R/Såˆ†æ
        rs_values = []
        ns = []
        
        # ä½¿ç”¨å¤šä¸ªçª—å£å¤§å°
        window_sizes = np.unique(np.logspace(1, np.log10(n//4), 10).astype(int))
        
        for window_size in window_sizes:
            if window_size < 5 or window_size >= n:
                continue
                
            # åˆ†å‰²æ•°æ®ä¸ºçª—å£
            num_windows = n // window_size
            rs_window = []
            
            for i in range(num_windows):
                start_idx = i * window_size
                end_idx = start_idx + window_size
                window_data = y[start_idx:end_idx]
                
                if len(window_data) < 5:
                    continue
                
                # è®¡ç®—R/S
                range_data = np.max(window_data) - np.min(window_data)
                std_data = np.std(data[start_idx:end_idx])
                
                if std_data > 0:
                    rs_window.append(range_data / std_data)
            
            if rs_window:
                rs_values.append(np.mean(rs_window))
                ns.append(window_size)
        
        if len(rs_values) < 3:
            return 0.5
        
        # çº¿æ€§å›å½’æ‹Ÿåˆ log(R/S) ~ log(n)
        log_ns = np.log(ns)
        log_rs = np.log(np.maximum(rs_values, 1e-10))  # é¿å…log(0)
        
        # ä½¿ç”¨æœ€å°äºŒä¹˜æ³•
        slope, _, _, _, _ = stats.linregress(log_ns, log_rs)
        
        # HurstæŒ‡æ•°å°±æ˜¯æ–œç‡
        hurst = np.clip(slope, 0.0, 1.0)
        
        return hurst
        
    except Exception:
        return 0.5

def calculate_fractal_dimension(data: np.ndarray) -> float:
    """è®¡ç®—åˆ†å½¢ç»´åº¦ï¼ˆç›’è®¡æ•°æ³•ï¼‰"""
    try:
        # ç®€åŒ–çš„åˆ†å½¢ç»´åº¦ä¼°è®¡
        n = len(data)
        if n < 20:
            return 1.0
            
        # æ ‡å‡†åŒ–æ•°æ®
        normalized_data = (data - np.min(data)) / (np.max(data) - np.min(data) + 1e-10)
        
        # è®¡ç®—æ›²çº¿é•¿åº¦åœ¨ä¸åŒå°ºåº¦ä¸‹çš„å˜åŒ–
        scales = np.logspace(0, np.log10(n//10), 10)
        lengths = []
        
        for scale in scales:
            step = max(1, int(scale))
            downsampled = normalized_data[::step]
            if len(downsampled) < 2:
                continue
            length = np.sum(np.sqrt(np.diff(downsampled)**2 + (1/scale)**2))
            lengths.append(length)
        
        if len(lengths) < 3:
            return 1.0
        
        # æ‹Ÿåˆlog-logå…³ç³»
        log_scales = np.log(scales[:len(lengths)])
        log_lengths = np.log(lengths)
        
        slope, _, _, _, _ = stats.linregress(log_scales, log_lengths)
        fractal_dim = 1 - slope  # åˆ†å½¢ç»´åº¦
        
        return np.clip(fractal_dim, 1.0, 2.0)
        
    except Exception:
        return 1.0

def calculate_correlation_dimension(data: np.ndarray, embed_dim: int = 3) -> float:
    """è®¡ç®—å…³è”ç»´æ•°"""
    try:
        if len(data) < 100:
            return 1.0
        
        # åµŒå…¥é‡æ„
        embedded = embed_time_series(data, embed_dim, 1)
        if embedded.size == 0:
            return 1.0
        
        # è®¡ç®—å…³è”ç§¯åˆ†
        distances = pdist(embedded)
        if len(distances) == 0:
            return 1.0
        
        # é€‰æ‹©è·ç¦»èŒƒå›´
        r_values = np.logspace(np.log10(np.percentile(distances, 1)), 
                              np.log10(np.percentile(distances, 50)), 10)
        
        correlations = []
        for r in r_values:
            c = np.sum(distances <= r) / len(distances)
            correlations.append(max(c, 1e-10))
        
        # æ‹Ÿåˆæ–œç‡
        log_r = np.log(r_values)
        log_c = np.log(correlations)
        
        slope, _, _, _, _ = stats.linregress(log_r, log_c)
        
        return np.clip(slope, 0.5, 3.0)
        
    except Exception:
        return 1.0

def calculate_autocorrelation_features(data: np.ndarray) -> dict:
    """è®¡ç®—è‡ªç›¸å…³ç‰¹å¾"""
    try:
        n = len(data)
        max_lag = min(100, n // 4)
        
        autocorr = np.correlate(data - np.mean(data), data - np.mean(data), mode='full')
        autocorr = autocorr[n-1:]  # å–æ­£lagéƒ¨åˆ†
        autocorr = autocorr / autocorr[0]  # å½’ä¸€åŒ–
        
        # è®¡ç®—ç‰¹å¾
        features = {
            'first_zero_crossing': 0,
            'decay_rate': 0.1,
            'periodicity': 0
        }
        
        # ç¬¬ä¸€ä¸ªé›¶äº¤å‰ç‚¹
        zero_crossings = np.where(np.diff(np.sign(autocorr)))[0]
        if len(zero_crossings) > 0:
            features['first_zero_crossing'] = zero_crossings[0]
        
        # è¡°å‡ç‡
        if len(autocorr) > 10:
            positive_autocorr = autocorr[autocorr > 0]
            if len(positive_autocorr) > 1:
                features['decay_rate'] = -np.polyfit(range(len(positive_autocorr)), 
                                                   np.log(positive_autocorr), 1)[0]
        
        return features
        
    except Exception:
        return {'first_zero_crossing': 0, 'decay_rate': 0.1, 'periodicity': 0}

def classify_brittleness_intelligent(chaos_indicators: dict, glucose_data: np.ndarray) -> Tuple[str, str, float, dict]:
    """æ™ºèƒ½è„†æ€§åˆ†å‹ - åŸºäºå¤šç»´æ··æ²ŒåŠ¨åŠ›å­¦æŒ‡æ ‡"""
    
    lyapunov = chaos_indicators.get('lyapunov_exponent', 0)
    entropy = chaos_indicators.get('approximate_entropy', 0)
    hurst = chaos_indicators.get('hurst_exponent', 0.5)
    cv = chaos_indicators.get('cv_percent', 0)
    shannon = chaos_indicators.get('shannon_entropy', 0)
    fractal_dim = chaos_indicators.get('fractal_dimension', 1.0)
    
    # æ”¹è¿›çš„è„†æ€§åˆ†å‹é˜ˆå€¼
    thresholds = {
        'lyapunov_chaotic': 0.01,      # æ··æ²Œé˜ˆå€¼
        'lyapunov_stable': -0.005,     # ç¨³å®šé˜ˆå€¼
        'cv_extreme': 60,              # æé«˜å˜å¼‚
        'cv_high': 50,                 # é«˜å˜å¼‚
        'cv_moderate': 35,             # ä¸­ç­‰å˜å¼‚
        'cv_stable': 20,               # ç¨³å®šå˜å¼‚
        'entropy_high': 0.8,           # é«˜ç†µé˜ˆå€¼
        'entropy_moderate': 0.4,       # ä¸­ç­‰ç†µé˜ˆå€¼
        'hurst_memory_loss': 0.35,     # è®°å¿†ç¼ºå¤±é˜ˆå€¼
        'hurst_random': 0.45,          # éšæœºæ¸¸èµ°ä¸‹ç•Œ
        'hurst_persistent_low': 0.55,  # æŒç»­æ€§ä¸‹ç•Œ
        'hurst_persistent_high': 0.65, # æŒç»­æ€§ä¸Šç•Œ
        'shannon_high': 5.0,           # é«˜Shannonç†µ
        'fractal_complex': 1.5         # å¤æ‚åˆ†å½¢ç»´åº¦
    }
    
    # å¤šç»´è¯„åˆ†ç³»ç»Ÿ
    scores = {
        'chaos_score': 0,      # æ··æ²Œç‰¹å¾è¯„åˆ†
        'memory_score': 0,     # è®°å¿†ç‰¹å¾è¯„åˆ†
        'variability_score': 0,# å˜å¼‚æ€§è¯„åˆ†
        'frequency_score': 0   # é¢‘åŸŸç‰¹å¾è¯„åˆ†
    }
    
    # 1. æ··æ²Œç‰¹å¾è¯„åˆ†
    if lyapunov > thresholds['lyapunov_chaotic']:
        scores['chaos_score'] += 3  # å¼ºæ··æ²Œ
    elif lyapunov > 0:
        scores['chaos_score'] += 2  # å¼±æ··æ²Œ
    elif lyapunov < thresholds['lyapunov_stable']:
        scores['chaos_score'] -= 1  # ç¨³å®šç³»ç»Ÿ
    
    if entropy > thresholds['entropy_high']:
        scores['chaos_score'] += 2
    elif entropy > thresholds['entropy_moderate']:
        scores['chaos_score'] += 1
    
    if shannon > thresholds['shannon_high']:
        scores['chaos_score'] += 1
    
    # 2. è®°å¿†ç‰¹å¾è¯„åˆ†
    if hurst < thresholds['hurst_memory_loss']:
        scores['memory_score'] = -3  # ä¸¥é‡è®°å¿†ç¼ºå¤±
    elif hurst < thresholds['hurst_random']:
        scores['memory_score'] = -2  # è®°å¿†ç¼ºå¤±
    elif hurst > thresholds['hurst_persistent_high']:
        scores['memory_score'] = 3   # å¼ºæŒç»­æ€§è®°å¿†
    elif hurst > thresholds['hurst_persistent_low']:
        scores['memory_score'] = 2   # æŒç»­æ€§è®°å¿†
    else:
        scores['memory_score'] = 0   # éšæœºæ¸¸èµ°
    
    # 3. å˜å¼‚æ€§è¯„åˆ†
    if cv > thresholds['cv_extreme']:
        scores['variability_score'] = 4  # æé«˜å˜å¼‚
    elif cv > thresholds['cv_high']:
        scores['variability_score'] = 3  # é«˜å˜å¼‚
    elif cv > thresholds['cv_moderate']:
        scores['variability_score'] = 2  # ä¸­ç­‰å˜å¼‚
    elif cv > thresholds['cv_stable']:
        scores['variability_score'] = 1  # è½»åº¦å˜å¼‚
    else:
        scores['variability_score'] = 0  # ç¨³å®š
    
    # 4. é¢‘åŸŸç‰¹å¾è¯„åˆ†ï¼ˆåŸºäºåˆ†å½¢ç»´åº¦å’Œè‡ªç›¸å…³ï¼‰
    if fractal_dim > thresholds['fractal_complex']:
        scores['frequency_score'] += 2
    elif fractal_dim > 1.2:
        scores['frequency_score'] += 1
    
    # æ™ºèƒ½åˆ†å‹å†³ç­–é€»è¾‘
    brittleness_type, severity, severity_score = decide_brittleness_type_intelligent(scores, chaos_indicators)
    
    return brittleness_type, severity, severity_score, scores

def decide_brittleness_type_intelligent(scores: dict, chaos_indicators: dict) -> Tuple[str, str, float]:
    """æ™ºèƒ½è„†æ€§åˆ†å‹å†³ç­–"""
    
    chaos_score = scores['chaos_score']
    memory_score = scores['memory_score'] 
    variability_score = scores['variability_score']
    frequency_score = scores['frequency_score']
    
    cv = chaos_indicators.get('cv_percent', 0)
    lyapunov = chaos_indicators.get('lyapunov_exponent', 0)
    hurst = chaos_indicators.get('hurst_exponent', 0.5)
    entropy = chaos_indicators.get('approximate_entropy', 0)
    
    # è„†æ€§åˆ†å‹å†³ç­–æ ‘
    if chaos_score >= 4 and variability_score >= 3:
        # å¼ºæ··æ²Œç‰¹å¾ + é«˜å˜å¼‚
        brittleness_type = "Iå‹æ··æ²Œè„†æ€§"
        severity = "æé«˜è„†æ€§"
        base_score = 90
        
    elif memory_score <= -2 and variability_score >= 1:
        # è®°å¿†ç¼ºå¤± + ä¸€å®šå˜å¼‚
        brittleness_type = "IVå‹è®°å¿†ç¼ºå¤±è„†æ€§"
        if memory_score <= -3:
            severity = "é‡åº¦è®°å¿†ç¼ºå¤±è„†æ€§"
            base_score = 75
        else:
            severity = "ä¸­åº¦è®°å¿†ç¼ºå¤±è„†æ€§"
            base_score = 65
            
    elif chaos_score >= 2 and variability_score >= 2 and abs(memory_score) <= 1:
        # ä¸­ç­‰æ··æ²Œ + ä¸­ç­‰å˜å¼‚ + è®°å¿†æ­£å¸¸
        brittleness_type = "IIIå‹éšæœºè„†æ€§"
        severity = "é«˜éšæœºè„†æ€§"
        base_score = 70
        
    elif frequency_score >= 2 or (variability_score >= 2 and 0.55 <= hurst <= 0.65):
        # é¢‘åŸŸç‰¹å¾æ˜æ˜¾æˆ–å‡†å‘¨æœŸç‰¹å¾
        if abs(memory_score) <= 1 and chaos_score <= 2:
            brittleness_type = "IIå‹å‡†å‘¨æœŸè„†æ€§"
            severity = "å‡†å‘¨æœŸè„†æ€§"
            base_score = 60
        else:
            brittleness_type = "Vå‹é¢‘åŸŸè„†æ€§"
            severity = "é¢‘åŸŸè„†æ€§"
            base_score = 55
            
    elif lyapunov < -0.01 and variability_score >= 1 and frequency_score >= 1:
        # ç³»ç»Ÿç¨³å®šä½†æœ‰ç‰¹å®šé¢‘åŸŸé—®é¢˜
        brittleness_type = "Vå‹é¢‘åŸŸè„†æ€§"
        severity = "è½»åº¦é¢‘åŸŸè„†æ€§"
        base_score = 45
        
    elif variability_score == 0 and chaos_score <= 1 and abs(memory_score) <= 1:
        # ä½å˜å¼‚ + ä½æ··æ²Œ + è®°å¿†æ­£å¸¸
        brittleness_type = "ç¨³å®šå‹"
        severity = "è¡€ç³–è°ƒèŠ‚ç¨³å®š"
        base_score = 25
        
    else:
        # é»˜è®¤åˆ†ç±»
        if variability_score >= 2:
            brittleness_type = "IIIå‹éšæœºè„†æ€§"
            severity = "ä¸­ç­‰éšæœºè„†æ€§"
            base_score = 50
        else:
            brittleness_type = "IIå‹å‡†å‘¨æœŸè„†æ€§"
            severity = "è½»åº¦å‡†å‘¨æœŸè„†æ€§"
            base_score = 40
    
    # ç»†åŒ–ä¸¥é‡ç¨‹åº¦è¯„åˆ†
    severity_adjustments = {
        'cv_bonus': min(20, cv / 3),
        'entropy_bonus': min(15, entropy * 30),
        'lyapunov_bonus': min(15, abs(lyapunov) * 500),
        'hurst_penalty': abs(hurst - 0.5) * 20
    }
    
    final_score = base_score + sum(severity_adjustments.values())
    final_score = np.clip(final_score, 0, 100)
    
    return brittleness_type, severity, final_score

def analyze_temporal_brittleness_patterns(df: pd.DataFrame, glucose_data: np.ndarray) -> dict:
    """24å°æ—¶æ—¶æ®µè„†æ€§åˆ†æ"""
    
    temporal_analysis = {
        "analysis_method": "åŸºäº24å°æ—¶è¡€ç³–æ¨¡å¼çš„æ™ºèƒ½æ—¶æ®µè„†æ€§é£é™©è¯„ä¼°",
        "time_segments": {},
        "temporal_patterns": {},
        "circadian_analysis": {}
    }
    
    # å¦‚æœæœ‰æ—¶é—´æˆ³ï¼Œè¿›è¡ŒçœŸå®çš„æ—¶æ®µåˆ†æ
    if 'timestamp' in df.columns and len(df) > 50:
        df['hour'] = df['timestamp'].dt.hour
        
        # å®šä¹‰æ—¶æ®µ
        time_segments = {
            "æ·±å¤œæ—¶æ®µ(00:00-06:00)": [0, 1, 2, 3, 4, 5],
            "é»æ˜æ—¶æ®µ(06:00-09:00)": [6, 7, 8],
            "ä¸Šåˆæ—¶æ®µ(09:00-12:00)": [9, 10, 11],
            "ä¸‹åˆæ—¶æ®µ(12:00-15:00)": [12, 13, 14],
            "å‚æ™šæ—¶æ®µ(15:00-18:00)": [15, 16, 17],
            "å¤œé—´æ—¶æ®µ(18:00-24:00)": [18, 19, 20, 21, 22, 23]
        }
        
        segment_brittleness_scores = {}
        
        for segment_name, hours in time_segments.items():
            segment_data = df[df['hour'].isin(hours)]['glucose_value']
            
            if len(segment_data) > 10:
                # è®¡ç®—æ—¶æ®µè„†æ€§æŒ‡æ ‡
                segment_analysis = analyze_segment_brittleness_intelligent(segment_data.values)
                temporal_analysis["time_segments"][segment_name] = segment_analysis
                segment_brittleness_scores[segment_name] = segment_analysis.get('brittleness_score', 0)
        
        # æ˜¼å¤œèŠ‚å¾‹åˆ†æ
        temporal_analysis["circadian_analysis"] = analyze_circadian_brittleness(df)
        
        # æ—¶æ®µæ¨¡å¼è¯†åˆ«
        temporal_analysis["temporal_patterns"] = identify_temporal_patterns(segment_brittleness_scores)
        
    else:
        # ç®€åŒ–åˆ†æ
        overall_cv = (np.std(glucose_data) / np.mean(glucose_data)) * 100
        temporal_analysis["time_segments"] = generate_simplified_temporal_analysis(overall_cv)
    
    return temporal_analysis

def analyze_segment_brittleness_intelligent(segment_data: np.ndarray) -> dict:
    """æ™ºèƒ½åˆ†æ®µè„†æ€§åˆ†æ"""
    
    if len(segment_data) < 5:
        return {"error": "æ•°æ®ç‚¹ä¸è¶³"}
    
    # åŸºç¡€æŒ‡æ ‡
    mean_glucose = np.mean(segment_data)
    std_glucose = np.std(segment_data)
    cv = (std_glucose / mean_glucose) * 100
    tir = ((segment_data >= 3.9) & (segment_data <= 10.0)).sum() / len(segment_data) * 100
    
    # ç®€åŒ–çš„æ··æ²ŒæŒ‡æ ‡
    segment_chaos = {
        'cv_percent': cv,
        'approximate_entropy': calculate_approximate_entropy(segment_data, m=2),
        'variability_index': np.std(np.diff(segment_data))
    }
    
    # è„†æ€§è¯„åˆ†
    brittleness_score = 0
    if cv > 45:
        brittleness_score += 30
        brittleness_level = "æé«˜è„†æ€§"
    elif cv > 35:
        brittleness_score += 25
        brittleness_level = "é«˜è„†æ€§"
    elif cv > 25:
        brittleness_score += 15
        brittleness_level = "ä¸­ç­‰è„†æ€§"
    else:
        brittleness_score += 5
        brittleness_level = "ä½è„†æ€§"
    
    # TIRå½±å“
    if tir < 50:
        brittleness_score += 15
    elif tir < 70:
        brittleness_score += 10
    
    # ç†µå½±å“
    entropy = segment_chaos['approximate_entropy']
    if entropy > 0.6:
        brittleness_score += 10
    
    # ç¡®å®šé£é™©ç­‰çº§
    if brittleness_score >= 50:
        risk_level = "é«˜é£é™©"
    elif brittleness_score >= 30:
        risk_level = "ä¸­ç­‰é£é™©"
    else:
        risk_level = "ä½é£é™©"
    
    return {
        "æ•°æ®ç‚¹æ•°": len(segment_data),
        "å¹³å‡è¡€ç³–": f"{mean_glucose:.1f} mmol/L",
        "å˜å¼‚ç³»æ•°": f"{cv:.1f}%",
        "TIR": f"{tir:.1f}%",
        "é£é™©ç­‰çº§": risk_level,
        "è„†æ€§ç­‰çº§": brittleness_level,
        "brittleness_score": brittleness_score,
        "è„†æ€§ç‰¹å¾": generate_segment_features(cv, mean_glucose, tir, entropy),
        "æ··æ²ŒæŒ‡æ ‡": segment_chaos
    }

def generate_segment_features(cv: float, mean_glucose: float, tir: float, entropy: float) -> List[str]:
    """ç”Ÿæˆæ—¶æ®µè„†æ€§ç‰¹å¾æè¿°"""
    
    features = []
    
    # åŸºäºCVçš„ç‰¹å¾
    if cv > 45:
        features.append("è¯¥æ—¶æ®µå‘ˆç°æé«˜è„†æ€§ç‰¹å¾ï¼Œè¡€ç³–æ³¢åŠ¨å‰§çƒˆ")
    elif cv > 35:
        features.append("è¯¥æ—¶æ®µå­˜åœ¨é«˜è„†æ€§é£é™©ï¼Œå˜å¼‚æ€§æ˜¾è‘—")
    elif cv > 25:
        features.append("è¯¥æ—¶æ®µè„†æ€§ç¨‹åº¦ä¸­ç­‰ï¼Œéœ€è¦å…³æ³¨")
    else:
        features.append("è¯¥æ—¶æ®µè¡€ç³–ç›¸å¯¹ç¨³å®šï¼Œè„†æ€§è¾ƒä½")
    
    # åŸºäºå¹³å‡è¡€ç³–çš„ç‰¹å¾
    if mean_glucose > 15:
        features.append("å¹³å‡è¡€ç³–ä¸¥é‡åé«˜ï¼Œå­˜åœ¨é…®ç—‡é£é™©")
    elif mean_glucose > 12:
        features.append("å¹³å‡è¡€ç³–åé«˜ï¼Œå­˜åœ¨æŒç»­é«˜è¡€ç³–é£é™©")
    elif mean_glucose > 10:
        features.append("å¹³å‡è¡€ç³–è½»åº¦åé«˜ï¼Œéœ€è¦è°ƒæ•´æ²»ç–—")
    elif mean_glucose < 6:
        features.append("å¹³å‡è¡€ç³–åä½ï¼Œå­˜åœ¨ä½è¡€ç³–é£é™©")
    
    # åŸºäºTIRçš„ç‰¹å¾
    if tir < 50:
        features.append("ç›®æ ‡èŒƒå›´æ—¶é—´ä¸¥é‡ä¸è¶³ï¼Œè¡€ç³–æ§åˆ¶ä¸ä½³")
    elif tir < 70:
        features.append("ç›®æ ‡èŒƒå›´æ—¶é—´ä¸è¶³ï¼Œéœ€è¦æ²»ç–—ä¼˜åŒ–")
    elif tir > 85:
        features.append("ç›®æ ‡èŒƒå›´æ—¶é—´ä¼˜ç§€ï¼Œè¡€ç³–æ§åˆ¶è‰¯å¥½")
    
    # åŸºäºç†µçš„ç‰¹å¾
    if entropy > 0.8:
        features.append("è¡€ç³–æ¨¡å¼å¤æ‚ï¼Œéš¾ä»¥é¢„æµ‹ï¼Œå»ºè®®å¯†åˆ‡ç›‘æµ‹")
    elif entropy > 0.5:
        features.append("è¡€ç³–æ¨¡å¼ä¸­ç­‰å¤æ‚ï¼Œå­˜åœ¨ä¸€å®šä¸å¯é¢„æµ‹æ€§")
    
    return features

def analyze_circadian_brittleness(df: pd.DataFrame) -> dict:
    """æ˜¼å¤œèŠ‚å¾‹è„†æ€§åˆ†æ"""
    
    try:
        df['hour'] = df['timestamp'].dt.hour
        
        # æ˜¼å¤œåˆ†ç»„
        day_hours = range(6, 22)  # 06:00-22:00
        night_hours = list(range(0, 6)) + list(range(22, 24))  # 22:00-06:00
        
        day_data = df[df['hour'].isin(day_hours)]['glucose_value']
        night_data = df[df['hour'].isin(night_hours)]['glucose_value']
        
        analysis = {}
        
        if len(day_data) > 10 and len(night_data) > 10:
            day_cv = (day_data.std() / day_data.mean()) * 100
            night_cv = (night_data.std() / night_data.mean()) * 100
            
            analysis = {
                "æ˜¼é—´è„†æ€§": {
                    "å˜å¼‚ç³»æ•°": f"{day_cv:.1f}%",
                    "å¹³å‡è¡€ç³–": f"{day_data.mean():.1f} mmol/L",
                    "è„†æ€§ç­‰çº§": "é«˜è„†æ€§" if day_cv > 35 else "ä¸­ç­‰è„†æ€§" if day_cv > 25 else "ä½è„†æ€§"
                },
                "å¤œé—´è„†æ€§": {
                    "å˜å¼‚ç³»æ•°": f"{night_cv:.1f}%",
                    "å¹³å‡è¡€ç³–": f"{night_data.mean():.1f} mmol/L",
                    "è„†æ€§ç­‰çº§": "é«˜è„†æ€§" if night_cv > 35 else "ä¸­ç­‰è„†æ€§" if night_cv > 25 else "ä½è„†æ€§"
                },
                "æ˜¼å¤œå·®å¼‚": {
                    "CVå·®å¼‚": f"{day_cv - night_cv:+.1f}%",
                    "è¡€ç³–å·®å¼‚": f"{day_data.mean() - night_data.mean():+.1f} mmol/L",
                    "æ¨¡å¼åˆ†ç±»": classify_circadian_pattern(day_cv, night_cv, day_data.mean(), night_data.mean())
                }
            }
        else:
            analysis = {"error": "æ˜¼å¤œæ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡ŒèŠ‚å¾‹åˆ†æ"}
        
        return analysis
        
    except Exception:
        return {"error": "æ˜¼å¤œèŠ‚å¾‹åˆ†æå¤±è´¥"}

def classify_circadian_pattern(day_cv: float, night_cv: float, day_mean: float, night_mean: float) -> str:
    """åˆ†ç±»æ˜¼å¤œèŠ‚å¾‹æ¨¡å¼"""
    
    if day_cv > night_cv + 10:
        return "æ˜¼é—´è„†æ€§å‹ - ç™½å¤©è¡€ç³–æ›´ä¸ç¨³å®š"
    elif night_cv > day_cv + 10:
        return "å¤œé—´è„†æ€§å‹ - å¤œé—´è¡€ç³–æ›´ä¸ç¨³å®š"
    elif abs(day_cv - night_cv) <= 10:
        return "å‡åŒ€è„†æ€§å‹ - æ˜¼å¤œè„†æ€§ç›¸å½“"
    else:
        return "èŠ‚å¾‹æ­£å¸¸å‹ - æ˜¼å¤œå·®å¼‚è½»å¾®"

def identify_temporal_patterns(segment_scores: dict) -> dict:
    """è¯†åˆ«æ—¶é—´æ¨¡å¼"""
    
    if not segment_scores:
        return {"error": "æ— æ—¶æ®µæ•°æ®"}
    
    scores = list(segment_scores.values())
    segments = list(segment_scores.keys())
    
    # æ‰¾å‡ºæœ€é«˜å’Œæœ€ä½é£é™©æ—¶æ®µ
    max_risk_idx = scores.index(max(scores))
    min_risk_idx = scores.index(min(scores))
    
    patterns = {
        "æœ€é«˜é£é™©æ—¶æ®µ": segments[max_risk_idx],
        "æœ€ä½é£é™©æ—¶æ®µ": segments[min_risk_idx],
        "é£é™©å·®å¼‚": max(scores) - min(scores),
        "å¹³å‡è„†æ€§": np.mean(scores),
        "è„†æ€§ç¨³å®šæ€§": "ç¨³å®š" if np.std(scores) < 10 else "ä¸ç¨³å®š"
    }
    
    # æ¨¡å¼åˆ†ç±»
    if max(scores) > 40:
        if segments[max_risk_idx] in ["æ·±å¤œæ—¶æ®µ(00:00-06:00)", "é»æ˜æ—¶æ®µ(06:00-09:00)"]:
            patterns["ä¸»è¦æ¨¡å¼"] = "å¤œé—´/é»æ˜é«˜è„†æ€§æ¨¡å¼"
        else:
            patterns["ä¸»è¦æ¨¡å¼"] = "æ—¥é—´é«˜è„†æ€§æ¨¡å¼"
    else:
        patterns["ä¸»è¦æ¨¡å¼"] = "å…¨å¤©ä½è„†æ€§æ¨¡å¼"
    
    return patterns

def generate_simplified_temporal_analysis(overall_cv: float) -> dict:
    """ç”Ÿæˆç®€åŒ–çš„æ—¶æ®µåˆ†æ"""
    
    risk_level = "é«˜é£é™©" if overall_cv > 40 else "ä¸­ç­‰é£é™©" if overall_cv > 25 else "ä½é£é™©"
    
    return {
        "æ·±å¤œæ—¶æ®µ(00:00-06:00)": {
            "é£é™©ç­‰çº§": risk_level,
            "ä¸»è¦ç‰¹å¾": "å¤œé—´è¡€ç³–è°ƒèŠ‚ç›¸å¯¹ç¨³å®š" if overall_cv < 30 else "å¤œé—´å­˜åœ¨è„†æ€§é£é™©",
            "ç®¡ç†é‡ç‚¹": "ç›‘æµ‹å¤œé—´ä½è¡€ç³–é£é™©"
        },
        "é»æ˜æ—¶æ®µ(06:00-09:00)": {
            "é£é™©ç­‰çº§": "é«˜é£é™©" if overall_cv > 30 else "ä¸­ç­‰é£é™©",
            "ä¸»è¦ç‰¹å¾": "é»æ˜ç°è±¡å¯èƒ½å½±å“è¡€ç³–ç¨³å®šæ€§",
            "ç®¡ç†é‡ç‚¹": "ä¼˜åŒ–åŸºç¡€èƒ°å²›ç´ ï¼Œè°ƒæ•´é»æ˜å‰‚é‡"
        },
        "æ—¥é—´æ—¶æ®µ(09:00-18:00)": {
            "é£é™©ç­‰çº§": risk_level,
            "ä¸»è¦ç‰¹å¾": "å—è¿›é¤å’Œæ´»åŠ¨å½±å“ï¼Œå˜å¼‚æ€§å¯èƒ½å¢åŠ ",
            "ç®¡ç†é‡ç‚¹": "é¤å‰èƒ°å²›ç´ å‰‚é‡ä¼˜åŒ–ï¼Œè¿åŠ¨è¡€ç³–ç®¡ç†"
        }
    }

def assess_dynamic_treatment_response(df: pd.DataFrame, glucose_values: np.ndarray) -> dict:
    """åŠ¨æ€æ²»ç–—ååº”è¯„ä¼°"""
    
    # å¦‚æœæ•°æ®é‡è¶³å¤Ÿï¼Œè¿›è¡Œåˆ†æ®µåˆ†æ
    if len(glucose_values) >= 400:  # è‡³å°‘400ä¸ªç‚¹è¿›è¡Œ4æ®µåˆ†æ
        segments = create_time_segments(glucose_values, 4)
        segment_analysis = []
        
        for i, segment in enumerate(segments):
            if len(segment) > 20:
                segment_cv = (np.std(segment) / np.mean(segment)) * 100
                segment_tir = ((segment >= 3.9) & (segment <= 10.0)).sum() / len(segment) * 100
                segment_mean = np.mean(segment)
                
                segment_analysis.append({
                    "æ—¶é—´æ®µ": f"ç¬¬{i+1}æ®µ",
                    "CV": segment_cv,
                    "TIR": segment_tir,
                    "å¹³å‡è¡€ç³–": segment_mean,
                    "æ•°æ®ç‚¹": len(segment)
                })
        
        if len(segment_analysis) >= 3:
            # è®¡ç®—è¶‹åŠ¿
            cvs = [s["CV"] for s in segment_analysis]
            tirs = [s["TIR"] for s in segment_analysis]
            means = [s["å¹³å‡è¡€ç³–"] for s in segment_analysis]
            
            # è¶‹åŠ¿åˆ†æ
            cv_trend = analyze_trend(cvs)
            tir_trend = analyze_trend(tirs)
            glucose_trend = analyze_trend(means, reverse=True)  # è¡€ç³–é™ä½æ˜¯å¥½çš„
            
            # ç»¼åˆè¯„ä¼°
            improvement_indicators = []
            if cv_trend in ["æ˜¾è‘—æ”¹å–„", "æ”¹å–„"]:
                improvement_indicators.append("å˜å¼‚æ€§æ”¹å–„")
            if tir_trend in ["æ˜¾è‘—æ”¹å–„", "æ”¹å–„"]:
                improvement_indicators.append("ç›®æ ‡èŒƒå›´æ”¹å–„")
            if glucose_trend in ["æ˜¾è‘—æ”¹å–„", "æ”¹å–„"]:
                improvement_indicators.append("è¡€ç³–æ°´å¹³æ”¹å–„")
            
            # æ²»ç–—ååº”åˆ†çº§
            if len(improvement_indicators) >= 2:
                response_grade = "ä¼˜ç§€"
                response_type = "æ˜¾è‘—æ²»ç–—ååº”"
                success_prob = "85-95%"
            elif len(improvement_indicators) == 1:
                response_grade = "è‰¯å¥½" 
                response_type = "éƒ¨åˆ†æ²»ç–—ååº”"
                success_prob = "70-85%"
            else:
                response_grade = "éœ€è¦è°ƒæ•´"
                response_type = "æ²»ç–—ååº”ä¸æ˜æ˜¾"
                success_prob = "50-70%"
            
            return {
                "è¯„ä¼°æ–¹æ³•": "åŸºäºæ—¶é—´åˆ†æ®µçš„åŠ¨æ€æ²»ç–—ååº”åˆ†æ",
                "æ—¶é—´æ®µåˆ†æ": segment_analysis,
                "è¶‹åŠ¿è¯„ä¼°": {
                    "å˜å¼‚ç³»æ•°è¶‹åŠ¿": cv_trend,
                    "TIRè¶‹åŠ¿": tir_trend,
                    "è¡€ç³–æ°´å¹³è¶‹åŠ¿": glucose_trend
                },
                "æ²»ç–—ååº”è¯„ä¼°": {
                    "ååº”åˆ†çº§": response_grade,
                    "ååº”ç±»å‹": response_type,
                    "æ”¹å–„ç»´åº¦": improvement_indicators,
                    "æˆåŠŸæ¦‚ç‡": success_prob,
                    "å½“å‰çŠ¶æ€": assess_current_control_status(segment_analysis[-1]) if segment_analysis else "æ— æ³•è¯„ä¼°"
                },
                "é¢„åè¯„ä¼°": generate_prognosis_assessment(response_grade, improvement_indicators)
            }
    
    # ç®€åŒ–è¯„ä¼°
    overall_cv = (np.std(glucose_values) / np.mean(glucose_values)) * 100
    overall_tir = ((glucose_values >= 3.9) & (glucose_values <= 10.0)).sum() / len(glucose_values) * 100
    
    return {
        "è¯„ä¼°æ–¹æ³•": "åŸºäºæ•´ä½“æŒ‡æ ‡çš„ç®€åŒ–æ²»ç–—ååº”è¯„ä¼°",
        "æ•´ä½“æŒ‡æ ‡": {
            "å˜å¼‚ç³»æ•°": f"{overall_cv:.1f}%",
            "TIR": f"{overall_tir:.1f}%",
            "å¹³å‡è¡€ç³–": f"{np.mean(glucose_values):.1f} mmol/L"
        },
        "æ²»ç–—ååº”è¯„ä¼°": {
            "ååº”åˆ†çº§": "è‰¯å¥½" if overall_cv < 30 and overall_tir > 70 else "éœ€è¦ä¼˜åŒ–",
            "ååº”ç±»å‹": "ç¨³å®šçŠ¶æ€",
            "å½“å‰çŠ¶æ€": "ä¼˜ç§€" if overall_cv < 25 and overall_tir > 80 else "è‰¯å¥½" if overall_cv < 35 and overall_tir > 60 else "éœ€æ”¹å–„"
        }
    }

def create_time_segments(data: np.ndarray, num_segments: int) -> List[np.ndarray]:
    """åˆ›å»ºæ—¶é—´åˆ†æ®µ"""
    segment_size = len(data) // num_segments
    segments = []
    
    for i in range(num_segments):
        start_idx = i * segment_size
        end_idx = (i + 1) * segment_size if i < num_segments - 1 else len(data)
        segments.append(data[start_idx:end_idx])
    
    return segments

def calculate_trend_strength(glucose_window: np.ndarray) -> float:
    """è®¡ç®—è¶‹åŠ¿å¼ºåº¦"""
    
    if len(glucose_window) < 5:
        return 0.0
    
    # çº¿æ€§è¶‹åŠ¿å›å½’
    x = np.arange(len(glucose_window))
    slope, _, r_value, _, _ = stats.linregress(x, glucose_window)
    
    # è¶‹åŠ¿å¼ºåº¦ = |æ–œç‡| * RÂ²
    trend_strength = abs(slope) * (r_value ** 2) if not np.isnan(r_value) else 0
    
    return min(10, trend_strength)  # é™åˆ¶åœ¨0-10èŒƒå›´

def analyze_trend(values: List[float], reverse: bool = False) -> str:
    """åˆ†æè¶‹åŠ¿"""
    if len(values) < 2:
        return "æ— æ³•è¯„ä¼°"
    
    first_val = values[0]
    last_val = values[-1]
    
    if reverse:
        # å¯¹äºè¡€ç³–ï¼Œé™ä½æ˜¯æ”¹å–„
        change = first_val - last_val
    else:
        # å¯¹äºTIRï¼Œå¢åŠ æ˜¯æ”¹å–„
        change = last_val - first_val
    
    change_percent = abs(change) / first_val * 100 if first_val != 0 else 0
    
    if change > 0:
        if change_percent > 30:
            return "æ˜¾è‘—æ”¹å–„"
        elif change_percent > 15:
            return "æ”¹å–„"
        elif change_percent > 5:
            return "è½»åº¦æ”¹å–„"
        else:
            return "ç¨³å®š"
    else:
        if change_percent > 15:
            return "æ¶åŒ–"
        else:
            return "ç¨³å®š"

def assess_current_control_status(latest_segment: dict) -> str:
    """è¯„ä¼°å½“å‰æ§åˆ¶çŠ¶æ€"""
    cv = latest_segment["CV"]
    tir = latest_segment["TIR"]
    mean_glucose = latest_segment["å¹³å‡è¡€ç³–"]
    
    if mean_glucose <= 8.0 and tir >= 80 and cv <= 25:
        return "ä¼˜ç§€"
    elif mean_glucose <= 10.0 and tir >= 60 and cv <= 35:
        return "è‰¯å¥½"
    elif mean_glucose <= 12.0 and tir >= 40 and cv <= 45:
        return "éœ€æ”¹å–„"
    else:
        return "è¾ƒå·®"

def generate_prognosis_assessment(response_grade: str, improvement_indicators: List[str]) -> dict:
    """ç”Ÿæˆé¢„åè¯„ä¼°"""
    
    prognosis_map = {
        "ä¼˜ç§€": {
            "æè¿°": "é¢„åä¼˜ç§€ï¼Œè¡€ç³–æ§åˆ¶æœ‰æœ›é•¿æœŸç¨³å®šï¼Œå¹¶å‘ç—‡é£é™©æ˜¾è‘—é™ä½",
            "å…³é”®å› ç´ ": "ç»´æŒå½“å‰æœ‰æ•ˆæ²»ç–—æ–¹æ¡ˆï¼Œæ³¨é‡ç”Ÿæ´»è´¨é‡ç®¡ç†",
            "æ—¶é—´é¢„æœŸ": "çŸ­æœŸå†…å¯è¾¾åˆ°ç†æƒ³æ§åˆ¶ç›®æ ‡ï¼Œé•¿æœŸé¢„åè‰¯å¥½"
        },
        "è‰¯å¥½": {
            "æè¿°": "é¢„åè‰¯å¥½ï¼Œåœ¨ç°æœ‰åŸºç¡€ä¸Šè¿›ä¸€æ­¥ä¼˜åŒ–æœ‰æœ›è¾¾åˆ°ç†æƒ³æ§åˆ¶",
            "å…³é”®å› ç´ ": "ç»§ç»­ç°æœ‰æ²»ç–—å¹¶ç²¾ç»†è°ƒæ•´ï¼ŒåŠ å¼ºæ‚£è€…æ•™è‚²",
            "æ—¶é—´é¢„æœŸ": "3-6ä¸ªæœˆå†…æœ‰æœ›å®ç°æ›´å¥½æ§åˆ¶"
        },
        "éœ€è¦è°ƒæ•´": {
            "æè¿°": "é¢„åå–å†³äºæ²»ç–—æ–¹æ¡ˆè°ƒæ•´æ•ˆæœï¼Œéœ€è¦é‡æ–°åˆ¶å®šç­–ç•¥",
            "å…³é”®å› ç´ ": "å…¨é¢è¯„ä¼°å½“å‰æ–¹æ¡ˆï¼Œè€ƒè™‘å¤šå­¦ç§‘å¹²é¢„",
            "æ—¶é—´é¢„æœŸ": "éœ€è¦1-3ä¸ªæœˆæ²»ç–—è°ƒæ•´æœŸ"
        }
    }
    
    base_assessment = prognosis_map.get(response_grade, prognosis_map["éœ€è¦è°ƒæ•´"])
    
    # æ ¹æ®æ”¹å–„ç»´åº¦è°ƒæ•´
    if len(improvement_indicators) >= 2:
        base_assessment["ä¼˜åŠ¿"] = "å¤šç»´åº¦æŒ‡æ ‡åŒæ—¶æ”¹å–„ï¼Œæ²»ç–—ååº”è‰¯å¥½"
    elif len(improvement_indicators) == 1:
        base_assessment["ä¼˜åŠ¿"] = f"åœ¨{improvement_indicators[0]}æ–¹é¢è¡¨ç°è‰¯å¥½"
    else:
        base_assessment["æŒ‘æˆ˜"] = "å„é¡¹æŒ‡æ ‡æ”¹å–„ä¸æ˜æ˜¾ï¼Œéœ€è¦é‡æ–°è¯„ä¼°æ²»ç–—ç­–ç•¥"
    
    return base_assessment

def get_risk_level(severity_score: float) -> str:
    """æ ¹æ®ä¸¥é‡ç¨‹åº¦è¯„åˆ†ç¡®å®šé£é™©ç­‰çº§"""
    if severity_score >= 80:
        return "ğŸ”´ æé«˜é£é™©"
    elif severity_score >= 65:
        return "ğŸŸ  é«˜é£é™©"
    elif severity_score >= 45:
        return "ğŸŸ¡ ä¸­ç­‰é£é™©"
    elif severity_score >= 25:
        return "ğŸŸ¢ ä½ä¸­é£é™©"
    else:
        return "ğŸŸ¢ ä½é£é™©"

def generate_clinical_features_intelligent(brittleness_type: str, chaos_indicators: dict) -> List[str]:
    """ç”Ÿæˆæ™ºèƒ½ä¸´åºŠç‰¹å¾æè¿°"""
    
    features_map = {
        "Iå‹æ··æ²Œè„†æ€§": [
            f"LyapunovæŒ‡æ•°ä¸º{chaos_indicators.get('lyapunov_exponent', 0):.4f}ï¼Œç³»ç»Ÿå‘ˆç°å¼ºæ··æ²Œç‰¹å¾",
            "è¡€ç³–ç³»ç»Ÿæéš¾é¢„æµ‹ï¼Œæ•æ„Ÿä¾èµ–åˆå§‹æ¡ä»¶",
            "éœ€è¦æœ€å¯†é›†çš„ç›‘æµ‹å’Œé¢‘ç¹çš„æ²»ç–—è°ƒæ•´",
            "å»ºè®®ä½¿ç”¨äººå·¥èƒ°è…ºæˆ–é—­ç¯ç³»ç»Ÿ"
        ],
        "IIå‹å‡†å‘¨æœŸè„†æ€§": [
            f"ç³»ç»Ÿå­˜åœ¨å‡†å‘¨æœŸæŒ¯è¡ï¼ŒHurstæŒ‡æ•°ä¸º{chaos_indicators.get('hurst_exponent', 0.5):.3f}",
            "è¡€ç³–æ¨¡å¼å…·æœ‰ä¸€å®šæ—¶é—´è§„å¾‹æ€§ä½†ä¸å®Œå…¨å‘¨æœŸ",
            "å¯èƒ½ä¸ç”Ÿç‰©èŠ‚å¾‹æˆ–ç”Ÿæ´»æ¨¡å¼ç›¸å…³",
            "é€‚åˆé‡‡ç”¨æ—¶é—´æ²»ç–—å­¦æ–¹æ³•"
        ],
        "IIIå‹éšæœºè„†æ€§": [
            f"è¿‘ä¼¼ç†µä¸º{chaos_indicators.get('approximate_entropy', 0):.3f}ï¼Œå‘ˆç°é«˜éšæœºæ€§",
            "è¡€ç³–å˜åŒ–ç¼ºä¹æ˜æ˜¾è§„å¾‹ï¼Œå—å¤šé‡å› ç´ å½±å“",
            "éœ€è¦ç»¼åˆæ€§ç¨³å®šåŒ–æ²»ç–—ç­–ç•¥",
            "é‡ç‚¹å…³æ³¨ç¯å¢ƒå’Œç”Ÿæ´»æ–¹å¼çš„å½±å“"
        ],
        "IVå‹è®°å¿†ç¼ºå¤±è„†æ€§": [
            f"HurstæŒ‡æ•°ä¸º{chaos_indicators.get('hurst_exponent', 0.5):.3f}(<0.45)ï¼Œå‘ˆåæŒç»­æ€§",
            "è¡€ç³–ç³»ç»Ÿè®°å¿†åŠŸèƒ½å—æŸï¼Œå†å²ä¾èµ–æ€§å¼±",
            "è‚ç³–åŸè°ƒèŠ‚èƒ½åŠ›å¯èƒ½ä¸‹é™",
            "é€‚åˆé•¿æ•ˆã€ç¨³å®šçš„æ²»ç–—æ–¹æ¡ˆ"
        ],
        "Vå‹é¢‘åŸŸè„†æ€§": [
            f"åˆ†å½¢ç»´åº¦ä¸º{chaos_indicators.get('fractal_dimension', 1.0):.3f}ï¼Œå­˜åœ¨é¢‘åŸŸä¸ç¨³å®šæ€§",
            "è¡€ç³–ç³»ç»Ÿåœ¨ç‰¹å®šé¢‘ç‡èŒƒå›´è¡¨ç°å¼‚å¸¸",
            "å¯èƒ½ä¸èƒ°å²›ç´ åˆ†æ³ŒèŠ‚å¾‹ç›¸å…³",
            "éœ€è¦é¢‘åŸŸåˆ†ææŒ‡å¯¼ä¸‹çš„ç²¾å‡†æ²»ç–—"
        ],
        "ç¨³å®šå‹": [
            f"å˜å¼‚ç³»æ•°ä¸º{chaos_indicators.get('cv_percent', 0):.1f}%ï¼Œç³»ç»Ÿç›¸å¯¹ç¨³å®š",
            "æ··æ²ŒæŒ‡æ ‡å‡åœ¨æ­£å¸¸èŒƒå›´å†…",
            "æ²»ç–—ååº”è‰¯å¥½ä¸”å¯é¢„æµ‹",
            "ç»´æŒç°æœ‰æ²»ç–—æ–¹æ¡ˆï¼Œé‡ç‚¹å…³æ³¨é•¿æœŸç®¡ç†"
        ]
    }
    
    return features_map.get(brittleness_type, ["éœ€è¦è¿›ä¸€æ­¥åˆ†æè„†æ€§ç‰¹å¾"])

def get_pathophysiology_mechanism_intelligent(brittleness_type: str, chaos_indicators: dict) -> str:
    """è·å–æ™ºèƒ½ç—…ç†ç”Ÿç†æœºåˆ¶è§£é‡Š"""
    
    cv = chaos_indicators.get('cv_percent', 0)
    lyapunov = chaos_indicators.get('lyapunov_exponent', 0)
    hurst = chaos_indicators.get('hurst_exponent', 0.5)
    entropy = chaos_indicators.get('approximate_entropy', 0)
    
    base_mechanisms = {
        "Iå‹æ··æ²Œè„†æ€§": "èƒ°å²›Î²ç»†èƒåŠŸèƒ½ä¸¥é‡å—æŸï¼Œèƒ°å²›ç´ åˆ†æ³Œå‘ˆæ··æ²ŒåŠ¨æ€ï¼›å¤šé‡è°ƒèŠ‚ç³»ç»Ÿå¤±è°ƒå¯¼è‡´éçº¿æ€§åé¦ˆå›è·¯ï¼›åè°ƒèŠ‚æ¿€ç´ åˆ†æ³Œç´Šä¹±ï¼Œç³»ç»Ÿå¤„äºæ··æ²Œè¾¹ç¼˜ã€‚",
        "IIå‹å‡†å‘¨æœŸè„†æ€§": "ç”Ÿç‰©é’Ÿè°ƒèŠ‚ä¸­æ¢åŠŸèƒ½å¼‚å¸¸ï¼Œèƒ°å²›ç´ åˆ†æ³Œå‘ˆå‡†å‘¨æœŸæ¨¡å¼ï¼›ä¸‹ä¸˜è„‘-å‚ä½“-èƒ°å²›è½´èŠ‚å¾‹ç´Šä¹±ï¼›å¯èƒ½ä¸è¤ªé»‘ç´ ã€çš®è´¨é†‡ç­‰æ¿€ç´ èŠ‚å¾‹å¼‚å¸¸ç›¸å…³ã€‚",
        "IIIå‹éšæœºè„†æ€§": "å¤šé‡éšæœºå› ç´ å åŠ æ•ˆåº”ï¼ŒåŒ…æ‹¬èƒ°å²›Î²ç»†èƒåŠŸèƒ½æ³¢åŠ¨ã€èƒ°å²›ç´ æ•æ„Ÿæ€§éšæœºå˜åŒ–ï¼›è‚ é“å¾®ç”Ÿç‰©ç¾¤è½ä¸ç¨³å®šï¼Œå½±å“ç³–ä»£è°¢ï¼›ç¯å¢ƒå› ç´ å’Œåº”æ¿€ååº”çš„éšæœºå½±å“ã€‚",
        "IVå‹è®°å¿†ç¼ºå¤±è„†æ€§": "è‚ç³–åŸåˆæˆé…¶å’Œåˆ†è§£é…¶æ´»æ€§å¼‚å¸¸ï¼Œå‚¨ç³–-é‡Šç³–åŠŸèƒ½å¤±è°ƒï¼›èƒ°å²›ç´ ä¿¡å·ä¼ å¯¼é€šè·¯ä¸­é•¿æœŸè®°å¿†æœºåˆ¶å—æŸï¼›å¯èƒ½ä¸AMPKã€mTORç­‰ä»£è°¢è®°å¿†ç›¸å…³è›‹ç™½å¼‚å¸¸æœ‰å…³ã€‚",
        "Vå‹é¢‘åŸŸè„†æ€§": "èƒ°å²›Î²ç»†èƒé’™ç¦»å­é€šé“æŒ¯è¡å¼‚å¸¸ï¼Œç‰¹å®šé¢‘ç‡èƒ°å²›ç´ è„‰å†²åˆ†æ³Œéšœç¢ï¼›èƒ°å²›å¾®å¾ªç¯è¡€æµåœ¨ç‰¹å®šé¢‘åŸŸå­˜åœ¨å¼‚å¸¸ï¼›ç¥ç»-å†…åˆ†æ³Œè°ƒèŠ‚çš„é¢‘åŸŸç‰¹å¼‚æ€§åŠŸèƒ½éšœç¢ã€‚",
        "ç¨³å®šå‹": "èƒ°å²›Î²ç»†èƒåŠŸèƒ½ç›¸å¯¹å®Œå¥½ï¼Œèƒ°å²›ç´ åˆ†æ³Œæ¨¡å¼ç¨³å®šï¼›è‚ç³–åŸè°ƒèŠ‚æ­£å¸¸ï¼Œç³–å¼‚ç”Ÿå’Œç³–é…µè§£å¹³è¡¡è‰¯å¥½ï¼›å„è°ƒèŠ‚ç³»ç»Ÿåè°ƒå·¥ä½œï¼Œç»´æŒç¨³æ€ã€‚"
    }
    
    base_mechanism = base_mechanisms.get(brittleness_type, "éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶ç›¸å…³ç—…ç†ç”Ÿç†æœºåˆ¶ã€‚")
    
    # æ ¹æ®å…·ä½“æŒ‡æ ‡æ·»åŠ è¯¦ç»†è§£é‡Š
    additional_info = []
    
    if cv > 50:
        additional_info.append(f"æé«˜å˜å¼‚ç³»æ•°({cv:.1f}%)æç¤ºèƒ°å²›ç´ ä½œç”¨æ•ˆæœé«˜åº¦ä¸ç¨³å®š")
    
    if lyapunov > 0.01:
        additional_info.append(f"æ­£LyapunovæŒ‡æ•°({lyapunov:.4f})è¯å®ç³»ç»Ÿæ··æ²Œç‰¹å¾")
    
    if hurst < 0.3:
        additional_info.append(f"æä½HurstæŒ‡æ•°({hurst:.3f})è¡¨æ˜ä¸¥é‡çš„åæŒç»­æ€§å’Œè®°å¿†ç¼ºå¤±")
    
    if entropy > 0.8:
        additional_info.append(f"é«˜è¿‘ä¼¼ç†µ({entropy:.3f})åæ˜ è¡€ç³–æ¨¡å¼æåº¦å¤æ‚éš¾é¢„æµ‹")
    
    if additional_info:
        return base_mechanism + " å…·ä½“è¡¨ç°ï¼š" + "ï¼›".join(additional_info) + "ã€‚"
    else:
        return base_mechanism

def generate_intelligent_interpretation(brittleness_type: str, severity_score: float, 
                                      chaos_indicators: dict, treatment_response: dict) -> dict:
    """ç”Ÿæˆæ™ºèƒ½ç»¼åˆè§£è¯»"""
    
    # ä¸»è¦å‘ç°
    main_findings = f"æ‚£è€…è¡€ç³–ç³»ç»Ÿåˆ†å‹ä¸º{brittleness_type}ï¼Œä¸¥é‡ç¨‹åº¦è¯„åˆ†{severity_score:.1f}åˆ†"
    
    # æ··æ²ŒåŠ¨åŠ›å­¦è§£è¯»
    chaos_interpretation = generate_chaos_interpretation(chaos_indicators)
    
    # æ²»ç–—ååº”è§£è¯»
    response_assessment = treatment_response.get("æ²»ç–—ååº”è¯„ä¼°", {})
    treatment_interpretation = f"æ²»ç–—ååº”{response_assessment.get('ååº”åˆ†çº§', 'æœªçŸ¥')}ï¼Œ{response_assessment.get('ååº”ç±»å‹', '')}"
    
    # é¢„ååˆ¤æ–­
    prognosis = generate_comprehensive_prognosis(brittleness_type, severity_score, treatment_response)
    
    # å…³é”®é£é™©å› ç´ 
    risk_factors = identify_key_risk_factors(brittleness_type, chaos_indicators, severity_score)
    
    return {
        "ä¸»è¦å‘ç°": main_findings,
        "æ··æ²ŒåŠ¨åŠ›å­¦è§£è¯»": chaos_interpretation,
        "æ²»ç–—ååº”è§£è¯»": treatment_interpretation,
        "ç»¼åˆé¢„ååˆ¤æ–­": prognosis,
        "å…³é”®é£é™©å› ç´ ": risk_factors,
        "ä¸´åºŠå†³ç­–å»ºè®®": generate_clinical_decision_advice(brittleness_type, severity_score, response_assessment)
    }

def generate_chaos_interpretation(chaos_indicators: dict) -> str:
    """ç”Ÿæˆæ··æ²ŒåŠ¨åŠ›å­¦è§£è¯»"""
    
    lyapunov = chaos_indicators.get('lyapunov_exponent', 0)
    entropy = chaos_indicators.get('approximate_entropy', 0)
    hurst = chaos_indicators.get('hurst_exponent', 0.5)
    fractal_dim = chaos_indicators.get('fractal_dimension', 1.0)
    
    interpretations = []
    
    # Lyapunovè§£è¯»
    if lyapunov > 0.01:
        interpretations.append("ç³»ç»Ÿå‘ˆç°æ˜¾è‘—æ··æ²Œç‰¹å¾ï¼Œé•¿æœŸé¢„æµ‹æå…¶å›°éš¾")
    elif lyapunov > 0:
        interpretations.append("ç³»ç»Ÿå­˜åœ¨è½»åº¦æ··æ²Œå€¾å‘ï¼Œé¢„æµ‹æ€§æœ‰é™")
    else:
        interpretations.append("ç³»ç»Ÿç›¸å¯¹ç¨³å®šï¼Œå…·æœ‰ä¸€å®šå¯é¢„æµ‹æ€§")
    
    # Hurstè§£è¯»
    if hurst < 0.35:
        interpretations.append("ç³»ç»Ÿè®°å¿†åŠŸèƒ½ä¸¥é‡å—æŸï¼Œå‘ˆå¼ºåæŒç»­æ€§")
    elif hurst < 0.45:
        interpretations.append("ç³»ç»Ÿè®°å¿†åŠŸèƒ½å—æŸï¼Œå†å²ä¿¡æ¯å¯¹æœªæ¥å½±å“å¾®å¼±")
    elif hurst > 0.65:
        interpretations.append("ç³»ç»Ÿå…·æœ‰é•¿ç¨‹è®°å¿†ç‰¹å¾ï¼Œå†å²æ¨¡å¼å½±å“æœªæ¥")
    else:
        interpretations.append("ç³»ç»Ÿå‘ˆéšæœºæ¸¸èµ°ç‰¹å¾ï¼Œè®°å¿†åŠŸèƒ½åŸºæœ¬æ­£å¸¸")
    
    # ç†µè§£è¯»
    if entropy > 0.8:
        interpretations.append("è¡€ç³–æ¨¡å¼æåº¦å¤æ‚ï¼Œä¿¡æ¯ç†µå¾ˆé«˜ï¼Œéš¾ä»¥å»ºç«‹é¢„æµ‹æ¨¡å‹")
    elif entropy > 0.5:
        interpretations.append("è¡€ç³–æ¨¡å¼ä¸­ç­‰å¤æ‚ï¼Œå­˜åœ¨ä¸€å®šè§„å¾‹æ€§ä½†é¢„æµ‹å›°éš¾")
    else:
        interpretations.append("è¡€ç³–æ¨¡å¼ç›¸å¯¹ç®€å•ï¼Œå­˜åœ¨å¯è¯†åˆ«çš„è§„å¾‹æ€§")
    
    return "ï¼›".join(interpretations) + "ã€‚"

def generate_comprehensive_prognosis(brittleness_type: str, severity_score: float, treatment_response: dict) -> str:
    """ç”Ÿæˆç»¼åˆé¢„ååˆ¤æ–­"""
    
    response_grade = treatment_response.get("æ²»ç–—ååº”è¯„ä¼°", {}).get("ååº”åˆ†çº§", "æœªçŸ¥")
    
    # åŸºäºè„†æ€§ç±»å‹çš„é¢„å
    type_prognosis = {
        "Iå‹æ··æ²Œè„†æ€§": "é¢„åéœ€è¦è°¨æ…ä¹è§‚ï¼Œéœ€è¦æœ€ç²¾å¯†çš„ç®¡ç†ç­–ç•¥",
        "IIå‹å‡†å‘¨æœŸè„†æ€§": "é¢„åè‰¯å¥½ï¼Œé€šè¿‡æ—¶é—´æ²»ç–—å­¦å¯ä»¥æ˜¾è‘—æ”¹å–„",
        "IIIå‹éšæœºè„†æ€§": "é¢„åä¸­ç­‰ï¼Œéœ€è¦ç»¼åˆå¹²é¢„æ§åˆ¶éšæœºå› ç´ ",
        "IVå‹è®°å¿†ç¼ºå¤±è„†æ€§": "é¢„åè¾ƒå¥½ï¼Œç¨³å®šæ²»ç–—æ–¹æ¡ˆæ•ˆæœæ˜¾è‘—",
        "Vå‹é¢‘åŸŸè„†æ€§": "é¢„åè‰¯å¥½ï¼Œé¢‘åŸŸåˆ†ææŒ‡å¯¼ä¸‹æ²»ç–—æ•ˆæœç†æƒ³",
        "ç¨³å®šå‹": "é¢„åä¼˜ç§€ï¼Œç»´æŒç°çŠ¶å³å¯è¾¾åˆ°é•¿æœŸç¨³å®š"
    }
    
    base_prognosis = type_prognosis.get(brittleness_type, "éœ€è¦è¿›ä¸€æ­¥è¯„ä¼°")
    
    # æ ¹æ®ä¸¥é‡ç¨‹åº¦è°ƒæ•´
    severity_modifier = ""
    if severity_score > 80:
        severity_modifier = "ï¼Œä½†éœ€è¦å¯†é›†ç›‘æµ‹å’Œé¢‘ç¹è°ƒæ•´"
    elif severity_score > 60:
        severity_modifier = "ï¼Œéœ€è¦åŠ å¼ºç®¡ç†å’Œå®šæœŸè¯„ä¼°"
    elif severity_score < 30:
        severity_modifier = "ï¼Œç®¡ç†ç›¸å¯¹ç®€å•ï¼Œé‡ç‚¹å…³æ³¨é¢„é˜²"
    
    # æ ¹æ®æ²»ç–—ååº”è°ƒæ•´
    response_modifier = ""
    if response_grade == "ä¼˜ç§€":
        response_modifier = "ã€‚å½“å‰æ²»ç–—ååº”ä¼˜ç§€ï¼Œæœ‰æœ›å®ç°é•¿æœŸç¨³å®šæ§åˆ¶"
    elif response_grade == "è‰¯å¥½":
        response_modifier = "ã€‚æ²»ç–—ååº”è‰¯å¥½ï¼Œç»§ç»­ä¼˜åŒ–æœ‰æœ›è¾¾åˆ°ç†æƒ³çŠ¶æ€"
    elif response_grade == "éœ€è¦è°ƒæ•´":
        response_modifier = "ã€‚å½“å‰æ²»ç–—ååº”ä¸ç†æƒ³ï¼Œéœ€è¦é‡æ–°åˆ¶å®šç­–ç•¥"
    
    return base_prognosis + severity_modifier + response_modifier

def identify_key_risk_factors(brittleness_type: str, chaos_indicators: dict, severity_score: float) -> List[str]:
    """è¯†åˆ«å…³é”®é£é™©å› ç´ """
    
    risk_factors = []
    
    cv = chaos_indicators.get('cv_percent', 0)
    lyapunov = chaos_indicators.get('lyapunov_exponent', 0)
    hurst = chaos_indicators.get('hurst_exponent', 0.5)
    entropy = chaos_indicators.get('approximate_entropy', 0)
    
    # åŸºäºè„†æ€§ç±»å‹çš„é£é™©
    if brittleness_type == "Iå‹æ··æ²Œè„†æ€§":
        risk_factors.extend(["ç³»ç»Ÿæ··æ²Œå¯¼è‡´çš„ä¸å¯é¢„æµ‹æ€§", "æ²»ç–—è°ƒæ•´å›°éš¾", "æ€¥æ€§å¹¶å‘ç—‡é£é™©é«˜"])
    elif brittleness_type == "IVå‹è®°å¿†ç¼ºå¤±è„†æ€§":
        risk_factors.extend(["è‚ç³–åŸåŠŸèƒ½å¼‚å¸¸", "æ²»ç–—ä¾ä»æ€§è¦æ±‚é«˜", "è®°å¿†æ”¯æŒç³»ç»Ÿä¾èµ–"])
    
    # åŸºäºæŒ‡æ ‡çš„é£é™©
    if cv > 50:
        risk_factors.append("æé«˜è¡€ç³–å˜å¼‚æ€§å¢åŠ å„ç§å¹¶å‘ç—‡é£é™©")
    
    if lyapunov > 0.02:
        risk_factors.append("å¼ºæ··æ²Œç‰¹å¾å¯¼è‡´æ²»ç–—æ•ˆæœéš¾ä»¥é¢„æµ‹")
    
    if hurst < 0.3:
        risk_factors.append("ä¸¥é‡è®°å¿†ç¼ºå¤±å½±å“é•¿æœŸè¡€ç³–æ§åˆ¶ç¨³å®šæ€§")
    
    if entropy > 0.9:
        risk_factors.append("æé«˜å¤æ‚æ€§ä½¿ä¸ªä½“åŒ–æ²»ç–—ç­–ç•¥åˆ¶å®šå›°éš¾")
    
    if severity_score > 85:
        risk_factors.append("é«˜è„†æ€§è¯„åˆ†é¢„ç¤ºç»¼åˆç®¡ç†æŒ‘æˆ˜æ€§å¤§")
    
    return risk_factors[:5]  # æœ€å¤šè¿”å›5ä¸ªå…³é”®é£é™©

def generate_clinical_decision_advice(brittleness_type: str, severity_score: float, response_assessment: dict) -> List[str]:
    """ç”Ÿæˆä¸´åºŠå†³ç­–å»ºè®®"""
    
    advice = []
    
    # åŸºäºè„†æ€§ç±»å‹çš„å»ºè®®
    type_advice = {
        "Iå‹æ··æ²Œè„†æ€§": [
            "å»ºè®®å¤šå­¦ç§‘å›¢é˜Ÿåä½œç®¡ç†",
            "è€ƒè™‘äººå·¥èƒ°è…ºæˆ–é—­ç¯ç³»ç»Ÿ",
            "åˆ¶å®šåº”æ€¥é¢„æ¡ˆåº”å¯¹çªå‘æƒ…å†µ"
        ],
        "IVå‹è®°å¿†ç¼ºå¤±è„†æ€§": [
            "é‡‡ç”¨é•¿æ•ˆç¨³å®šçš„æ²»ç–—æ–¹æ¡ˆ",
            "å»ºç«‹å¤–éƒ¨è®°å¿†æ”¯æŒç³»ç»Ÿ",
            "ç®€åŒ–æ²»ç–—ç¨‹åºæé«˜ä¾ä»æ€§"
        ],
        "ç¨³å®šå‹": [
            "ç»´æŒç°æœ‰æœ‰æ•ˆæ²»ç–—æ–¹æ¡ˆ",
            "é‡ç‚¹è½¬å‘é•¿æœŸå¹¶å‘ç—‡é¢„é˜²",
            "é€‚åº¦æ”¾å®½ç›‘æµ‹é¢‘ç‡"
        ]
    }
    
    advice.extend(type_advice.get(brittleness_type, ["åˆ¶å®šä¸ªä½“åŒ–æ²»ç–—ç­–ç•¥"]))
    
    # åŸºäºä¸¥é‡ç¨‹åº¦çš„å»ºè®®
    if severity_score > 75:
        advice.append("å»ºè®®ä½é™¢è°ƒæ•´æ²»ç–—æ–¹æ¡ˆä»¥å¿«é€Ÿç¨³å®šç—…æƒ…")
    elif severity_score > 50:
        advice.append("å¢åŠ é—¨è¯Šéšè®¿é¢‘ç‡ï¼Œå¯†åˆ‡ç›‘æµ‹æ²»ç–—ååº”")
    else:
        advice.append("å¯æŒ‰å¸¸è§„éšè®¿è®¡åˆ’è¿›è¡Œç®¡ç†")
    
    # åŸºäºæ²»ç–—ååº”çš„å»ºè®®
    response_grade = response_assessment.get("ååº”åˆ†çº§", "")
    if response_grade == "ä¼˜ç§€":
        advice.append("å½“å‰ç­–ç•¥é«˜åº¦æœ‰æ•ˆï¼Œå»ºè®®ç»´æŒå¹¶ç²¾ç»†åŒ–è°ƒæ•´")
    elif response_grade == "éœ€è¦è°ƒæ•´":
        advice.append("ç°æœ‰æ–¹æ¡ˆæ•ˆæœä¸ä½³ï¼Œå»ºè®®å…¨é¢é‡æ–°è¯„ä¼°")
    
    return advice

def generate_intelligent_recommendations(brittleness_type: str, severity_score: float, 
                                       chaos_indicators: dict, temporal_analysis: dict, 
                                       treatment_response: dict) -> dict:
    """ç”Ÿæˆæ™ºèƒ½æ²»ç–—å»ºè®®"""
    
    response_assessment = treatment_response.get("æ²»ç–—ååº”è¯„ä¼°", {})
    response_grade = response_assessment.get("ååº”åˆ†çº§", "éœ€è¦è°ƒæ•´")
    
    recommendations = {
        "immediate_actions": generate_immediate_actions_intelligent(brittleness_type, severity_score, response_grade),
        "treatment_strategies": generate_treatment_strategies_intelligent(brittleness_type, chaos_indicators),
        "monitoring_protocols": generate_monitoring_protocols_intelligent(brittleness_type, severity_score),
        "lifestyle_modifications": generate_lifestyle_modifications_intelligent(brittleness_type, temporal_analysis),
        "technology_recommendations": generate_technology_recommendations(brittleness_type, severity_score),
        "follow_up_plan": generate_follow_up_plan_intelligent(response_grade, severity_score),
        "emergency_preparedness": generate_emergency_preparedness(brittleness_type, severity_score)
    }
    
    return recommendations

def generate_immediate_actions_intelligent(brittleness_type: str, severity_score: float, response_grade: str) -> List[str]:
    """ç”Ÿæˆå³æ—¶è¡ŒåŠ¨å»ºè®®"""
    
    actions = []
    
    if severity_score > 80:
        actions.append("å»ºè®®ç«‹å³ä¸“ç§‘æ€¥è¯Šè¯„ä¼°ï¼Œè€ƒè™‘ä½é™¢è°ƒæ•´")
    elif severity_score > 60:
        actions.append("1-3å¤©å†…ç´§æ€¥ä¸“ç§‘é—¨è¯Šå¤è¯Š")
    else:
        actions.append("1-2å‘¨å†…ä¸“ç§‘é—¨è¯Šéšè®¿")
    
    if response_grade == "ä¼˜ç§€":
        actions.extend([
            "ç»´æŒå½“å‰æœ‰æ•ˆæ²»ç–—æ–¹æ¡ˆ",
            "å¯è€ƒè™‘é€‚åº¦ä¼˜åŒ–å‰‚é‡è¿½æ±‚æ›´ç²¾å‡†æ§åˆ¶"
        ])
    elif response_grade == "éœ€è¦è°ƒæ•´":
        actions.extend([
            "æš‚åœå½“å‰æ— æ•ˆæ²»ç–—è°ƒæ•´",
            "å…¨é¢é‡æ–°è¯„ä¼°æ²»ç–—æ–¹æ¡ˆ",
            "åŠ å¼ºè¡€ç³–ç›‘æµ‹é¢‘ç‡"
        ])
    
    if brittleness_type == "Iå‹æ··æ²Œè„†æ€§":
        actions.append("å¯åŠ¨å¯†é›†ç›‘æµ‹æ¨¡å¼ï¼Œæ¯æ—¥è‡³å°‘8æ¬¡è¡€ç³–ç›‘æµ‹")
    elif brittleness_type == "IVå‹è®°å¿†ç¼ºå¤±è„†æ€§":
        actions.append("å»ºç«‹ç”¨è¯æé†’ç³»ç»Ÿï¼Œç¡®ä¿æ²»ç–—ä¾ä»æ€§")
    
    return actions

def generate_treatment_strategies_intelligent(brittleness_type: str, chaos_indicators: dict) -> List[str]:
    """ç”Ÿæˆæ™ºèƒ½æ²»ç–—ç­–ç•¥"""
    
    strategies = []
    cv = chaos_indicators.get('cv_percent', 0)
    
    strategy_map = {
        "Iå‹æ··æ²Œè„†æ€§": [
            "äººå·¥èƒ°è…ºç³»ç»Ÿæˆ–é—­ç¯èƒ°å²›ç´ æ³µ",
            "è¶…çŸ­æ•ˆèƒ°å²›ç´ ç±»ä¼¼ç‰©ä¼˜åŒ–",
            "å¤šå­¦ç§‘å›¢é˜Ÿåä½œç®¡ç†æ¨¡å¼",
            "å®æ—¶è¡€ç³–ç›‘æµ‹ç³»ç»Ÿå¿…éœ€"
        ],
        "IIå‹å‡†å‘¨æœŸè„†æ€§": [
            "æ—¶é—´æ²»ç–—å­¦æŒ‡å¯¼çš„ç»™è¯æ–¹æ¡ˆ",
            "è°ƒèŠ‚ç”Ÿç‰©é’Ÿè¯ç‰©å¦‚è¤ªé»‘ç´ ",
            "è§„å¾‹åŒ–ç”Ÿæ´»ä½œæ¯å¹²é¢„",
            "æŒ‰æ—¶é—´èŠ‚å¾‹è°ƒæ•´èƒ°å²›ç´ å‰‚é‡"
        ],
        "IIIå‹éšæœºè„†æ€§": [
            "å¤šå› ç´ ç¨³å®šåŒ–æ²»ç–—ç­–ç•¥",
            "ç¯å¢ƒå› ç´ æ§åˆ¶å’Œå¿ƒç†å¹²é¢„",
            "é•¿æ•ˆè¯ç‰©å‡å°‘æ³¢åŠ¨",
            "åº”æ¿€ç®¡ç†å’Œæƒ…ç»ªç¨³å®šåŒ–"
        ],
        "IVå‹è®°å¿†ç¼ºå¤±è„†æ€§": [
            "é•¿æ•ˆåŸºç¡€èƒ°å²›ç´ ä¸ºä¸»çš„æ–¹æ¡ˆ",
            "ç®€åŒ–æ²»ç–—æ–¹æ¡ˆæé«˜ä¾ä»æ€§",
            "è‚åŠŸèƒ½æ”¯æŒå’Œè¥å…»å¹²é¢„",
            "å¤–éƒ¨è®°å¿†è¾…åŠ©ç³»ç»Ÿå»ºç«‹"
        ],
        "Vå‹é¢‘åŸŸè„†æ€§": [
            "é¢‘åŸŸåˆ†ææŒ‡å¯¼çš„ç²¾å‡†æ²»ç–—",
            "ç‰¹å®šæ—¶é—´çª—å£å¼ºåŒ–å¹²é¢„",
            "èƒ°å²›ç´ æ³µç¨‹åºåŒ–ç»™è¯",
            "ç”Ÿç†æ€§èƒ°å²›ç´ åˆ†æ³Œæ¨¡æ‹Ÿ"
        ],
        "ç¨³å®šå‹": [
            "ç»´æŒç°æœ‰ç¨³å®šæ–¹æ¡ˆ",
            "é‡ç‚¹å…³æ³¨é•¿æœŸå¹¶å‘ç—‡é¢„é˜²",
            "ç”Ÿæ´»è´¨é‡å¯¼å‘çš„ç®¡ç†",
            "é¢„é˜²æ€§ç›‘æµ‹å’Œå¹²é¢„"
        ]
    }
    
    strategies.extend(strategy_map.get(brittleness_type, ["ä¸ªä½“åŒ–ç»¼åˆæ²»ç–—æ–¹æ¡ˆ"]))
    
    # æ ¹æ®å˜å¼‚ç³»æ•°æ·»åŠ ç‰¹æ®Šç­–ç•¥
    if cv > 60:
        strategies.append("è€ƒè™‘èƒ°å²›ç§»æ¤æˆ–å¹²ç»†èƒæ²»ç–—ç­‰å‰æ²¿ç–—æ³•")
    elif cv > 40:
        strategies.append("å¼ºåŒ–èƒ°å²›ç´ æ²»ç–—è”åˆå®æ—¶ç›‘æµ‹")
    
    return strategies

def generate_monitoring_protocols_intelligent(brittleness_type: str, severity_score: float) -> List[str]:
    """ç”Ÿæˆæ™ºèƒ½ç›‘æµ‹æ–¹æ¡ˆ"""
    
    protocols = []
    
    # åŸºäºè„†æ€§ç±»å‹çš„ç›‘æµ‹
    if brittleness_type in ["Iå‹æ··æ²Œè„†æ€§", "IIIå‹éšæœºè„†æ€§"]:
        protocols.extend([
            "è¿ç»­è¡€ç³–ç›‘æµ‹(CGM)è‡³å°‘4-6å‘¨",
            "æ¯æ—¥è¡€ç³–è‡ªç›‘æµ‹8-10æ¬¡",
            "å®æ—¶è¡€ç³–æ•°æ®äº‘ç«¯ä¼ è¾“å’Œåˆ†æ"
        ])
    elif brittleness_type in ["IVå‹è®°å¿†ç¼ºå¤±è„†æ€§", "Vå‹é¢‘åŸŸè„†æ€§"]:
        protocols.extend([
            "CGMç›‘æµ‹3-4å‘¨è¯„ä¼°æ¨¡å¼ç‰¹å¾",
            "æ¯æ—¥è¡€ç³–è‡ªç›‘æµ‹6-8æ¬¡",
            "é‡ç‚¹ç›‘æµ‹ç‰¹å®šæ—¶é—´çª—å£è¡€ç³–å˜åŒ–"
        ])
    else:
        protocols.extend([
            "CGMç›‘æµ‹2-3å‘¨ç¡®è®¤ç¨³å®šæ€§",
            "æ¯æ—¥è¡€ç³–è‡ªç›‘æµ‹4-6æ¬¡",
            "å®šæœŸè¯„ä¼°è¡€ç³–æ§åˆ¶è´¨é‡"
        ])
    
    # åŸºäºä¸¥é‡ç¨‹åº¦è°ƒæ•´
    if severity_score > 75:
        protocols.append("å‰2å‘¨æ¯æ—¥ç”µè¯æˆ–è¿œç¨‹éšè®¿")
        protocols.append("è¡€ç³–å¼‚å¸¸è‡ªåŠ¨æŠ¥è­¦ç³»ç»Ÿ")
    elif severity_score > 50:
        protocols.append("æ¯å‘¨è‡³å°‘2æ¬¡æ•°æ®ä¼ è¾“å’Œåˆ†æ")
    
    # ç‰¹æ®Šç›‘æµ‹é¡¹ç›®
    protocols.extend([
        "æ¯æœˆç³–åŒ–è¡€çº¢è›‹ç™½è¶‹åŠ¿ç›‘æµ‹",
        "æ¯3ä¸ªæœˆèƒ°å²›åŠŸèƒ½è¯„ä¼°",
        "æ¯6ä¸ªæœˆè„†æ€§é‡æ–°åˆ†å‹è¯„ä¼°"
    ])
    
    return protocols

def generate_lifestyle_modifications_intelligent(brittleness_type: str, temporal_analysis: dict) -> List[str]:
    """ç”Ÿæˆæ™ºèƒ½ç”Ÿæ´»æ–¹å¼æŒ‡å¯¼"""
    
    modifications = [
        "æˆ’çƒŸé™é…’ï¼Œé¿å…è¡€ç³–é¢å¤–æ‰°åŠ¨å› ç´ ",
        "å­¦ä¹ è¡€ç³–è‡ªæˆ‘ç®¡ç†æŠ€èƒ½",
        "å»ºç«‹åº”æ€¥å¤„ç½®é¢„æ¡ˆ"
    ]
    
    # åŸºäºè„†æ€§ç±»å‹çš„ç‰¹æ®Šå»ºè®®
    type_specific = {
        "Iå‹æ··æ²Œè„†æ€§": [
            "æåº¦è§„å¾‹åŒ–ç”Ÿæ´»ï¼Œé¿å…ä»»ä½•å¯èƒ½å¢åŠ å˜å¼‚çš„å› ç´ ",
            "å»ºç«‹é«˜åº¦ç»“æ„åŒ–çš„æ—¥å¸¸ç®¡ç†æ¨¡å¼"
        ],
        "IIå‹å‡†å‘¨æœŸè„†æ€§": [
            "ä¸¥æ ¼æŒ‰æ—¶é—´è¡¨è¿›è¡Œå„é¡¹æ´»åŠ¨",
            "ä¼˜åŒ–ç¡çœ è´¨é‡ï¼Œå›ºå®šä½œæ¯æ—¶é—´",
            "è€ƒè™‘å…‰ç…§æ²»ç–—è°ƒèŠ‚ç”Ÿç‰©é’Ÿ"
        ],
        "IVå‹è®°å¿†ç¼ºå¤±è„†æ€§": [
            "å»ºç«‹è¯¦ç»†çš„æ²»ç–—æ—¥è®°å’Œæé†’ç³»ç»Ÿ",
            "å®¶å±å‚ä¸ç®¡ç†ï¼Œæä¾›å¤–éƒ¨è®°å¿†æ”¯æŒ",
            "ç®€åŒ–é¥®é£Ÿå’Œè¿åŠ¨æ¨¡å¼"
        ],
        "ç¨³å®šå‹": [
            "ä¿æŒå¥åº·ç”Ÿæ´»æ–¹å¼ï¼Œé¢„é˜²å¹¶å‘ç—‡",
            "é€‚é‡å¢åŠ ä½“è‚²æ´»åŠ¨",
            "æ³¨é‡å¿ƒç†å¥åº·å’Œç”Ÿæ´»è´¨é‡"
        ]
    }
    
    modifications.extend(type_specific.get(brittleness_type, []))
    
    # åŸºäºæ—¶æ®µåˆ†æçš„å»ºè®®
    temporal_patterns = temporal_analysis.get("temporal_patterns", {})
    highest_risk = temporal_patterns.get("æœ€é«˜é£é™©æ—¶æ®µ", "")
    if "é»æ˜" in highest_risk:
        modifications.append("é»æ˜æ—¶æ®µç‰¹åˆ«æ³¨æ„è¡€ç³–ç›‘æµ‹å’Œç®¡ç†")
    elif "å¤œé—´" in highest_risk:
        modifications.append("å¤œé—´è¡€ç³–ç®¡ç†åŠ å¼ºï¼Œé¢„é˜²å¤œé—´ä½è¡€ç³–")
    
    return modifications

def generate_technology_recommendations(brittleness_type: str, severity_score: float) -> List[str]:
    """ç”ŸæˆæŠ€æœ¯è®¾å¤‡å»ºè®®"""
    
    tech_recommendations = []
    
    if brittleness_type == "Iå‹æ··æ²Œè„†æ€§" or severity_score > 75:
        tech_recommendations.extend([
            "äººå·¥èƒ°è…ºç³»ç»Ÿ(APS)æˆ–æ··åˆé—­ç¯ç³»ç»Ÿ",
            "å®æ—¶CGMä¸èƒ°å²›ç´ æ³µé›†æˆç³»ç»Ÿ",
            "è¡€ç³–é¢„æµ‹å’Œé¢„è­¦ç®—æ³•"
        ])
    elif severity_score > 50:
        tech_recommendations.extend([
            "èƒ°å²›ç´ æ³µæ²»ç–—ç³»ç»Ÿ",
            "è¿ç»­è¡€ç³–ç›‘æµ‹è®¾å¤‡(CGM)",
            "è¡€ç³–ç®¡ç†APPå’Œæ•°æ®åˆ†æå·¥å…·"
        ])
    else:
        tech_recommendations.extend([
            "é—´æ­‡æ€§CGMç›‘æµ‹",
            "æ™ºèƒ½è¡€ç³–ä»ªå’Œæ•°æ®ç®¡ç†",
            "ç§»åŠ¨å¥åº·ç®¡ç†åº”ç”¨"
        ])
    
    # ç‰¹æ®ŠæŠ€æœ¯éœ€æ±‚
    if brittleness_type == "Vå‹é¢‘åŸŸè„†æ€§":
        tech_recommendations.append("é¢‘åŸŸåˆ†æè½¯ä»¶å’Œä¸ªä½“åŒ–ç®—æ³•")
    elif brittleness_type == "IVå‹è®°å¿†ç¼ºå¤±è„†æ€§":
        tech_recommendations.append("æ™ºèƒ½æé†’å’Œä¾ä»æ€§ç›‘æµ‹ç³»ç»Ÿ")
    
    return tech_recommendations

def generate_follow_up_plan_intelligent(response_grade: str, severity_score: float) -> List[str]:
    """ç”Ÿæˆæ™ºèƒ½éšè®¿è®¡åˆ’"""
    
    follow_up = []
    
    # åŸºäºæ²»ç–—ååº”çš„éšè®¿é¢‘ç‡
    if response_grade == "ä¼˜ç§€":
        follow_up.extend([
            "4-6å‘¨åå¸¸è§„ä¸“ç§‘å¤è¯Š",
            "3ä¸ªæœˆåç³–åŒ–è¡€çº¢è›‹ç™½å¤æŸ¥",
            "6ä¸ªæœˆåè„†æ€§é‡æ–°è¯„ä¼°"
        ])
    elif response_grade == "è‰¯å¥½":
        follow_up.extend([
            "2-4å‘¨åä¸“ç§‘å¤è¯Šè¯„ä¼°",
            "6-8å‘¨åæ²»ç–—ååº”è¯„ä¼°",
            "3ä¸ªæœˆåå…¨é¢é‡æ–°è¯„ä¼°"
        ])
    else:
        follow_up.extend([
            "1å‘¨åç´§æ€¥å¤è¯Š",
            "æ²»ç–—è°ƒæ•´æœŸé—´æ¯å‘¨éšè®¿",
            "4å‘¨åæ–°æ–¹æ¡ˆæ•ˆæœè¯„ä¼°"
        ])
    
    # åŸºäºä¸¥é‡ç¨‹åº¦çš„ç‰¹æ®Šéšè®¿
    if severity_score > 75:
        follow_up.append("å‰2å‘¨æ¯æ—¥è¿œç¨‹ç›‘æµ‹å’ŒæŒ‡å¯¼")
    elif severity_score > 50:
        follow_up.append("å‰4å‘¨æ¯å‘¨ç”µè¯éšè®¿")
    
    # é•¿æœŸéšè®¿è®¡åˆ’
    follow_up.extend([
        "æ¯6ä¸ªæœˆè„†æ€§åˆ†å‹é‡æ–°è¯„ä¼°",
        "æ¯å¹´åº¦å¹¶å‘ç—‡å…¨é¢ç­›æŸ¥",
        "æ¯2å¹´æ™ºèƒ½åˆ†æç³»ç»Ÿå‡çº§è¯„ä¼°"
    ])
    
    return follow_up

def generate_emergency_preparedness(brittleness_type: str, severity_score: float) -> List[str]:
    """ç”Ÿæˆåº”æ€¥é¢„æ¡ˆå»ºè®®"""
    
    emergency_plans = [
        "å»ºç«‹24å°æ—¶ç´§æ€¥è”ç³»æœºåˆ¶",
        "å‡†å¤‡ä½è¡€ç³–å’Œé«˜è¡€ç³–åº”æ€¥ç”¨å“",
        "åˆ¶å®šå®¶å±åº”æ€¥å¤„ç½®åŸ¹è®­è®¡åˆ’"
    ]
    
    if brittleness_type == "Iå‹æ··æ²Œè„†æ€§" or severity_score > 75:
        emergency_plans.extend([
            "å»ºç«‹ä¸“ç§‘åŒ»ç”Ÿç›´é€šçƒ­çº¿",
            "å‡†å¤‡èƒ°é«˜è¡€ç³–ç´ åº”æ€¥æ³¨å°„",
            "åˆ¶å®šä½é™¢åº”æ€¥é¢„æ¡ˆå’Œç»¿è‰²é€šé“"
        ])
    
    if brittleness_type == "IVå‹è®°å¿†ç¼ºå¤±è„†æ€§":
        emergency_plans.append("å»ºç«‹è®¤çŸ¥åŠŸèƒ½åº”æ€¥è¯„ä¼°æµç¨‹")
    
    return emergency_plans

def analyze_intelligent_longitudinal_segments(df: pd.DataFrame, glucose_values: np.ndarray, total_days: int) -> dict:
    """æ™ºèƒ½æ—¶é—´åˆ†æ®µçºµå‘è„†æ€§åˆ†æ - åŸºäºæ•°æ®é©±åŠ¨çš„å˜åŒ–ç‚¹æ£€æµ‹"""
    
    try:
        print("[æ™ºèƒ½åˆ†æ®µ] å¼€å§‹å¤šç»´åº¦å˜åŒ–ç‚¹æ£€æµ‹...")
        
        # 1. æ•°æ®é¢„å¤„ç†
        df_processed = df.copy()
        df_processed['timestamp'] = pd.to_datetime(df_processed['timestamp'])
        df_processed = df_processed.sort_values('timestamp')
        df_processed['day_from_start'] = (df_processed['timestamp'] - df_processed['timestamp'].min()).dt.days
        df_processed['hours_from_start'] = (df_processed['timestamp'] - df_processed['timestamp'].min()).dt.total_seconds() / 3600
        
        # 2. è®¡ç®—æ»‘åŠ¨çª—å£æŒ‡æ ‡
        indicators = calculate_sliding_window_indicators(df_processed, glucose_values)
        
        # 3. ç»¼åˆå˜åŒ–ç‚¹æ£€æµ‹
        change_points = detect_comprehensive_change_points(indicators, df_processed)
        
        # 4. å˜åŒ–ç‚¹èåˆå’Œæ®µè½ç”Ÿæˆ
        final_segments = merge_and_generate_segments(change_points, df_processed, total_days)
        
        # 5. æ®µé—´å·®å¼‚è¯¦ç»†åˆ†æ
        segment_analysis = analyze_detailed_segment_differences(final_segments, df_processed, glucose_values)
        
        # 6. ç”Ÿæˆæ™ºèƒ½åˆ†æ®µæŠ¥å‘Š
        intelligent_report = {
            "åˆ†æ®µæ–¹æ³•è¯´æ˜": "åŸºäºæ•°æ®é©±åŠ¨çš„å¤šç»´åº¦æ™ºèƒ½å˜åŒ–ç‚¹æ£€æµ‹æŠ€æœ¯",
            "æ£€æµ‹ç»´åº¦": ["è¡€ç³–æ§åˆ¶è´¨é‡å˜åŒ–", "è„†æ€§ç‰¹å¾æ¼”å˜", "å˜å¼‚æ¨¡å¼è½¬æ¢", "æ²»ç–—ååº”é˜¶æ®µ"],
            "å˜åŒ–ç‚¹æ£€æµ‹è¯¦æƒ…": {
                "æ£€æµ‹æ–¹æ³•": ["ç»Ÿè®¡å­¦å˜åŒ–ç‚¹æ£€æµ‹", "èšç±»åˆ†æåˆ†æ®µ", "æ¢¯åº¦å˜åŒ–åˆ†æ", "è„†æ€§é˜¶æ®µè¯†åˆ«"],
                "è¯†åˆ«å‡ºçš„å˜åŒ–ç‚¹": change_points,
                "ä¿¡åº¦è¯„ä¼°": "é«˜ç½®ä¿¡åº¦" if len(change_points) >= 1 else "éœ€è¦æ›´å¤šæ•°æ®"
            },
            "æœ€ç»ˆæ™ºèƒ½åˆ†æ®µ": final_segments,
            "æ®µé—´è¯¦ç»†å¯¹æ¯”åˆ†æ": segment_analysis,
            "åˆ†æ®µè´¨é‡è¯„ä¼°": evaluate_intelligent_segmentation_quality(segment_analysis, final_segments),
            "ä¸´åºŠæ„ä¹‰è§£è¯»": generate_clinical_significance_interpretation(segment_analysis, change_points)
        }
        
        return intelligent_report
        
    except Exception as e:
        print(f"[æ™ºèƒ½åˆ†æ®µ] é”™è¯¯: {e}")
        return {
            "åˆ†æ®µæ–¹æ³•è¯´æ˜": "æ™ºèƒ½åˆ†æ®µåˆ†æé‡åˆ°æŠ€æœ¯é—®é¢˜",
            "error": str(e),
            "fallback_analysis": "å·²åˆ‡æ¢åˆ°åŸºç¡€åˆ†æ®µæ¨¡å¼"
        }

def calculate_sliding_window_indicators(df: pd.DataFrame, glucose_values: np.ndarray) -> dict:
    """è®¡ç®—æ»‘åŠ¨çª—å£å¤šç»´æŒ‡æ ‡"""
    
    print("[æ™ºèƒ½åˆ†æ®µ] è®¡ç®—æ»‘åŠ¨çª—å£æŒ‡æ ‡...")
    
    # æ»‘åŠ¨çª—å£å‚æ•°
    window_size = max(48, int(len(glucose_values) * 0.08))  # è‡³å°‘48ä¸ªç‚¹ï¼Œçº¦8%çš„æ•°æ®
    step_size = max(12, window_size // 4)  # æ­¥é•¿ä¸ºçª—å£çš„1/4
    
    indicators = {
        'timestamps': [],
        'window_centers': [],
        'mean_glucose': [],
        'cv': [],
        'tir': [],
        'gmi': [],
        'brittleness_score': [],
        'variability_index': [],
        'stability_score': [],
        'trend_strength': [],
        'chaos_score': []
    }
    
    hours_from_start = df['hours_from_start'].values
    
    for i in range(0, len(glucose_values) - window_size + 1, step_size):
        window_glucose = glucose_values[i:i+window_size]
        window_center_hour = hours_from_start[i + window_size // 2]
        
        if len(window_glucose) < 20:  # æ•°æ®ç‚¹å¤ªå°‘
            continue
            
        # åŸºç¡€æŒ‡æ ‡
        mean_glucose = np.mean(window_glucose)
        std_glucose = np.std(window_glucose)
        cv = (std_glucose / mean_glucose) * 100 if mean_glucose > 0 else 0
        tir = ((window_glucose >= 3.9) & (window_glucose <= 10.0)).sum() / len(window_glucose) * 100
        
        # GMIè®¡ç®—
        gmi = (3.31 + 0.02392 * mean_glucose * 18.01) if mean_glucose > 0 else 0
        
        # è„†æ€§è¯„åˆ† (ç®€åŒ–ç‰ˆ)
        brittleness_score = calculate_window_brittleness_score(window_glucose)
        
        # å˜å¼‚æ€§æŒ‡æ•°
        variability_index = np.std(np.diff(window_glucose)) if len(window_glucose) > 1 else 0
        
        # ç¨³å®šæ€§è¯„åˆ†
        stability_score = 100 - min(100, cv * 1.5)  # CVè¶Šä½ç¨³å®šæ€§è¶Šé«˜
        
        # è¶‹åŠ¿å¼ºåº¦ (ä½¿ç”¨å·²æœ‰å‡½æ•°)
        trend_strength = calculate_trend_strength(window_glucose) if len(window_glucose) > 3 else 0
        
        # æ··æ²Œè¯„åˆ† (ç®€åŒ–)
        chaos_score = calculate_simple_chaos_score(window_glucose)
        
        # å­˜å‚¨æŒ‡æ ‡
        indicators['timestamps'].append(df.iloc[i + window_size // 2]['timestamp'])
        indicators['window_centers'].append(window_center_hour)
        indicators['mean_glucose'].append(mean_glucose)
        indicators['cv'].append(cv)
        indicators['tir'].append(tir)
        indicators['gmi'].append(gmi)
        indicators['brittleness_score'].append(brittleness_score)
        indicators['variability_index'].append(variability_index)
        indicators['stability_score'].append(stability_score)
        indicators['trend_strength'].append(trend_strength)
        indicators['chaos_score'].append(chaos_score)
    
    return indicators

def calculate_window_brittleness_score(glucose_window: np.ndarray) -> float:
    """è®¡ç®—çª—å£è„†æ€§è¯„åˆ†"""
    
    if len(glucose_window) < 10:
        return 0.0
    
    mean_glucose = np.mean(glucose_window)
    cv = (np.std(glucose_window) / mean_glucose) * 100 if mean_glucose > 0 else 0
    tir = ((glucose_window >= 3.9) & (glucose_window <= 10.0)).sum() / len(glucose_window) * 100
    
    # åŸºç¡€è„†æ€§è¯„åˆ†
    brittleness = 0
    
    # CVè´¡çŒ®
    if cv > 50:
        brittleness += 40
    elif cv > 35:
        brittleness += 30
    elif cv > 25:
        brittleness += 20
    elif cv > 15:
        brittleness += 10
    
    # TIRè´¡çŒ® (åå‘)
    if tir < 50:
        brittleness += 20
    elif tir < 70:
        brittleness += 10
    
    # æå€¼è´¡çŒ®
    if np.max(glucose_window) > 20:
        brittleness += 15
    if np.min(glucose_window) < 3.0:
        brittleness += 15
    
    # å˜å¼‚æ€§è´¡çŒ®
    if len(glucose_window) > 1:
        diff_std = np.std(np.diff(glucose_window))
        if diff_std > 3:
            brittleness += 10
    
    return min(100, brittleness)

def calculate_simple_chaos_score(glucose_window: np.ndarray) -> float:
    """è®¡ç®—ç®€åŒ–æ··æ²Œè¯„åˆ†"""
    
    if len(glucose_window) < 10:
        return 0.0
    
    chaos_score = 0
    
    # åŸºäºå˜å¼‚ç³»æ•°
    cv = (np.std(glucose_window) / np.mean(glucose_window)) * 100 if np.mean(glucose_window) > 0 else 0
    if cv > 40:
        chaos_score += 3
    elif cv > 30:
        chaos_score += 2
    elif cv > 20:
        chaos_score += 1
    
    # åŸºäºç›¸é‚»å·®å¼‚
    if len(glucose_window) > 1:
        diffs = np.abs(np.diff(glucose_window))
        large_jumps = np.sum(diffs > 3) / len(diffs)
        if large_jumps > 0.3:
            chaos_score += 2
        elif large_jumps > 0.2:
            chaos_score += 1
    
    # åŸºäºåˆ†å¸ƒä¸è§„åˆ™æ€§
    try:
        hist, _ = np.histogram(glucose_window, bins=min(10, len(glucose_window)//3))
        entropy = -np.sum(hist[hist>0] / np.sum(hist) * np.log(hist[hist>0] / np.sum(hist)))
        if entropy > 2:
            chaos_score += 1
    except:
        pass
    
    return min(10, chaos_score)  # é™åˆ¶åœ¨0-10èŒƒå›´

def detect_comprehensive_change_points(indicators: dict, df: pd.DataFrame) -> dict:
    """ç»¼åˆå˜åŒ–ç‚¹æ£€æµ‹"""
    
    print("[æ™ºèƒ½åˆ†æ®µ] æ‰§è¡Œå¤šç®—æ³•å˜åŒ–ç‚¹æ£€æµ‹...")
    
    change_points = {
        "ç»Ÿè®¡å˜åŒ–ç‚¹": [],
        "èšç±»å˜åŒ–ç‚¹": [],
        "æ¢¯åº¦å˜åŒ–ç‚¹": [],
        "è„†æ€§å˜åŒ–ç‚¹": [],
        "ç»¼åˆå˜åŒ–ç‚¹": []
    }
    
    if len(indicators['mean_glucose']) < 6:
        return change_points
    
    window_centers = np.array(indicators['window_centers'])
    
    # 1. ç»Ÿè®¡å­¦å˜åŒ–ç‚¹æ£€æµ‹ (åŸºäºTIRå’Œè¡€ç³–å‡å€¼)
    tir_changes = detect_statistical_change_points(indicators['tir'], window_centers)
    glucose_changes = detect_statistical_change_points(indicators['mean_glucose'], window_centers)
    change_points["ç»Ÿè®¡å˜åŒ–ç‚¹"] = list(set(tir_changes + glucose_changes))
    
    # 2. èšç±»åˆ†æå˜åŒ–ç‚¹
    clustering_changes = detect_clustering_change_points(indicators, window_centers)
    change_points["èšç±»å˜åŒ–ç‚¹"] = clustering_changes
    
    # 3. æ¢¯åº¦å˜åŒ–æ£€æµ‹
    gradient_changes = detect_gradient_change_points(indicators, window_centers)
    change_points["æ¢¯åº¦å˜åŒ–ç‚¹"] = gradient_changes
    
    # 4. è„†æ€§é˜¶æ®µå˜åŒ–æ£€æµ‹
    brittleness_changes = detect_brittleness_phase_changes(indicators['brittleness_score'], window_centers)
    change_points["è„†æ€§å˜åŒ–ç‚¹"] = brittleness_changes
    
    # 5. ç»¼åˆå˜åŒ–ç‚¹èåˆ
    all_changes = []
    for method_changes in change_points.values():
        all_changes.extend(method_changes)
    
    # å»é‡å¹¶æ’åº
    if all_changes:
        all_changes = sorted(list(set(all_changes)))
        # åˆå¹¶ç›¸è¿‘çš„å˜åŒ–ç‚¹ (24å°æ—¶å†…ï¼Œæ›´ä¸¥æ ¼çš„åˆå¹¶)
        merged_changes = merge_nearby_change_points(all_changes, merge_threshold_hours=24.0)
        change_points["ç»¼åˆå˜åŒ–ç‚¹"] = merged_changes
    
    return change_points

def detect_statistical_change_points(values: list, time_points: np.ndarray, significance=0.01) -> list:
    """ç»Ÿè®¡å­¦å˜åŒ–ç‚¹æ£€æµ‹"""
    
    if len(values) < 6:
        return []
    
    change_points = []
    values_array = np.array(values)
    
    # æ»‘åŠ¨çª—å£Tæ£€éªŒ
    for i in range(2, len(values) - 2):
        before_window = values_array[max(0, i-2):i]
        after_window = values_array[i:min(len(values), i+3)]
        
        if len(before_window) >= 2 and len(after_window) >= 2:
            # æ‰§è¡ŒTæ£€éªŒ
            t_stat, p_value = stats.ttest_ind(before_window, after_window)
            
            if p_value < significance and abs(t_stat) > 3.0:
                change_points.append(time_points[i])
    
    return change_points

def detect_clustering_change_points(indicators: dict, time_points: np.ndarray) -> list:
    """åŸºäºèšç±»çš„å˜åŒ–ç‚¹æ£€æµ‹"""
    
    if len(indicators['mean_glucose']) < 6:
        return []
    
    # å‡†å¤‡å¤šç»´ç‰¹å¾
    features = []
    for i in range(len(indicators['mean_glucose'])):
        feature_vector = [
            indicators['mean_glucose'][i],
            indicators['cv'][i],
            indicators['tir'][i],
            indicators['brittleness_score'][i]
        ]
        features.append(feature_vector)
    
    features_array = np.array(features)
    
    # æ ‡å‡†åŒ–ç‰¹å¾
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features_array)
    
    change_points = []
    
    # å°è¯•2-4ä¸ªèšç±»
    for n_clusters in range(2, min(5, len(features_scaled))):
        try:
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            labels = kmeans.fit_predict(features_scaled)
            
            # å¯»æ‰¾èšç±»æ ‡ç­¾å˜åŒ–çš„ç‚¹
            for i in range(1, len(labels)):
                if labels[i] != labels[i-1]:
                    change_points.append(time_points[i])
        except:
            continue
    
    return sorted(list(set(change_points)))

def detect_gradient_change_points(indicators: dict, time_points: np.ndarray, threshold=1.5) -> list:
    """åŸºäºæ¢¯åº¦å˜åŒ–çš„å˜åŒ–ç‚¹æ£€æµ‹"""
    
    change_points = []
    
    # å¯¹ä¸»è¦æŒ‡æ ‡è®¡ç®—æ¢¯åº¦
    for key in ['mean_glucose', 'tir', 'brittleness_score']:
        if key in indicators and len(indicators[key]) > 3:
            values = np.array(indicators[key])
            
            # è®¡ç®—ä¸€é˜¶å¯¼æ•° (æ¢¯åº¦)
            gradients = np.gradient(values)
            
            # è®¡ç®—äºŒé˜¶å¯¼æ•° (æ¢¯åº¦å˜åŒ–ç‡)
            gradient_changes = np.gradient(gradients)
            
            # å¯»æ‰¾æ¢¯åº¦æ€¥å‰§å˜åŒ–çš„ç‚¹
            threshold_value = threshold * np.std(gradient_changes) if np.std(gradient_changes) > 0 else 0
            
            for i in range(1, len(gradient_changes) - 1):
                if abs(gradient_changes[i]) > threshold_value:
                    change_points.append(time_points[i])
    
    return sorted(list(set(change_points)))

def detect_brittleness_phase_changes(brittleness_scores: list, time_points: np.ndarray) -> list:
    """è„†æ€§é˜¶æ®µå˜åŒ–æ£€æµ‹"""
    
    if len(brittleness_scores) < 4:
        return []
    
    change_points = []
    scores = np.array(brittleness_scores)
    
    # å®šä¹‰è„†æ€§é˜ˆå€¼
    low_brittleness = 25
    medium_brittleness = 50
    high_brittleness = 75
    
    # æ£€æµ‹è„†æ€§ç­‰çº§å˜åŒ–
    previous_level = classify_brittleness_level(scores[0], low_brittleness, medium_brittleness, high_brittleness)
    
    for i in range(1, len(scores)):
        current_level = classify_brittleness_level(scores[i], low_brittleness, medium_brittleness, high_brittleness)
        
        if current_level != previous_level:
            change_points.append(time_points[i])
            previous_level = current_level
    
    return change_points

def classify_brittleness_level(score: float, low_thresh: float, med_thresh: float, high_thresh: float) -> str:
    """åˆ†ç±»è„†æ€§ç­‰çº§"""
    if score >= high_thresh:
        return "é«˜è„†æ€§"
    elif score >= med_thresh:
        return "ä¸­è„†æ€§"
    elif score >= low_thresh:
        return "ä½è„†æ€§"
    else:
        return "ç¨³å®š"

def merge_nearby_change_points(change_points: list, merge_threshold_hours: float = 6.0) -> list:
    """åˆå¹¶ç›¸è¿‘çš„å˜åŒ–ç‚¹"""
    
    if len(change_points) <= 1:
        return change_points
    
    merged = [change_points[0]]
    
    for current_point in change_points[1:]:
        last_merged = merged[-1]
        
        # å¦‚æœå½“å‰å˜åŒ–ç‚¹è·ç¦»ä¸Šä¸€ä¸ªåˆå¹¶ç‚¹å¤ªè¿‘ï¼Œåˆ™è·³è¿‡
        if abs(current_point - last_merged) > merge_threshold_hours:
            merged.append(current_point)
    
    return merged

def merge_and_generate_segments(change_points: dict, df: pd.DataFrame, total_days: int) -> dict:
    """å˜åŒ–ç‚¹èåˆå’Œæ®µè½ç”Ÿæˆ"""
    
    print("[æ™ºèƒ½åˆ†æ®µ] ç”Ÿæˆæœ€ç»ˆåˆ†æ®µ...")
    
    comprehensive_changes = change_points.get("ç»¼åˆå˜åŒ–ç‚¹", [])
    
    # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°å˜åŒ–ç‚¹ï¼ŒæŒ‰æ—¶é—´ç­‰åˆ†
    if not comprehensive_changes:
        print("[æ™ºèƒ½åˆ†æ®µ] æœªæ£€æµ‹åˆ°æ˜æ˜¾å˜åŒ–ç‚¹ï¼Œä½¿ç”¨æ—¶é—´ç­‰åˆ†æ³•")
        total_hours = df['hours_from_start'].max()
        segment_boundaries = [0, total_hours / 2, total_hours]
    else:
        # ä½¿ç”¨æ£€æµ‹åˆ°çš„å˜åŒ–ç‚¹ä½œä¸ºè¾¹ç•Œ
        segment_boundaries = [0] + comprehensive_changes + [df['hours_from_start'].max()]
        segment_boundaries = sorted(list(set(segment_boundaries)))
    
    # ç”Ÿæˆæ®µè½ä¿¡æ¯
    segments = {
        "åˆ†æ®µæ•°é‡": len(segment_boundaries) - 1,
        "åˆ†æ®µè¾¹ç•Œ": segment_boundaries,
        "è¯¦ç»†åˆ†æ®µ": []
    }
    
    for i in range(len(segment_boundaries) - 1):
        start_hour = segment_boundaries[i]
        end_hour = segment_boundaries[i + 1]
        
        start_day = start_hour / 24
        end_day = end_hour / 24
        duration_days = (end_hour - start_hour) / 24
        
        segment_info = {
            "æ®µè½ç¼–å·": i + 1,
            "å¼€å§‹æ—¶é—´": f"ç¬¬{start_day:.1f}å¤©",
            "ç»“æŸæ—¶é—´": f"ç¬¬{end_day:.1f}å¤©",
            "æŒç»­æ—¶é—´": f"{duration_days:.1f}å¤©",
            "èµ·å§‹å°æ—¶": f"{start_hour:.1f}å°æ—¶",
            "ç»“æŸå°æ—¶": f"{end_hour:.1f}å°æ—¶"
        }
        
        segments["è¯¦ç»†åˆ†æ®µ"].append(segment_info)
    
    return segments

def analyze_detailed_segment_differences(segments: dict, df: pd.DataFrame, glucose_values: np.ndarray) -> dict:
    """æ®µé—´å·®å¼‚è¯¦ç»†åˆ†æ"""
    
    print("[æ™ºèƒ½åˆ†æ®µ] åˆ†ææ®µé—´å·®å¼‚...")
    
    segment_analysis = {
        "å¯¹æ¯”æ–¹æ³•": "åŸºäºå¤šç»´æŒ‡æ ‡çš„æ®µé—´å·®å¼‚é‡åŒ–åˆ†æ",
        "åˆ†æç»´åº¦": ["è¡€ç³–æ§åˆ¶è´¨é‡", "å˜å¼‚æ€§ç‰¹å¾", "è„†æ€§ç¨‹åº¦", "æ²»ç–—æ•ˆæœ"],
        "å„æ®µè¯¦ç»†æŒ‡æ ‡": [],
        "æ®µé—´å¯¹æ¯”": [],
        "æ•´ä½“è¶‹åŠ¿è¯„ä¼°": {}
    }
    
    if "è¯¦ç»†åˆ†æ®µ" not in segments or len(segments["è¯¦ç»†åˆ†æ®µ"]) < 2:
        return {"error": "åˆ†æ®µæ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå¯¹æ¯”åˆ†æ"}
    
    # åˆ†æå„æ®µæŒ‡æ ‡
    for i, segment in enumerate(segments["è¯¦ç»†åˆ†æ®µ"]):
        start_hour = float(segment["èµ·å§‹å°æ—¶"].replace("å°æ—¶", ""))
        end_hour = float(segment["ç»“æŸå°æ—¶"].replace("å°æ—¶", ""))
        
        # ç­›é€‰è¯¥æ®µæ•°æ®
        segment_mask = (df['hours_from_start'] >= start_hour) & (df['hours_from_start'] < end_hour)
        segment_data = df[segment_mask]['glucose_value'].values
        
        if len(segment_data) < 10:  # æ•°æ®ç‚¹å¤ªå°‘
            continue
        
        # è®¡ç®—è¯¥æ®µçš„è¯¦ç»†æŒ‡æ ‡
        segment_metrics = calculate_comprehensive_segment_metrics(segment_data, i + 1)
        segment_analysis["å„æ®µè¯¦ç»†æŒ‡æ ‡"].append(segment_metrics)
    
    # æ®µé—´å¯¹æ¯”åˆ†æ
    if len(segment_analysis["å„æ®µè¯¦ç»†æŒ‡æ ‡"]) >= 2:
        segment_analysis["æ®µé—´å¯¹æ¯”"] = generate_segment_comparisons(segment_analysis["å„æ®µè¯¦ç»†æŒ‡æ ‡"])
        segment_analysis["æ•´ä½“è¶‹åŠ¿è¯„ä¼°"] = assess_overall_trends(segment_analysis["å„æ®µè¯¦ç»†æŒ‡æ ‡"])
    
    return segment_analysis

def calculate_comprehensive_segment_metrics(segment_data: np.ndarray, segment_number: int) -> dict:
    """è®¡ç®—æ®µè½ç»¼åˆæŒ‡æ ‡"""
    
    metrics = {
        "æ®µè½ç¼–å·": segment_number,
        "æ•°æ®ç‚¹æ•°": len(segment_data),
        "åŸºç¡€æŒ‡æ ‡": {},
        "è„†æ€§æŒ‡æ ‡": {},
        "é«˜çº§æŒ‡æ ‡": {},
        "ä¸´åºŠè§£è¯»": {}
    }
    
    # åŸºç¡€æŒ‡æ ‡
    mean_glucose = np.mean(segment_data)
    std_glucose = np.std(segment_data)
    cv = (std_glucose / mean_glucose) * 100 if mean_glucose > 0 else 0
    tir = ((segment_data >= 3.9) & (segment_data <= 10.0)).sum() / len(segment_data) * 100
    tbr = (segment_data < 3.9).sum() / len(segment_data) * 100
    tar = (segment_data > 10.0).sum() / len(segment_data) * 100
    gmi = (3.31 + 0.02392 * mean_glucose * 18.01) if mean_glucose > 0 else 0
    
    metrics["åŸºç¡€æŒ‡æ ‡"] = {
        "å¹³å‡è¡€ç³–": f"{mean_glucose:.2f} mmol/L",
        "è¡€ç³–æ ‡å‡†å·®": f"{std_glucose:.2f} mmol/L",
        "å˜å¼‚ç³»æ•°": f"{cv:.1f}%",
        "ç›®æ ‡èŒƒå›´æ—¶é—´(TIR)": f"{tir:.1f}%",
        "ä½è¡€ç³–æ—¶é—´(TBR)": f"{tbr:.1f}%",
        "é«˜è¡€ç³–æ—¶é—´(TAR)": f"{tar:.1f}%",
        "ç³–åŒ–è¡€çº¢è›‹ç™½ä¼°å€¼(GMI)": f"{gmi:.1f}%",
        "è¡€ç³–èŒƒå›´": f"{np.min(segment_data):.1f}-{np.max(segment_data):.1f} mmol/L"
    }
    
    # è„†æ€§æŒ‡æ ‡
    brittleness_score = calculate_window_brittleness_score(segment_data)
    variability_index = np.std(np.diff(segment_data)) if len(segment_data) > 1 else 0
    stability_score = max(0, 100 - cv * 1.5)
    
    metrics["è„†æ€§æŒ‡æ ‡"] = {
        "è„†æ€§è¯„åˆ†": f"{brittleness_score:.1f}/100",
        "å˜å¼‚æ€§æŒ‡æ•°": f"{variability_index:.2f}",
        "ç¨³å®šæ€§è¯„åˆ†": f"{stability_score:.1f}/100",
        "è„†æ€§ç­‰çº§": classify_brittleness_level(brittleness_score, 25, 50, 75)
    }
    
    # é«˜çº§æŒ‡æ ‡
    trend_strength = calculate_trend_strength(segment_data) if len(segment_data) > 3 else 0
    chaos_score = calculate_simple_chaos_score(segment_data)
    
    metrics["é«˜çº§æŒ‡æ ‡"] = {
        "è¶‹åŠ¿å¼ºåº¦": f"{trend_strength:.2f}",
        "æ··æ²Œè¯„åˆ†": f"{chaos_score:.1f}/10",
        "å¤æ‚åº¦ç­‰çº§": "é«˜" if chaos_score > 5 else "ä¸­" if chaos_score > 3 else "ä½"
    }
    
    # ä¸´åºŠè§£è¯»
    control_quality = "ä¼˜ç§€" if tir > 80 and cv < 25 else "è‰¯å¥½" if tir > 60 and cv < 35 else "éœ€æ”¹å–„"
    risk_level = "é«˜é£é™©" if brittleness_score > 75 else "ä¸­ç­‰é£é™©" if brittleness_score > 50 else "ä½é£é™©"
    
    metrics["ä¸´åºŠè§£è¯»"] = {
        "è¡€ç³–æ§åˆ¶è´¨é‡": control_quality,
        "è„†æ€§é£é™©ç­‰çº§": risk_level,
        "ä¸»è¦ç‰¹å¾": generate_segment_clinical_features(mean_glucose, cv, tir, brittleness_score),
        "ç®¡ç†å»ºè®®": generate_segment_management_advice(control_quality, risk_level, cv)
    }
    
    return metrics

def generate_segment_clinical_features(mean_glucose: float, cv: float, tir: float, brittleness: float) -> List[str]:
    """ç”Ÿæˆæ®µè½ä¸´åºŠç‰¹å¾"""
    
    features = []
    
    # è¡€ç³–æ°´å¹³ç‰¹å¾
    if mean_glucose > 12:
        features.append("å¹³å‡è¡€ç³–åé«˜ï¼Œå­˜åœ¨é•¿æœŸé«˜è¡€ç³–æš´éœ²")
    elif mean_glucose < 6:
        features.append("å¹³å‡è¡€ç³–åä½ï¼Œéœ€è­¦æƒ•ä½è¡€ç³–é£é™©")
    else:
        features.append("å¹³å‡è¡€ç³–æ§åˆ¶åœ¨åˆç†èŒƒå›´å†…")
    
    # å˜å¼‚æ€§ç‰¹å¾
    if cv > 40:
        features.append("è¡€ç³–å˜å¼‚æ€§æé«˜ï¼Œç³»ç»Ÿæä¸ç¨³å®š")
    elif cv > 30:
        features.append("è¡€ç³–å˜å¼‚æ€§åé«˜ï¼Œç¨³å®šæ€§ä¸è¶³")
    elif cv < 20:
        features.append("è¡€ç³–å˜å¼‚æ€§æ§åˆ¶è‰¯å¥½")
    
    # TIRç‰¹å¾
    if tir > 85:
        features.append("ç›®æ ‡èŒƒå›´æ—¶é—´ä¼˜ç§€")
    elif tir < 50:
        features.append("ç›®æ ‡èŒƒå›´æ—¶é—´ä¸¥é‡ä¸è¶³")
    
    # è„†æ€§ç‰¹å¾
    if brittleness > 75:
        features.append("å‘ˆç°é«˜åº¦è„†æ€§ç‰¹å¾ï¼Œç®¡ç†éš¾åº¦å¤§")
    elif brittleness < 30:
        features.append("è„†æ€§ç‰¹å¾è½»å¾®ï¼Œç›¸å¯¹ç¨³å®š")
    
    return features

def generate_segment_management_advice(control_quality: str, risk_level: str, cv: float) -> List[str]:
    """ç”Ÿæˆæ®µè½ç®¡ç†å»ºè®®"""
    
    advice = []
    
    if control_quality == "éœ€æ”¹å–„":
        advice.append("éœ€è¦é‡æ–°è¯„ä¼°å’Œè°ƒæ•´æ²»ç–—æ–¹æ¡ˆ")
        if cv > 40:
            advice.append("è€ƒè™‘ä½¿ç”¨èƒ°å²›ç´ æ³µæˆ–CGMç³»ç»Ÿ")
    elif control_quality == "ä¼˜ç§€":
        advice.append("ç»´æŒå½“å‰æœ‰æ•ˆæ²»ç–—ç­–ç•¥")
    
    if risk_level == "é«˜é£é™©":
        advice.append("å»ºè®®å¢åŠ ç›‘æµ‹é¢‘ç‡å’Œä¸“ç§‘éšè®¿")
        advice.append("åˆ¶å®šåº”æ€¥å¤„ç½®é¢„æ¡ˆ")
    
    return advice

def generate_segment_comparisons(segment_metrics: List[dict]) -> dict:
    """ç”Ÿæˆæ®µé—´å¯¹æ¯”"""
    
    if len(segment_metrics) < 2:
        return {"error": "è‡³å°‘éœ€è¦ä¸¤ä¸ªæ®µè½è¿›è¡Œå¯¹æ¯”"}
    
    comparisons = {
        "å¯¹æ¯”ç»´åº¦": ["è¡€ç³–æ§åˆ¶æ”¹å–„", "å˜å¼‚æ€§å˜åŒ–", "è„†æ€§ç¨‹åº¦å˜åŒ–", "æ²»ç–—ååº”"],
        "å…³é”®æŒ‡æ ‡å˜åŒ–": {},
        "æ˜¾è‘—æ€§å·®å¼‚": [],
        "ä¸´åºŠæ”¹å–„è¯„ä¼°": {}
    }
    
    # æå–å…³é”®æŒ‡æ ‡è¿›è¡Œå¯¹æ¯”
    first_segment = segment_metrics[0]
    last_segment = segment_metrics[-1]
    
    # æå–æ•°å€¼
    def extract_numeric(value_str):
        import re
        match = re.search(r'(\d+\.?\d*)', str(value_str))
        return float(match.group(1)) if match else 0
    
    first_tir = extract_numeric(first_segment["åŸºç¡€æŒ‡æ ‡"]["ç›®æ ‡èŒƒå›´æ—¶é—´(TIR)"])
    last_tir = extract_numeric(last_segment["åŸºç¡€æŒ‡æ ‡"]["ç›®æ ‡èŒƒå›´æ—¶é—´(TIR)"])
    first_cv = extract_numeric(first_segment["åŸºç¡€æŒ‡æ ‡"]["å˜å¼‚ç³»æ•°"])
    last_cv = extract_numeric(last_segment["åŸºç¡€æŒ‡æ ‡"]["å˜å¼‚ç³»æ•°"])
    first_glucose = extract_numeric(first_segment["åŸºç¡€æŒ‡æ ‡"]["å¹³å‡è¡€ç³–"])
    last_glucose = extract_numeric(last_segment["åŸºç¡€æŒ‡æ ‡"]["å¹³å‡è¡€ç³–"])
    first_brittleness = extract_numeric(first_segment["è„†æ€§æŒ‡æ ‡"]["è„†æ€§è¯„åˆ†"])
    last_brittleness = extract_numeric(last_segment["è„†æ€§æŒ‡æ ‡"]["è„†æ€§è¯„åˆ†"])
    
    # è®¡ç®—å˜åŒ–
    tir_change = last_tir - first_tir
    cv_change = last_cv - first_cv
    glucose_change = last_glucose - first_glucose
    brittleness_change = last_brittleness - first_brittleness
    
    comparisons["å…³é”®æŒ‡æ ‡å˜åŒ–"] = {
        "TIRå˜åŒ–": f"{tir_change:+.1f}% ({first_tir:.1f}% â†’ {last_tir:.1f}%)",
        "CVå˜åŒ–": f"{cv_change:+.1f}% ({first_cv:.1f}% â†’ {last_cv:.1f}%)",
        "å¹³å‡è¡€ç³–å˜åŒ–": f"{glucose_change:+.2f} mmol/L ({first_glucose:.1f} â†’ {last_glucose:.1f})",
        "è„†æ€§è¯„åˆ†å˜åŒ–": f"{brittleness_change:+.1f}åˆ† ({first_brittleness:.1f} â†’ {last_brittleness:.1f})"
    }
    
    # æ˜¾è‘—æ€§è¯„ä¼°
    if abs(tir_change) > 15:
        significance = "ææ˜¾è‘—" if abs(tir_change) > 30 else "æ˜¾è‘—"
        direction = "æ”¹å–„" if tir_change > 0 else "æ¶åŒ–"
        comparisons["æ˜¾è‘—æ€§å·®å¼‚"].append(f"TIR{significance}{direction} ({tir_change:+.1f}%)")
    
    if abs(cv_change) > 10:
        significance = "ææ˜¾è‘—" if abs(cv_change) > 20 else "æ˜¾è‘—"
        direction = "æ”¹å–„" if cv_change < 0 else "æ¶åŒ–"
        comparisons["æ˜¾è‘—æ€§å·®å¼‚"].append(f"è¡€ç³–å˜å¼‚æ€§{significance}{direction} ({cv_change:+.1f}%)")
    
    if abs(brittleness_change) > 20:
        significance = "ææ˜¾è‘—" if abs(brittleness_change) > 40 else "æ˜¾è‘—"
        direction = "æ”¹å–„" if brittleness_change < 0 else "æ¶åŒ–"
        comparisons["æ˜¾è‘—æ€§å·®å¼‚"].append(f"è„†æ€§ç¨‹åº¦{significance}{direction} ({brittleness_change:+.1f}åˆ†)")
    
    # ä¸´åºŠæ”¹å–„è¯„ä¼°
    improvement_count = 0
    if tir_change > 10: improvement_count += 1
    if cv_change < -10: improvement_count += 1
    if glucose_change < -1 and first_glucose > 8: improvement_count += 1
    if brittleness_change < -15: improvement_count += 1
    
    if improvement_count >= 3:
        overall_assessment = "æ˜¾è‘—æ”¹å–„"
        success_probability = "85-95%"
    elif improvement_count >= 2:
        overall_assessment = "æ˜æ˜¾æ”¹å–„"
        success_probability = "70-85%"
    elif improvement_count >= 1:
        overall_assessment = "è½»åº¦æ”¹å–„"
        success_probability = "60-75%"
    else:
        overall_assessment = "å˜åŒ–ä¸æ˜æ˜¾"
        success_probability = "40-60%"
    
    comparisons["ä¸´åºŠæ”¹å–„è¯„ä¼°"] = {
        "æ•´ä½“è¯„ä¼°": overall_assessment,
        "æ”¹å–„ç»´åº¦æ•°": f"{improvement_count}/4ä¸ªç»´åº¦",
        "æ²»ç–—æˆåŠŸæ¦‚ç‡": success_probability,
        "åç»­å»ºè®®": generate_follow_up_recommendations(overall_assessment, improvement_count)
    }
    
    return comparisons

def generate_follow_up_recommendations(overall_assessment: str, improvement_count: int) -> List[str]:
    """ç”Ÿæˆåç»­å»ºè®®"""
    
    recommendations = []
    
    if overall_assessment == "æ˜¾è‘—æ”¹å–„":
        recommendations.extend([
            "ç»´æŒå½“å‰æ²»ç–—æ–¹æ¡ˆï¼Œç»§ç»­å¯†åˆ‡ç›‘æµ‹",
            "å¯è€ƒè™‘é€‚åº¦æ”¾å®½ç›‘æµ‹é¢‘ç‡",
            "é‡ç‚¹å…³æ³¨é•¿æœŸç»´æŒå’Œå¹¶å‘ç—‡é¢„é˜²"
        ])
    elif overall_assessment == "æ˜æ˜¾æ”¹å–„":
        recommendations.extend([
            "å½“å‰æ²»ç–—æ–¹å‘æ­£ç¡®ï¼Œç»§ç»­ä¼˜åŒ–å‰‚é‡",
            "ç»´æŒç°æœ‰ç›‘æµ‹é¢‘ç‡",
            "å…³æ³¨æœªæ”¹å–„ç»´åº¦çš„è¿›ä¸€æ­¥ä¼˜åŒ–"
        ])
    elif overall_assessment == "è½»åº¦æ”¹å–„":
        recommendations.extend([
            "æ²»ç–—æœ‰æ•ˆæœä½†éœ€è¦åŠ å¼º",
            "è€ƒè™‘å¢åŠ æ²»ç–—å¼ºåº¦æˆ–è°ƒæ•´æ–¹æ¡ˆ",
            "å¢åŠ ç›‘æµ‹é¢‘ç‡ç¡®ä¿å®‰å…¨æ€§"
        ])
    else:
        recommendations.extend([
            "å½“å‰æ–¹æ¡ˆæ•ˆæœä¸ç†æƒ³ï¼Œéœ€è¦é‡æ–°è¯„ä¼°",
            "è€ƒè™‘æ›´æ¢æ²»ç–—ç­–ç•¥æˆ–è¯ç‰©",
            "å»ºè®®ä¸“ç§‘ä¼šè¯Šåˆ¶å®šæ–°çš„æ²»ç–—è®¡åˆ’"
        ])
    
    return recommendations

def assess_overall_trends(segment_metrics: List[dict]) -> dict:
    """è¯„ä¼°æ•´ä½“è¶‹åŠ¿"""
    
    if len(segment_metrics) < 2:
        return {"error": "æ•°æ®ä¸è¶³"}
    
    trends = {
        "è¶‹åŠ¿åˆ†ææ–¹æ³•": "åŸºäºå¤šæ®µè½æŒ‡æ ‡çš„çº¿æ€§å’Œéçº¿æ€§è¶‹åŠ¿è¯†åˆ«",
        "ä¸»è¦è¶‹åŠ¿": {},
        "è¶‹åŠ¿å¼ºåº¦": {},
        "é¢„æµ‹è¯„ä¼°": {}
    }
    
    # æå–å„æ®µå…³é”®æŒ‡æ ‡
    def extract_numeric(value_str):
        import re
        match = re.search(r'(\d+\.?\d*)', str(value_str))
        return float(match.group(1)) if match else 0
    
    tir_values = [extract_numeric(seg["åŸºç¡€æŒ‡æ ‡"]["ç›®æ ‡èŒƒå›´æ—¶é—´(TIR)"]) for seg in segment_metrics]
    cv_values = [extract_numeric(seg["åŸºç¡€æŒ‡æ ‡"]["å˜å¼‚ç³»æ•°"]) for seg in segment_metrics]
    glucose_values = [extract_numeric(seg["åŸºç¡€æŒ‡æ ‡"]["å¹³å‡è¡€ç³–"]) for seg in segment_metrics]
    brittleness_values = [extract_numeric(seg["è„†æ€§æŒ‡æ ‡"]["è„†æ€§è¯„åˆ†"]) for seg in segment_metrics]
    
    # è®¡ç®—è¶‹åŠ¿æ–¹å‘å’Œå¼ºåº¦
    def calculate_trend(values):
        if len(values) < 2:
            return "æ— æ³•è®¡ç®—", 0
        
        x = np.arange(len(values))
        slope, _, r_value, p_value, _ = stats.linregress(x, values)
        
        if p_value < 0.05:  # æ˜¾è‘—æ€§æ£€éªŒ
            if slope > 0:
                direction = "ä¸Šå‡è¶‹åŠ¿"
            else:
                direction = "ä¸‹é™è¶‹åŠ¿"
            strength = abs(r_value)
        else:
            direction = "è¶‹åŠ¿ä¸æ˜¾è‘—"
            strength = 0
        
        return direction, strength
    
    # åˆ†æå„æŒ‡æ ‡è¶‹åŠ¿
    tir_trend, tir_strength = calculate_trend(tir_values)
    cv_trend, cv_strength = calculate_trend(cv_values)
    glucose_trend, glucose_strength = calculate_trend(glucose_values)
    brittleness_trend, brittleness_strength = calculate_trend(brittleness_values)
    
    trends["ä¸»è¦è¶‹åŠ¿"] = {
        "TIRè¶‹åŠ¿": tir_trend,
        "å˜å¼‚ç³»æ•°è¶‹åŠ¿": cv_trend,
        "å¹³å‡è¡€ç³–è¶‹åŠ¿": glucose_trend,
        "è„†æ€§è¯„åˆ†è¶‹åŠ¿": brittleness_trend
    }
    
    trends["è¶‹åŠ¿å¼ºåº¦"] = {
        "TIRè¶‹åŠ¿å¼ºåº¦": f"{tir_strength:.3f}",
        "å˜å¼‚æ€§è¶‹åŠ¿å¼ºåº¦": f"{cv_strength:.3f}",
        "è¡€ç³–è¶‹åŠ¿å¼ºåº¦": f"{glucose_strength:.3f}",
        "è„†æ€§è¶‹åŠ¿å¼ºåº¦": f"{brittleness_strength:.3f}"
    }
    
    # ç»¼åˆè¯„ä¼°
    positive_trends = 0
    if tir_trend == "ä¸Šå‡è¶‹åŠ¿": positive_trends += 1
    if cv_trend == "ä¸‹é™è¶‹åŠ¿": positive_trends += 1
    if glucose_trend == "ä¸‹é™è¶‹åŠ¿" and glucose_values[0] > 8: positive_trends += 1
    if brittleness_trend == "ä¸‹é™è¶‹åŠ¿": positive_trends += 1
    
    if positive_trends >= 3:
        overall_trend = "å¤šç»´åº¦æ˜¾è‘—æ”¹å–„"
        prediction = "é¢„åä¼˜ç§€ï¼Œæœ‰æœ›è¾¾åˆ°é•¿æœŸç¨³å®šæ§åˆ¶"
    elif positive_trends >= 2:
        overall_trend = "ä¸»è¦ç»´åº¦æ”¹å–„"
        prediction = "é¢„åè‰¯å¥½ï¼Œç»§ç»­å½“å‰æ²»ç–—ç­–ç•¥"
    elif positive_trends >= 1:
        overall_trend = "éƒ¨åˆ†æ”¹å–„"
        prediction = "é¢„åä¸€èˆ¬ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–æ²»ç–—"
    else:
        overall_trend = "æ”¹å–„ä¸æ˜æ˜¾"
        prediction = "é¢„åéœ€è°¨æ…è¯„ä¼°ï¼Œå»ºè®®è°ƒæ•´æ²»ç–—æ–¹æ¡ˆ"
    
    trends["é¢„æµ‹è¯„ä¼°"] = {
        "æ•´ä½“è¶‹åŠ¿è¯„ä¼°": overall_trend,
        "é˜³æ€§è¶‹åŠ¿æ•°é‡": f"{positive_trends}/4",
        "é¢„åé¢„æµ‹": prediction,
        "æ¨èç­–ç•¥": generate_trend_based_strategy(overall_trend, positive_trends)
    }
    
    return trends

def generate_trend_based_strategy(overall_trend: str, positive_trends: int) -> List[str]:
    """åŸºäºè¶‹åŠ¿ç”Ÿæˆç­–ç•¥å»ºè®®"""
    
    strategies = []
    
    if overall_trend == "å¤šç»´åº¦æ˜¾è‘—æ”¹å–„":
        strategies.extend([
            "ç»´æŒå½“å‰ä¼˜ç§€çš„æ²»ç–—æ•ˆæœ",
            "é€æ­¥è¿‡æ¸¡åˆ°ç»´æŒæœŸç®¡ç†æ¨¡å¼",
            "é‡ç‚¹å…³æ³¨é•¿æœŸç¨³å®šæ€§å’Œç”Ÿæ´»è´¨é‡"
        ])
    elif overall_trend == "ä¸»è¦ç»´åº¦æ”¹å–„":
        strategies.extend([
            "ç»§ç»­å½“å‰æœ‰æ•ˆæ²»ç–—æ–¹æ¡ˆ",
            "é’ˆå¯¹æœªæ”¹å–„ç»´åº¦è¿›è¡Œç²¾ç»†è°ƒæ•´",
            "ä¿æŒå½“å‰ç›‘æµ‹é¢‘ç‡"
        ])
    elif overall_trend == "éƒ¨åˆ†æ”¹å–„":
        strategies.extend([
            "éƒ¨åˆ†æœ‰æ•ˆä½†éœ€è¦å¼ºåŒ–æ²»ç–—",
            "è€ƒè™‘å¢åŠ æ²»ç–—å¼ºåº¦æˆ–è”åˆç”¨è¯",
            "å¯†åˆ‡ç›‘æµ‹æ²»ç–—ååº”"
        ])
    else:
        strategies.extend([
            "å½“å‰ç­–ç•¥æ•ˆæœä¸ç†æƒ³",
            "å»ºè®®å…¨é¢é‡æ–°è¯„ä¼°æ²»ç–—æ–¹æ¡ˆ",
            "è€ƒè™‘ä¸“ç§‘ä¼šè¯Šæˆ–ä½é™¢è°ƒæ•´"
        ])
    
    return strategies

def evaluate_intelligent_segmentation_quality(segment_analysis: dict, segments: dict) -> dict:
    """è¯„ä¼°æ™ºèƒ½åˆ†æ®µè´¨é‡"""
    
    quality_assessment = {
        "è¯„ä¼°ç»´åº¦": ["åˆ†æ®µåˆç†æ€§", "å·®å¼‚æ˜¾è‘—æ€§", "ä¸´åºŠæ„ä¹‰", "æŠ€æœ¯å¯é æ€§"],
        "è´¨é‡è¯„åˆ†": {},
        "ä¼˜åŠ¿åˆ†æ": [],
        "æ”¹è¿›å»ºè®®": []
    }
    
    # åˆ†æ®µæ•°é‡è¯„ä¼°
    num_segments = segments.get("åˆ†æ®µæ•°é‡", 0)
    if 2 <= num_segments <= 4:
        segmentation_score = 85
        quality_assessment["ä¼˜åŠ¿åˆ†æ"].append("åˆ†æ®µæ•°é‡é€‚ä¸­ï¼Œä¾¿äºåˆ†æå’Œç†è§£")
    elif num_segments == 1:
        segmentation_score = 40
        quality_assessment["æ”¹è¿›å»ºè®®"].append("åˆ†æ®µè¿‡å°‘ï¼Œå¯èƒ½é—æ¼é‡è¦å˜åŒ–ç‚¹")
    elif num_segments > 5:
        segmentation_score = 60
        quality_assessment["æ”¹è¿›å»ºè®®"].append("åˆ†æ®µè¿‡å¤šï¼Œå¯èƒ½å­˜åœ¨è¿‡åº¦åˆ†å‰²")
    else:
        segmentation_score = 70
    
    # å·®å¼‚æ˜¾è‘—æ€§è¯„ä¼°
    significant_differences = 0
    if "æ®µé—´å¯¹æ¯”" in segment_analysis and "æ˜¾è‘—æ€§å·®å¼‚" in segment_analysis["æ®µé—´å¯¹æ¯”"]:
        significant_differences = len(segment_analysis["æ®µé—´å¯¹æ¯”"]["æ˜¾è‘—æ€§å·®å¼‚"])
    
    if significant_differences >= 2:
        difference_score = 90
        quality_assessment["ä¼˜åŠ¿åˆ†æ"].append("æ®µé—´å·®å¼‚æ˜¾è‘—ï¼Œåˆ†æ®µå…·æœ‰ä¸´åºŠæ„ä¹‰")
    elif significant_differences >= 1:
        difference_score = 75
        quality_assessment["ä¼˜åŠ¿åˆ†æ"].append("å­˜åœ¨æœ‰æ„ä¹‰çš„æ®µé—´å·®å¼‚")
    else:
        difference_score = 50
        quality_assessment["æ”¹è¿›å»ºè®®"].append("æ®µé—´å·®å¼‚ä¸å¤Ÿæ˜¾è‘—ï¼Œéœ€è¦ä¼˜åŒ–æ£€æµ‹å‚æ•°")
    
    # ä¸´åºŠæ„ä¹‰è¯„ä¼°
    clinical_score = 80  # åŸºç¡€åˆ†
    if "ä¸´åºŠæ”¹å–„è¯„ä¼°" in segment_analysis.get("æ®µé—´å¯¹æ¯”", {}):
        improvement = segment_analysis["æ®µé—´å¯¹æ¯”"]["ä¸´åºŠæ”¹å–„è¯„ä¼°"].get("æ•´ä½“è¯„ä¼°", "")
        if "æ˜¾è‘—" in improvement:
            clinical_score = 95
            quality_assessment["ä¼˜åŠ¿åˆ†æ"].append("æ£€æµ‹åˆ°æ˜¾è‘—çš„ä¸´åºŠæ”¹å–„")
        elif "æ˜æ˜¾" in improvement:
            clinical_score = 85
            quality_assessment["ä¼˜åŠ¿åˆ†æ"].append("è¯†åˆ«å‡ºæ˜æ˜¾çš„æ²»ç–—ååº”")
    
    # æŠ€æœ¯å¯é æ€§è¯„ä¼°
    tech_score = 85  # åŸºäºå¤šç®—æ³•èåˆçš„é»˜è®¤é«˜åˆ†
    quality_assessment["ä¼˜åŠ¿åˆ†æ"].append("é‡‡ç”¨å¤šç®—æ³•èåˆæŠ€æœ¯ï¼Œæé«˜æ£€æµ‹å¯é æ€§")
    
    # ç»¼åˆè¯„åˆ†
    overall_score = (segmentation_score * 0.3 + difference_score * 0.3 + 
                    clinical_score * 0.3 + tech_score * 0.1)
    
    quality_assessment["è´¨é‡è¯„åˆ†"] = {
        "åˆ†æ®µåˆç†æ€§": f"{segmentation_score}/100",
        "å·®å¼‚æ˜¾è‘—æ€§": f"{difference_score}/100",
        "ä¸´åºŠæ„ä¹‰": f"{clinical_score}/100",
        "æŠ€æœ¯å¯é æ€§": f"{tech_score}/100",
        "ç»¼åˆè¯„åˆ†": f"{overall_score:.1f}/100"
    }
    
    # è´¨é‡ç­‰çº§
    if overall_score >= 85:
        quality_level = "ä¼˜ç§€"
        quality_assessment["ä¼˜åŠ¿åˆ†æ"].append("æ™ºèƒ½åˆ†æ®µè´¨é‡ä¼˜ç§€ï¼Œç»“æœé«˜åº¦å¯ä¿¡")
    elif overall_score >= 75:
        quality_level = "è‰¯å¥½"
        quality_assessment["ä¼˜åŠ¿åˆ†æ"].append("æ™ºèƒ½åˆ†æ®µè´¨é‡è‰¯å¥½ï¼Œç»“æœå¯ä¿¡")
    elif overall_score >= 65:
        quality_level = "ä¸­ç­‰"
        quality_assessment["æ”¹è¿›å»ºè®®"].append("åˆ†æ®µè´¨é‡ä¸­ç­‰ï¼Œå»ºè®®ä¼˜åŒ–æ£€æµ‹ç®—æ³•")
    else:
        quality_level = "éœ€è¦æ”¹è¿›"
        quality_assessment["æ”¹è¿›å»ºè®®"].append("åˆ†æ®µè´¨é‡æœ‰å¾…æé«˜ï¼Œéœ€è¦æ”¹è¿›æŠ€æœ¯æ–¹æ³•")
    
    quality_assessment["è´¨é‡ç­‰çº§"] = quality_level
    
    return quality_assessment

def generate_clinical_significance_interpretation(segment_analysis: dict, change_points: dict) -> dict:
    """ç”Ÿæˆä¸´åºŠæ„ä¹‰è§£è¯»"""
    
    interpretation = {
        "è§£è¯»è§’åº¦": ["æ²»ç–—æ•ˆæœè¯„ä¼°", "ç—…æƒ…æ¼”å˜åˆ†æ", "ç®¡ç†ç­–ç•¥æŒ‡å¯¼", "é¢„ååˆ¤æ–­"],
        "å…³é”®å‘ç°": [],
        "ä¸´åºŠä»·å€¼": [],
        "æŒ‡å¯¼æ„ä¹‰": []
    }
    
    # åˆ†æå˜åŒ–ç‚¹çš„ä¸´åºŠæ„ä¹‰
    comprehensive_changes = change_points.get("ç»¼åˆå˜åŒ–ç‚¹", [])
    if comprehensive_changes:
        interpretation["å…³é”®å‘ç°"].append(f"è¯†åˆ«å‡º{len(comprehensive_changes)}ä¸ªé‡è¦çš„ç—…æƒ…è½¬æŠ˜ç‚¹")
        
        if len(comprehensive_changes) == 1:
            interpretation["å…³é”®å‘ç°"].append("å­˜åœ¨æ˜æ˜¾çš„æ²»ç–—ååº”é˜¶æ®µè½¬æ¢")
        elif len(comprehensive_changes) >= 2:
            interpretation["å…³é”®å‘ç°"].append("ç—…æƒ…æ¼”å˜å‘ˆç°å¤šé˜¶æ®µç‰¹å¾")
    
    # åˆ†ææ”¹å–„è¶‹åŠ¿
    if "æ®µé—´å¯¹æ¯”" in segment_analysis and "ä¸´åºŠæ”¹å–„è¯„ä¼°" in segment_analysis["æ®µé—´å¯¹æ¯”"]:
        improvement = segment_analysis["æ®µé—´å¯¹æ¯”"]["ä¸´åºŠæ”¹å–„è¯„ä¼°"]
        overall_assessment = improvement.get("æ•´ä½“è¯„ä¼°", "")
        
        if "æ˜¾è‘—" in overall_assessment:
            interpretation["å…³é”®å‘ç°"].append("æ²»ç–—ç­–ç•¥é«˜åº¦æœ‰æ•ˆï¼Œè¡€ç³–æ§åˆ¶æ˜¾è‘—æ”¹å–„")
            interpretation["ä¸´åºŠä»·å€¼"].append("ä¸ºæ²»ç–—æœ‰æ•ˆæ€§æä¾›äº†å®¢è§‚çš„é‡åŒ–è¯æ®")
        elif "æ˜æ˜¾" in overall_assessment:
            interpretation["å…³é”®å‘ç°"].append("æ²»ç–—æ–¹æ¡ˆæœ‰æ•ˆï¼Œè¡€ç³–ç®¡ç†æŒç»­æ”¹å–„")
            interpretation["ä¸´åºŠä»·å€¼"].append("è¯å®äº†å½“å‰æ²»ç–—ç­–ç•¥çš„æ­£ç¡®æ€§")
        else:
            interpretation["å…³é”®å‘ç°"].append("æ²»ç–—ååº”æœ‰é™ï¼Œéœ€è¦è°ƒæ•´ç®¡ç†ç­–ç•¥")
            interpretation["ä¸´åºŠä»·å€¼"].append("ä¸ºæ²»ç–—è°ƒæ•´æä¾›äº†é‡è¦çš„å†³ç­–ä¾æ®")
    
    # åˆ†ææ•´ä½“è¶‹åŠ¿çš„ä¸´åºŠä»·å€¼
    if "æ•´ä½“è¶‹åŠ¿è¯„ä¼°" in segment_analysis:
        trends = segment_analysis["æ•´ä½“è¶‹åŠ¿è¯„ä¼°"]
        prediction = trends.get("é¢„æµ‹è¯„ä¼°", {})
        overall_trend = prediction.get("æ•´ä½“è¶‹åŠ¿è¯„ä¼°", "")
        
        interpretation["ä¸´åºŠä»·å€¼"].extend([
            "é€šè¿‡æ™ºèƒ½åˆ†æ®µè¯†åˆ«å‡ºè¡€ç³–æ§åˆ¶çš„åŠ¨æ€å˜åŒ–æ¨¡å¼",
            "ä¸ºä¸ªä½“åŒ–æ²»ç–—æ–¹æ¡ˆåˆ¶å®šæä¾›äº†ç§‘å­¦ä¾æ®",
            "æœ‰åŠ©äºé¢„æµ‹æ‚£è€…çš„æ²»ç–—ååº”å’Œé¢„å"
        ])
    
    # ç®¡ç†æŒ‡å¯¼æ„ä¹‰
    interpretation["æŒ‡å¯¼æ„ä¹‰"].extend([
        "å¸®åŠ©åŒ»ç”Ÿè¯†åˆ«æ²»ç–—çš„å…³é”®æ—¶é—´èŠ‚ç‚¹",
        "ä¸ºè°ƒæ•´æ²»ç–—ç­–ç•¥çš„æ—¶æœºé€‰æ‹©æä¾›å‚è€ƒ",
        "æ”¯æŒåˆ¶å®šä¸ªæ€§åŒ–çš„ç›‘æµ‹å’Œéšè®¿è®¡åˆ’",
        "ä¸ºæ‚£è€…æ•™è‚²å’Œä¾ä»æ€§ç®¡ç†æä¾›ç§‘å­¦ä¾æ®"
    ])
    
    # æŠ€æœ¯åˆ›æ–°ä»·å€¼
    interpretation["æŠ€æœ¯åˆ›æ–°ä»·å€¼"] = [
        "é¦–æ¬¡å°†å¤šç®—æ³•èåˆåº”ç”¨äºè¡€ç³–ç®¡ç†åˆ†æ®µåˆ†æ",
        "å®ç°äº†ä»å›ºå®šæ—¶é—´åˆ†æ®µåˆ°æ™ºèƒ½æ•°æ®é©±åŠ¨åˆ†æ®µçš„è·¨è¶Š",
        "ä¸ºç²¾å‡†åŒ»å­¦åœ¨ç³–å°¿ç—…ç®¡ç†ä¸­çš„åº”ç”¨æä¾›äº†æ–°å·¥å…·",
        "å»ºç«‹äº†è¡€ç³–è„†æ€§åŠ¨æ€è¯„ä¼°çš„æ–°æ–¹æ³•å­¦"
    ]
    
    return interpretation

# ä¸»ç¨‹åºæ‰§è¡Œ
if __name__ == "__main__":
    import sys
    try:
        # è·å–å‘½ä»¤è¡Œå‚æ•°
        if len(sys.argv) >= 3:
            filepath = sys.argv[1] 
            patient_id = sys.argv[2]
        else:
            filepath = "/Users/williamsun/Documents/gplus/docs/AGPAI/demodata/èƒ°è…ºå¤–ç§‘/ä¸Šå®˜æå†›-253124-1MH011R56MM.xlsx"
            patient_id = "ä¸Šå®˜æå†›-253124"
        
        # åˆ†ææ‚£è€…æ•°æ®
        print("[Agent2 Intelligence] å¼€å§‹æ™ºèƒ½è„†æ€§åˆ†æ...")
        print(f"[Agent2 Intelligence] æ•°æ®æ–‡ä»¶: {filepath}")
        report = analyze_intelligent_brittleness(filepath, patient_id)
        
        # è¾“å‡ºå…³é”®ä¿¡æ¯
        brittleness = report["æ™ºèƒ½è„†æ€§åˆ†å‹è¯„ä¼°"]
        treatment = report["æ²»ç–—ååº”åŠ¨æ€è¯„ä¼°"]
        
        print("[Agent2 Intelligence] æ™ºèƒ½è„†æ€§åˆ†æå®Œæˆ")
        print(f"è„†æ€§åˆ†å‹: {brittleness['è„†æ€§åˆ†å‹']}")
        print(f"è„†æ€§ä¸¥é‡ç¨‹åº¦: {brittleness['è„†æ€§ä¸¥é‡ç¨‹åº¦']}")
        print(f"é£é™©ç­‰çº§: {brittleness['é£é™©ç­‰çº§']}")
        print(f"è„†æ€§è¯„åˆ†: {brittleness['è„†æ€§è¯„åˆ†']}")
        
        if "æ²»ç–—ååº”è¯„ä¼°" in treatment:
            response_info = treatment["æ²»ç–—ååº”è¯„ä¼°"]
            print(f"æ²»ç–—ååº”: {response_info.get('ååº”åˆ†çº§', 'æœªè¯„ä¼°')}")
            print(f"ååº”ç±»å‹: {response_info.get('ååº”ç±»å‹', 'æœªçŸ¥')}")
        
        # ä¿å­˜æŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"Agent2_Intelligent_Analysis_{patient_id}_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"æ™ºèƒ½åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {filename}")
        
    except Exception as e:
        print(f"[Agent2 Intelligence] åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()